# Hugging Face ä¸Šçš„ AWQ åŒ»ç–—æ¨¡å‹èµ„æº

## ğŸ” å¦‚ä½•æœç´¢ AWQ æ¨¡å‹

### æ–¹æ³• 1ï¼šç›´æ¥æœç´¢
è®¿é—®ï¼šhttps://huggingface.co/models?other=autoawq

åœ¨æœç´¢æ¡†è¾“å…¥ï¼š
- `medical AWQ`
- `clinical AWQ`
- `radiology AWQ`
- `biomedical AWQ`

### æ–¹æ³• 2ï¼šæŒ‰ç”¨æˆ·/ç»„ç»‡ç­›é€‰

```
https://huggingface.co/TheBloke?search=AWQ
https://huggingface.co/models?author=TheBloke&other=autoawq
```

---

## ğŸ“¦ å¸¸è§åŒ»ç–—/é€šç”¨ AWQ æ¨¡å‹

### 1. é€šç”¨åŒ»ç–—æ¨¡å‹ï¼ˆå¯èƒ½é€‚ç”¨ï¼‰

è™½ç„¶ Hugging Face ä¸Šä¸“é—¨çš„åŒ»ç–— AWQ æ¨¡å‹ä¸å¤šï¼Œä½†ä»¥ä¸‹é€šç”¨æ¨¡å‹å¯èƒ½å¯¹åŒ»ç–—ä»»åŠ¡æœ‰å¸®åŠ©ï¼š

| æ¨¡å‹ | å¤§å° | è¯´æ˜ | HF é“¾æ¥ |
|------|------|------|---------|
| Mistral-7B-Instruct-AWQ | 7B | é€šç”¨æŒ‡ä»¤æ¨¡å‹ï¼Œé€‚åˆå¾®è°ƒ | TheBloke/Mistral-7B-Instruct-v0.2-AWQ |
| Llama-2-7B-AWQ | 7B | Meta çš„åŸºç¡€æ¨¡å‹ | TheBloke/Llama-2-7B-AWQ |
| Gemma-7B-AWQ | 7B | Google çš„å¼€æºæ¨¡å‹ | æœç´¢ "gemma awq" |
| Qwen-7B-AWQ | 7B | é˜¿é‡Œçš„å¤šè¯­è¨€æ¨¡å‹ | æœç´¢ "qwen awq" |

### 2. å¦‚æœæ‰¾ä¸åˆ°åŒ»ç–—ä¸“ç”¨çš„ AWQ ç‰ˆæœ¬

**æ–¹æ¡ˆ Aï¼šè‡ªå·±é‡åŒ–ï¼ˆæ¨èï¼‰**
```bash
# ä½¿ç”¨æˆ‘ä»¬æä¾›çš„è„šæœ¬
python quantize_medgamma_awq.py \
    --model_path "ä½ çš„åŒ»ç–—æ¨¡å‹" \
    --output_path "./model-awq"
```

**æ–¹æ¡ˆ Bï¼šæ‰¾åŸå§‹åŒ»ç–—æ¨¡å‹ + AWQ è„šæœ¬**
```python
# ç¤ºä¾‹ï¼šé‡åŒ– BioGPT
from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_pretrained("microsoft/BioGPT")
model.quantize(...)
model.save_quantized("./BioGPT-AWQ")
```

---

## ğŸ¯ åŒ»ç–—é¢†åŸŸå¸¸è§æ¨¡å‹ï¼ˆéœ€è‡ªå·±é‡åŒ–ï¼‰

ä»¥ä¸‹æ˜¯ Hugging Face ä¸Šæµè¡Œçš„åŒ»ç–—æ¨¡å‹ï¼Œä½ å¯ä»¥ç”¨æˆ‘ä»¬çš„å·¥å…·é‡åŒ–å®ƒä»¬ï¼š

### åŒ»ç–—æ–‡æœ¬æ¨¡å‹

| æ¨¡å‹åç§° | ä¸“é•¿ | HF é“¾æ¥ |
|---------|------|---------|
| **BioGPT** | ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬ç”Ÿæˆ | microsoft/BioGPT |
| **PubMedBERT** | åŒ»å­¦æ–‡çŒ®ç†è§£ï¼ˆBERT æ¶æ„ï¼Œä¸é€‚åˆç”Ÿæˆï¼‰ | microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract |
| **ClinicalBERT** | ä¸´åºŠç¬”è®°åˆ†æ | emilyalsentzer/Bio_ClinicalBERT |
| **BioBERT** | ç”Ÿç‰©åŒ»å­¦ NER/QA | dmis-lab/biobert-v1.1 |

### åŒ»ç–—å¤šæ¨¡æ€æ¨¡å‹

| æ¨¡å‹åç§° | ä¸“é•¿ | HF é“¾æ¥ |
|---------|------|---------|
| **MedGamma** | åŒ»ç–—å½±åƒæŠ¥å‘Šç”Ÿæˆ | google/medgamma-* |
| **LLaVA-Med** | åŒ»ç–—è§†è§‰é—®ç­” | microsoft/llava-med |
| **MAIRA** | X-ray åˆ†æ | microsoft/maira |

---

## ğŸ› ï¸ é‡åŒ–è¿™äº›æ¨¡å‹çš„æ­¥éª¤

### ç¤ºä¾‹ï¼šé‡åŒ– BioGPT

```bash
# 1. ä¸‹è½½æ¨¡å‹
git lfs install
git clone https://huggingface.co/microsoft/BioGPT

# 2. å‡†å¤‡åŒ»ç–—æ–‡æœ¬æ ¡å‡†æ•°æ®ï¼ˆä»ä½ çš„ MIMIC æ•°æ®ï¼‰
python prepare_calibration.py \
    --input mimic_train_cleaned.csv \
    --output biogpt_calib.json \
    --num_samples 500

# 3. é‡åŒ–
python quantize_medgamma_awq.py \
    --model_path "microsoft/BioGPT" \
    --output_path "./BioGPT-AWQ" \
    --calibration_data "biogpt_calib.json" \
    --mode quantize

# 4. æµ‹è¯•
python quantize_medgamma_awq.py \
    --model_path "./BioGPT-AWQ" \
    --mode test
```

---

## ğŸ“Š AWQ æ¨¡å‹è¯†åˆ«æ–¹æ³•

### å¦‚ä½•ç¡®è®¤ä¸€ä¸ªæ¨¡å‹æ˜¯ AWQ é‡åŒ–çš„ï¼Ÿ

#### æ–¹æ³• 1ï¼šæ£€æŸ¥æ¨¡å‹åç§°
- åŒ…å« `AWQ` æˆ– `awq` å…³é”®è¯
- ç¤ºä¾‹ï¼š`TheBloke/Mistral-7B-AWQ`

#### æ–¹æ³• 2ï¼šæ£€æŸ¥ config.json
```json
{
  "quantization_config": {
    "quant_method": "awq",
    "zero_point": true,
    "group_size": 128,
    "bits": 4,
    "version": "gemm"
  }
}
```

#### æ–¹æ³• 3ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°
- åŸå§‹ 7B æ¨¡å‹ï¼š~14 GB
- AWQ 4-bitï¼š~3.5 GB
- å¦‚æœæ˜¯ ~3-4 GBï¼Œå¾ˆå¯èƒ½æ˜¯é‡åŒ–è¿‡çš„

---

## ğŸ”— æœ‰ç”¨çš„é“¾æ¥

### å®˜æ–¹èµ„æº
- **AutoAWQ GitHub**: https://github.com/casper-hansen/AutoAWQ
- **HuggingFace AWQ æ–‡æ¡£**: https://huggingface.co/docs/transformers/quantization/awq
- **AWQ è®ºæ–‡**: https://arxiv.org/abs/2306.00978

### ç¤¾åŒºèµ„æº
- **TheBloke çš„æ‰€æœ‰ AWQ æ¨¡å‹**: https://huggingface.co/TheBloke?search=AWQ
- **HF è®ºå› - é‡åŒ–è®¨è®º**: https://discuss.huggingface.co/c/quantization
- **Reddit r/LocalLLaMA**: https://reddit.com/r/LocalLLaMAï¼ˆé‡åŒ–ç»éªŒåˆ†äº«ï¼‰

### åŒ»ç–— AI èµ„æº
- **PhysioNet MIMIC**: https://physionet.org/
- **Stanford AIMI**: https://stanfordaimi.azurewebsites.net/
- **RadGraph**: https://github.com/jbdel/radgraph

---

## ğŸ’¡ å®ç”¨æŠ€å·§

### æŠ€å·§ 1ï¼šæœç´¢ç‰¹å®šç”¨æˆ·çš„ AWQ æ¨¡å‹
```
site:huggingface.co TheBloke AWQ medical
```

### æŠ€å·§ 2ï¼šæŒ‰æ¨¡å‹å¤§å°ç­›é€‰
```
åœ¨ HuggingFace æœç´¢é¡µé¢ï¼š
1. è¾“å…¥ "AWQ"
2. åœ¨å·¦ä¾§ Filters é€‰æ‹© Model size
3. é€‰æ‹©é€‚åˆä½  GPU çš„å¤§å°ï¼ˆå¦‚ < 10GBï¼‰
```

### æŠ€å·§ 3ï¼šæŸ¥çœ‹æ¨¡å‹å¡ç‰‡çš„é‡åŒ–ä¿¡æ¯
```
è®¿é—®æ¨¡å‹é¡µé¢ â†’ README â†’ æŸ¥æ‰¾:
- Quantization method
- Bits per weight
- Group size
- æ€§èƒ½åŸºå‡†
```

---

## ğŸ“ å¦‚æœä½ æƒ³åˆ†äº«ä½ çš„ AWQ æ¨¡å‹

### ä¸Šä¼ åˆ° Hugging Face

```bash
# 1. ç™»å½•
huggingface-cli login

# 2. åˆ›å»º repo
huggingface-cli repo create your-model-awq --type model

# 3. ä¸Šä¼ 
cd medgamma-awq-4bit
git lfs install
git init
git remote add origin https://huggingface.co/your-username/your-model-awq
git add .
git commit -m "Add AWQ quantized model"
git push origin main
```

### æ¨¡å‹å¡ç‰‡æ¨¡æ¿

```markdown
---
tags:
- medical
- radiology
- awq
- quantized
- 4-bit
license: apache-2.0
---

# MedGamma-3B-AWQ

AWQ 4-bit quantized version of google/medgamma-3b for medical report generation.

## Model Details
- **Original Model**: google/medgamma-3b
- **Quantization**: 4-bit AWQ
- **Group Size**: 128
- **Model Size**: 3.5 GB (75% reduction)

## Performance
- **F1 Score**: 0.845 (vs 0.850 original, -0.6%)
- **Inference Speed**: 2.5x faster
- **VRAM**: 5 GB (vs 16 GB original)

## Usage
\```python
from awq import AutoAWQForCausalLM
model = AutoAWQForCausalLM.from_quantized("your-username/medgamma-awq")
\```
```

---

## ğŸ“ å­¦ä¹ èµ„æº

### è§†é¢‘æ•™ç¨‹
- YouTube: "AWQ Quantization Explained"
- YouTube: "Quantizing LLMs for Production"

### åšå®¢æ–‡ç« 
- HuggingFace Blog: "AWQ Quantization"
- Medium: "Guide to LLM Quantization"

### ç ”ç©¶è®ºæ–‡
- AWQ (2023): Activation-aware Weight Quantization
- GPTQ (2022): Post-Training Quantization
- SmoothQuant (2023): Mixed Precision

---

**ç¥ä½ æ‰¾åˆ°åˆé€‚çš„æ¨¡å‹ï¼ğŸš€**

å¦‚æœæœ‰é—®é¢˜ï¼Œå¯ä»¥ï¼š
1. åœ¨ AutoAWQ GitHub æ Issue
2. åœ¨ HuggingFace è®ºå›å‘å¸–
3. æ‰¾ä½ çš„é˜Ÿå‹ Lili å’Œ Ashley è®¨è®º
