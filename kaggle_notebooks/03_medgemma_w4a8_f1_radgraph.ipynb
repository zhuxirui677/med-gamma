{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 0: 环境设置（Kaggle P100）\n",
        "\n",
        "- **P100 兼容**：Kaggle 预装 PyTorch cu128 不支持 P100 (sm_60)，需先运行下方安装 Cell 安装 PyTorch cu118\n",
        "- **安装后**：若仍报 GPU 不兼容，请 **Restart Session** 后重新 Run All\n",
        "- P100 不支持 BF16，自动用 FP16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# P100 (sm_60) 需 PyTorch cu118。transformers 4.46 安装到独立目录，避免与系统 5.x 冲突\n",
        "import subprocess\n",
        "import sys\n",
        "subprocess.run([\"pip\", \"install\", \"-q\", \"torch==2.7.1\", \"torchvision==0.22.1\", \"torchaudio==2.7.1\", \"--index-url\", \"https://download.pytorch.org/whl/cu118\"], capture_output=True)\n",
        "subprocess.run([\"pip\", \"install\", \"-q\", \"pillow>=9.0,<12\", \"jinja2\"], capture_output=True)\n",
        "TF_ENV = \"/kaggle/working/transformers_4.46\"\n",
        "subprocess.run([\"pip\", \"install\", \"--target\", TF_ENV, \"--no-cache-dir\", \"-q\", \"transformers==4.46.0\", \"radgraph\"], capture_output=True)\n",
        "subprocess.run([\"pip\", \"install\", \"-q\", \"bitsandbytes\"], capture_output=True)\n",
        "sys.path.insert(0, TF_ENV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 1: W4A8 量化 + F1 + RadGraph\n",
        "\n",
        "## W4A8 演算逻辑\n",
        "- **W4**：权重 4-bit 量化（bitsandbytes NF4，因 MedGemma 无官方 AWQ）\n",
        "- **A8**：激活 8-bit 量化。对每层 Linear 输入做 per-tensor 对称量化：\n",
        "  - 范围 [-128, 127]，scale = max(|x|)/127\n",
        "  - q = round(x * scale).clamp(-128, 127)，反量化 x' = q/scale\n",
        "- **运用方法**：`forward_pre_hook` 在 Linear 前对输入做 fake 量化；真实加速需 QServe 等 INT4×INT8 GEMM kernel。\n",
        "- **与 W4A4 对比**：A8 精度更高，F1 更接近原始；A4 显存更省，F1 略降。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 2: 环境检查（需先运行上方安装）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(f\"Python: {sys.version}\")\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    USE_BF16 = torch.cuda.get_device_capability(0)[0] >= 8\n",
        "    DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
        "    print(f\"精度: {'BF16' if USE_BF16 else 'FP16 (P100)'}\")\n",
        "else:\n",
        "    DTYPE = torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 3: 激活量化工具（W4A8: 8-bit 激活）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import List\n",
        "\n",
        "def _fake_quant_activation(x: torch.Tensor, bits: int) -> torch.Tensor:\n",
        "    \"\"\"Per-tensor 对称 fake 量化: 8-bit [-128,127], 4-bit [-8,7]\"\"\"\n",
        "    if not x.is_floating_point():\n",
        "        return x\n",
        "    max_val = x.abs().max().clamp(min=1e-8)\n",
        "    if bits == 8:\n",
        "        scale = 127.0 / max_val\n",
        "        q = (x * scale).round().clamp(-128, 127)\n",
        "    else:\n",
        "        scale = 7.0 / max_val\n",
        "        q = (x * scale).round().clamp(-8, 7)\n",
        "    return q / scale\n",
        "\n",
        "def _is_linear_like(module):\n",
        "    return hasattr(module, \"weight\") and hasattr(module.weight, \"shape\") and len(module.weight.shape) == 2\n",
        "\n",
        "def register_activation_quant_hooks(model: nn.Module, bits: int = 8) -> List:\n",
        "    hooks = []\n",
        "    def make_hook(b):\n",
        "        def hook(module, input):\n",
        "            if not input or not isinstance(input[0], torch.Tensor):\n",
        "                return input\n",
        "            inp = input[0]\n",
        "            if inp.is_floating_point():\n",
        "                return (_fake_quant_activation(inp, b),) + input[1:]\n",
        "            return input\n",
        "        return hook\n",
        "    for name, m in model.named_modules():\n",
        "        if \"lm_head\" in name:\n",
        "            continue\n",
        "        if _is_linear_like(m):\n",
        "            h = m.register_forward_pre_hook(make_hook(bits))\n",
        "            hooks.append((name, h))\n",
        "    return hooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 4: 导入与路径配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATASET_ROOT = \"/kaggle/input/mimic-cxr-dataset/official_data_iccv_final\"\n",
        "CSV_CANDIDATES = [\"/kaggle/input/mimic-cxr-dataset/mimic_eval_single_image_final_233.csv\", \"/kaggle/input/mimic-cxr-dataset/official_data_iccv_final/mimic_eval_single_image_final_233.csv\", \"/kaggle/input/mimic-eval-233/mimic_eval_single_image_final_233.csv\", \"/kaggle/working/mimic_eval_single_image_final_233.csv\", \"./mimic_eval_single_image_final_233.csv\"]\n",
        "CSV_PATH = next((p for p in CSV_CANDIDATES if os.path.exists(p)), CSV_CANDIDATES[0])\n",
        "print(f\"CSV: {CSV_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 5: 先删除原始/其他模型，再加载 W4A8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "print(\"GPU 已清空，准备加载 W4A8 模型\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 6: 加载 W4A8（4-bit 权重 + 8-bit 激活）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"google/medgemma-1.5-4b-it\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=DTYPE,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "print(\"加载 W4A8: 4-bit 权重 (bitsandbytes) + 8-bit 激活 (hook)\")\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "hooks = register_activation_quant_hooks(model, bits=8)\n",
        "print(f\"已注册 {len(hooks)} 个 Linear 层的 8-bit 激活量化\")\n",
        "\n",
        "W4A8_GPU_GB = torch.cuda.max_memory_allocated(0) / (1024**3) if torch.cuda.is_available() else 0\n",
        "print(f\"W4A8 模型 GPU 占用: {W4A8_GPU_GB:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 7: 图像到报告生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast, re\n",
        "PROMPT_TEMPLATE = \"You are an expert radiologist. Describe this {view} view chest X-ray. Provide a concise report consisting of Findings and Impression. Focus on the heart, lungs, mediastinum, pleural space, and bones. Do NOT use bullet points, asterisks, or section headers. Do NOT include disclaimers or 'AI' warnings. Output pure medical text only.\"\n",
        "def get_single_image_path(cell_val):\n",
        "    if pd.isna(cell_val): return None\n",
        "    s = str(cell_val).strip().replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace('\"',\"\").split(\",\")[0].strip()\n",
        "    if \"files\" in s: rel = \"files\" + s.split(\"files\",1)[1]\n",
        "    else: rel = s.strip(\"/\")\n",
        "    full = os.path.join(DATASET_ROOT, rel) if not rel.startswith(\"/\") else rel\n",
        "    return full if os.path.exists(full) else None\n",
        "def generate_report(model, processor, img_path, view=\"PA\"):\n",
        "    if not os.path.exists(img_path): return \"\"\n",
        "    try: img = Image.open(img_path).convert(\"RGB\")\n",
        "    except: return \"\"\n",
        "    msgs = [{\"role\":\"user\",\"content\":[{\"type\":\"image\",\"image\":img},{\"type\":\"text\",\"text\":PROMPT_TEMPLATE.format(view=view)}]}]\n",
        "    inp = processor.apply_chat_template(msgs, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=DTYPE)\n",
        "    L = inp[\"input_ids\"].shape[-1]\n",
        "    with torch.inference_mode(): out = model.generate(**inp, max_new_tokens=300, do_sample=False)\n",
        "    return re.sub(r'\\\\s+', ' ', processor.decode(out[0][L:], skip_special_tokens=True).replace(\"Findings:\",\"\").replace(\"Impression:\",\"\")).strip()\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "GT_COL = \"Ground_Truth\" if \"Ground_Truth\" in df.columns else \"text\"\n",
        "IMG_COL = \"Image_Path\" if \"Image_Path\" in df.columns else None\n",
        "rows_out = []\n",
        "for idx, row in tqdm(df.head(50).iterrows(), total=min(50,len(df))):\n",
        "    path, view = (row.get(IMG_COL), row.get(\"View\",\"PA\")) if IMG_COL else (None, \"PA\")\n",
        "    if not path:\n",
        "        for c, v in [(\"PA\",\"PA\"),(\"AP\",\"AP\"),(\"Lateral\",\"Lateral\")]:\n",
        "            if c in df.columns and (p := get_single_image_path(row.get(c))): path, view = p, v; break\n",
        "    if not path: continue\n",
        "    gt = str(row.get(GT_COL) or \"\").strip()\n",
        "    if not gt or gt.startswith(\"You are\"): continue\n",
        "    rep = generate_report(model, processor, path, view)\n",
        "    rows_out.append({\"subject_id\":row[\"subject_id\"],\"View\":view,\"Image_Path\":path,\"Ground_Truth\":gt,\"Generated_Report\":rep})\n",
        "df_sub = pd.DataFrame(rows_out)\n",
        "print(f\"生成 {len(df_sub)} 条\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 8: RadGraph F1 评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from radgraph import F1RadGraph\n",
        "\n",
        "refs = df_sub[\"Ground_Truth\"].fillna(\"\").tolist()\n",
        "hyps = df_sub[\"Generated_Report\"].fillna(\"\").tolist()\n",
        "f1radgraph = F1RadGraph(reward_level=\"all\", model_type=\"modern-radgraph-xl\")\n",
        "mean_reward, _, _, _ = f1radgraph(hyps=hyps, refs=refs)\n",
        "rg_e, rg_er, rg_er_bar = mean_reward\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"W4A8 RadGraph F1 分数\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"RG_E:        {float(rg_e)*100:.2f}\")\n",
        "print(f\"RG_ER:       {float(rg_er)*100:.2f}\")\n",
        "print(f\"RG_ER_bar:   {float(rg_er_bar)*100:.2f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "W4A8_SCORES = {\"rg_e\": float(rg_e), \"rg_er\": float(rg_er), \"rg_er_bar\": float(rg_er_bar)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 9: 与原始模型对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "orig_path = \"/kaggle/working/original_scores.json\"\n",
        "if os.path.exists(orig_path):\n",
        "    with open(orig_path) as f:\n",
        "        orig = json.load(f)\n",
        "    o_scores = orig.get(\"scores\", {})\n",
        "    o_gpu = orig.get(\"gpu_gb\", 0)\n",
        "    print(\"=\" * 55)\n",
        "    print(\"W4A8 与原始模型对比\")\n",
        "    print(\"-\" * 55)\n",
        "    print(f\"{'指标':<12} {'原始':>10} {'W4A8':>10} {'变化':>10}\")\n",
        "    for k in [\"rg_e\", \"rg_er\", \"rg_er_bar\"]:\n",
        "        o = o_scores.get(k, 0) * 100\n",
        "        q = W4A8_SCORES.get(k, 0) * 100\n",
        "        delta = q - o\n",
        "        print(f\"{k:<12} {o:>9.2f} {q:>9.2f} {delta:>+9.2f}\")\n",
        "    print(f\"{'GPU (GB)':<12} {o_gpu:>9.2f} {W4A8_GPU_GB:>9.2f} {W4A8_GPU_GB-o_gpu:>+9.2f}\")\n",
        "    print(\"=\" * 55)\n",
        "else:\n",
        "    print(\"未找到 original_scores.json，请先运行 01 原始模型 Notebook\")\n",
        "\n",
        "df_sub.to_csv(\"/kaggle/working/w4a8_results.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
