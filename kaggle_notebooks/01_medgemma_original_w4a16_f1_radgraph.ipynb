{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 1: 安装（必须先运行，transformers 4.44 装到独立目录，避免与系统 5.x 冲突）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 0: 环境设置（Kaggle P100 / Colab）\n",
        "\n",
        "- **P100 兼容**：Kaggle 预装 PyTorch cu128 不支持 P100 (sm_60)，需先运行下方安装 Cell 安装 PyTorch cu118\n",
        "- **安装后**：若仍报 GPU 不兼容，请 **Restart Session** 后重新 Run All\n",
        "- P100 不支持 BF16，自动用 FP16\n",
        "- **pip 的 dependency conflicts 警告可忽略**：transformers 装在独立目录，不影响系统包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装 transformers 4.44 到独立目录，避免与系统 5.x 冲突（不降级 numpy/pandas，减少依赖冲突）\n",
        "import sys\n",
        "import os\n",
        "TF_ENV = \"/content/tf_env\" if os.path.exists(\"/content\") else \"/kaggle/working/tf_env\"\n",
        "# 移除可能残留的旧路径，确保加载本次安装\n",
        "for p in [\"/kaggle/working/transformers_4.46\", \"/content/transformers_4.46\"]:\n",
        "    if p in sys.path:\n",
        "        sys.path.remove(p)\n",
        "# 先安装 torch+torchvision 到 TF_ENV，确保版本匹配（避免 torchvision::nms 报错）\n",
        "if os.path.exists(\"/kaggle\"):\n",
        "    !pip install --target $TF_ENV torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu118 -q 2>&1 | grep -vE \"dependency conflicts|incompatible\" || true\n",
        "else:\n",
        "    !pip install --target $TF_ENV torch==2.7.1 torchvision==0.22.1 -q 2>&1 | grep -vE \"dependency conflicts|incompatible\" || true\n",
        "# 分两步安装：先 tokenizers 0.20，再 transformers（避免 pip 依赖冲突）\n",
        "!pip install --target $TF_ENV --no-cache-dir -q \"tokenizers>=0.20,<0.21\" 2>&1 | grep -vE \"dependency conflicts|incompatible\" || true\n",
        "!pip install --target $TF_ENV --no-cache-dir -q transformers==4.44.2 radgraph 2>&1 | grep -vE \"dependency conflicts|incompatible\" || true\n",
        "!pip install -q pillow jinja2 2>&1 | grep -vE \"dependency conflicts|incompatible\" || true\n",
        "\n",
        "# 必须优先加载 TF_ENV，否则会用到系统 transformers\n",
        "sys.path.insert(0, TF_ENV)\n",
        "for k in list(sys.modules.keys()):\n",
        "    if k == \"transformers\" or k.startswith(\"transformers.\"):\n",
        "        del sys.modules[k]\n",
        "import transformers\n",
        "print(f\"✓ transformers: {transformers.__version__} from {transformers.__file__}\")\n",
        "print(\"✅ 安装完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 1: 原始 MedGemma (W4A16/FP16) + F1 + RadGraph\n",
        "\n",
        "## 说明\n",
        "- **W4A16**：4-bit 权重（此处为基线，实际为 FP16/BF16 全精度）+ 16-bit 激活\n",
        "- 本 Notebook 为**原始模型**基线，用于与 W4A4、W4A8 量化版本对比\n",
        "\n",
        "## Kaggle 输入\n",
        "- **Add Input** 添加 `mimic-cxr-dataset`（含 official_data_iccv_final/files/ 图片）\n",
        "- **mimic_eval_single_image_final_233.csv**：可单独建数据集上传，或放入 mimic-cxr-dataset 根目录"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 2.5: HuggingFace 登录（MedGemma 1.5 为 gated 模型，需授权）\n",
        "\n",
        "- 在 [模型页](https://huggingface.co/google/medgemma-1.5-4b-it) 申请访问\n",
        "- Kaggle：Add-ons → Secrets 添加 secret，Label 填 `zhuxiruimedgamma`，Value 填你的 HuggingFace token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "from huggingface_hub import login\n",
        "# Kaggle Secrets 中 secret 名称需与下方一致（如 zhuxiruimedgamma 存的是 HF token）\n",
        "user_secrets = UserSecretsClient()\n",
        "tok = user_secrets.get_secret(\"zhuxiruimedgamma\")\n",
        "if tok:\n",
        "    login(token=tok)\n",
        "else:\n",
        "    print(\"未找到 secret，请确认 Kaggle Add-ons → Secrets 已添加 zhuxiruimedgamma（值为 HF token）\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 跳过 pipeline demo（Kaggle 环境有 numpy/scipy 兼容问题），直接运行下方 AutoProcessor 流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline 在 Kaggle 易触发 numpy/scipy 冲突，已跳过。直接运行下方「加载原始 MedGemma」即可\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 2: 环境检查（需先运行上方安装）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(f\"Python: {sys.version}\")\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    USE_BF16 = torch.cuda.get_device_capability(0)[0] >= 8\n",
        "    DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
        "    print(f\"精度: {'BF16' if USE_BF16 else 'FP16 (P100)'}\")\n",
        "else:\n",
        "    DTYPE = torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 3: 导入与路径配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "TF_ENV = \"/content/tf_env\" if os.path.exists(\"/content\") else \"/kaggle/working/tf_env\"\n",
        "for p in [\"/kaggle/working/transformers_4.46\", \"/content/transformers_4.46\"]:\n",
        "    if p in sys.path:\n",
        "        sys.path.remove(p)\n",
        "for k in list(sys.modules.keys()):\n",
        "    if k == \"transformers\" or k.startswith(\"transformers.\"):\n",
        "        del sys.modules[k]\n",
        "if TF_ENV not in sys.path:\n",
        "    sys.path.insert(0, TF_ENV)\n",
        "elif sys.path[0] != TF_ENV:\n",
        "    sys.path.remove(TF_ENV)\n",
        "    sys.path.insert(0, TF_ENV)\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Kaggle 路径：Add Input 添加 mimic-cxr-dataset\n",
        "DATASET_ROOT = \"/kaggle/input/mimic-cxr-dataset/official_data_iccv_final\"\n",
        "CSV_CANDIDATES = [\n",
        "    \"/kaggle/input/mimic-cxr-dataset/mimic_eval_single_image_final_233.csv\",\n",
        "    \"/kaggle/input/mimic-cxr-dataset/official_data_iccv_final/mimic_eval_single_image_final_233.csv\",\n",
        "    \"/kaggle/input/mimic-eval-233/mimic_eval_single_image_final_233.csv\",\n",
        "    \"/kaggle/working/mimic_eval_single_image_final_233.csv\",\n",
        "    \"./mimic_eval_single_image_final_233.csv\",\n",
        "]\n",
        "CSV_PATH = next((p for p in CSV_CANDIDATES if os.path.exists(p)), CSV_CANDIDATES[0])\n",
        "print(f\"CSV: {CSV_PATH}\")\n",
        "print(f\"Dataset: {DATASET_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 4: 加载原始 MedGemma（全精度）并测量 GPU 占用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pass  # 诊断 cell 已移除，已改用 pip 降级方案"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle: 使用 TF_ENV 的 transformers，torch 沿用 env check 已加载的\n",
        "import sys, os\n",
        "TF_ENV = \"/kaggle/working/tf_env\"\n",
        "for p in [\"/kaggle/working/transformers_4.46\"]:\n",
        "    if p in sys.path:\n",
        "        sys.path.remove(p)\n",
        "for k in list(sys.modules.keys()):\n",
        "    if k == \"transformers\" or k.startswith(\"transformers.\"):\n",
        "        del sys.modules[k]\n",
        "if TF_ENV not in sys.path or sys.path[0] != TF_ENV:\n",
        "    if TF_ENV in sys.path:\n",
        "        sys.path.remove(TF_ENV)\n",
        "    sys.path.insert(0, TF_ENV)\n",
        "\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "model_id = \"google/medgemma-1.5-4b-it\"\n",
        "print(f\"加载原始 MedGemma ({DTYPE})...\")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    mem_gb = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "    print(f\"原始模型 GPU 占用: {mem_gb:.2f} GB\")\n",
        "    torch.cuda.reset_peak_memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 5: 图像到报告生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import re\n",
        "\n",
        "PROMPT_TEMPLATE = (\n",
        "    \"You are an expert radiologist. Describe this {view} view chest X-ray. \"\n",
        "    \"Provide a concise report consisting of Findings and Impression. \"\n",
        "    \"Focus on the heart, lungs, mediastinum, pleural space, and bones. \"\n",
        "    \"Do NOT use bullet points, asterisks, or section headers. \"\n",
        "    \"Do NOT include disclaimers or 'AI' warnings. Output pure medical text only.\"\n",
        ")\n",
        "\n",
        "def get_single_image_path(cell_val):\n",
        "    if pd.isna(cell_val): return None\n",
        "    s = str(cell_val).strip().replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace('\"',\"\").split(\",\")[0].strip()\n",
        "    if \"files\" in s: rel = \"files\" + s.split(\"files\",1)[1]\n",
        "    else: rel = s.strip(\"/\")\n",
        "    full = os.path.join(DATASET_ROOT, rel) if not rel.startswith(\"/\") else rel\n",
        "    return full if os.path.exists(full) else None\n",
        "\n",
        "def generate_report(model, processor, img_path, view=\"PA\"):\n",
        "    if not os.path.exists(img_path): return \"\"\n",
        "    try: img = Image.open(img_path).convert(\"RGB\")\n",
        "    except: return \"\"\n",
        "    prompt = PROMPT_TEMPLATE.format(view=view)\n",
        "    msgs = [{\"role\":\"user\",\"content\":[{\"type\":\"image\",\"image\":img},{\"type\":\"text\",\"text\":prompt}]}]\n",
        "    inp = processor.apply_chat_template(msgs, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=DTYPE)\n",
        "    L = inp[\"input_ids\"].shape[-1]\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(**inp, max_new_tokens=300, do_sample=False)\n",
        "    txt = processor.decode(out[0][L:], skip_special_tokens=True)\n",
        "    return re.sub(r'\\s+', ' ', txt.replace(\"Findings:\",\"\").replace(\"Impression:\",\"\")).strip()\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "# Ground_Truth 必须用人类报告(text)，绝不能是 prompt\n",
        "GT_COL = \"Ground_Truth\" if \"Ground_Truth\" in df.columns else \"text\"\n",
        "IMG_COL = \"Image_Path\" if \"Image_Path\" in df.columns else None\n",
        "\n",
        "rows_out = []\n",
        "NUM = min(50, len(df))\n",
        "for idx, row in tqdm(df.head(NUM).iterrows(), total=NUM):\n",
        "    path, view = None, \"PA\"\n",
        "    if IMG_COL:\n",
        "        path, view = row.get(IMG_COL), row.get(\"View\", \"PA\")\n",
        "    else:\n",
        "        for c, v in [(\"PA\",\"PA\"),(\"AP\",\"AP\"),(\"Lateral\",\"Lateral\")]:\n",
        "            if c in df.columns and (p := get_single_image_path(row.get(c))):\n",
        "                path, view = p, v\n",
        "                break\n",
        "    if not path: continue\n",
        "    gt = str(row.get(GT_COL) or \"\").strip()\n",
        "    if not gt or gt.startswith(\"You are\"): continue  # 防止 prompt 当 Ground_Truth\n",
        "    rep = generate_report(model, processor, path, view)\n",
        "    rows_out.append({\"subject_id\":row[\"subject_id\"],\"View\":view,\"Image_Path\":path,\"Ground_Truth\":gt,\"Generated_Report\":rep})\n",
        "\n",
        "df_sub = pd.DataFrame(rows_out)\n",
        "print(f\"生成 {len(df_sub)} 条\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 6: RadGraph F1 评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from radgraph import F1RadGraph\n",
        "import numpy as np\n",
        "\n",
        "refs = df_sub[\"Ground_Truth\"].fillna(\"\").tolist()\n",
        "hyps = df_sub[\"Generated_Report\"].fillna(\"\").tolist()\n",
        "\n",
        "f1radgraph = F1RadGraph(reward_level=\"all\", model_type=\"modern-radgraph-xl\")\n",
        "mean_reward, reward_list, _, _ = f1radgraph(hyps=hyps, refs=refs)\n",
        "rg_e, rg_er, rg_er_bar = mean_reward\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"原始 MedGemma (W4A16/FP16) RadGraph F1 分数\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"RG_E (仅实体):        {float(rg_e)*100:.2f}\")\n",
        "print(f\"RG_ER (实体+关系):   {float(rg_er)*100:.2f}  <- 论文常用\")\n",
        "print(f\"RG_ER_bar (完整):    {float(rg_er_bar)*100:.2f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "ORIGINAL_SCORES = {\"rg_e\": float(rg_e), \"rg_er\": float(rg_er), \"rg_er_bar\": float(rg_er_bar)}\n",
        "ORIGINAL_GPU_GB = torch.cuda.max_memory_allocated(0) / (1024**3) if torch.cuda.is_available() else 0\n",
        "print(f\"\\n原始模型峰值 GPU: {ORIGINAL_GPU_GB:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 7: 释放 GPU 并保存结果（供后续对比）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model\n",
        "del processor\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "print(\"已释放原始模型，GPU 可加载量化模型\")\n",
        "\n",
        "# 保存原始模型生成结果，供 W4A4/W4A8 Notebook 对比\n",
        "df_sub.to_csv(\"/kaggle/working/original_medgemma_results.csv\", index=False)\n",
        "import json\n",
        "with open(\"/kaggle/working/original_scores.json\", \"w\") as f:\n",
        "    json.dump({\"scores\": ORIGINAL_SCORES, \"gpu_gb\": ORIGINAL_GPU_GB}, f)\n",
        "print(\"结果已保存至 /kaggle/working/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 8: 与量化模型对比说明\n",
        "\n",
        "本 Notebook 为**基线**。运行 W4A4、W4A8 Notebook 后，将得到：\n",
        "- 原始: RG_ER ≈ 27–30（MIMIC-CXR 论文值）\n",
        "- W4A4: 预期略降，显存显著减少\n",
        "- W4A8: 预期接近原始，显存减少\n",
        "\n",
        "**重要**：每次只加载一个模型，跑完删除再加载下一个，才能准确对比 GPU 占用。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
