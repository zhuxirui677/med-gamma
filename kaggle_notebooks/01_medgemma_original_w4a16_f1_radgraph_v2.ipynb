{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MedGemma 1.5 原始模型 (W4A16/FP16) + RadGraph F1 评估\n",
        "\n",
        "## 环境说明\n",
        "- **Kaggle P100**：需 PyTorch cu118（预装 cu128 不支持 sm_60）\n",
        "- **transformers 4.47.1**：支持 Gemma3 的稳定版本\n",
        "- **PyTorch**：使用 Kaggle 预装版本（避免网络下载错误）\n",
        "- **运行流程**：安装 → Restart Session → Run All\n",
        "\n",
        "## Kaggle 数据集\n",
        "- **Add Input**：`mimic-cxr-dataset`（含 official_data_iccv_final/files/ 图片）\n",
        "- **CSV**：`mimic_eval_single_image_final_233.csv`（可单独上传或放 mimic-cxr-dataset 根目录）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 1: 安装（必须先运行，完成后 Restart Session）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle P100 环境：使用预装 PyTorch + transformers 4.47.1\n",
        "import subprocess, sys, os, shutil\n",
        "\n",
        "TF_ENV = \"/kaggle/working/tf_env\"\n",
        "CONSTRAINTS = \"/kaggle/working/constraints.txt\"\n",
        "\n",
        "# 清理旧环境\n",
        "if os.path.exists(TF_ENV):\n",
        "    shutil.rmtree(TF_ENV)\n",
        "\n",
        "# 1. 使用 Kaggle 预装 PyTorch（避免网络下载错误）\n",
        "print(\"使用 Kaggle 预装的 PyTorch（通常是 2.x + cu121，兼容 P100）...\")\n",
        "\n",
        "# 2. 创建约束文件（只锁定 transformers 版本）\n",
        "with open(CONSTRAINTS, \"w\") as f:\n",
        "    f.write(\"transformers==4.47.1\\n\")\n",
        "\n",
        "# 3. 使用约束文件安装 transformers 到 TF_ENV（使用清华镜像加速）\n",
        "subprocess.run([\n",
        "    \"pip\", \"install\", \"--target\", TF_ENV, \"-q\", \"-c\", CONSTRAINTS,\n",
        "    \"transformers==4.47.1\", \"pillow<12\", \"jinja2\",\n",
        "    \"-i\", \"https://pypi.tuna.tsinghua.edu.cn/simple\"\n",
        "], check=True)\n",
        "\n",
        "# 4. 单独安装 radgraph 到系统（使用清华镜像）\n",
        "subprocess.run([\n",
        "    \"pip\", \"install\", \"-q\", \"radgraph\",\n",
        "    \"-i\", \"https://pypi.tuna.tsinghua.edu.cn/simple\"\n",
        "], check=True)\n",
        "\n",
        "# 5. 设置路径优先级\n",
        "sys.path.insert(0, TF_ENV)\n",
        "\n",
        "print(\"✅ 安装完成，请立刻 Restart Session，然后 Run All\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 2: HuggingFace 登录（MedGemma 为 gated 模型）\n",
        "\n",
        "- [申请访问](https://huggingface.co/google/medgemma-1.5-4b-it)\n",
        "- Kaggle：Add-ons → Secrets → Label 填 `zhuxirui11`，Value 填你的 HF token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "from huggingface_hub import login\n",
        "\n",
        "try:\n",
        "    tok = UserSecretsClient().get_secret(\"zhuxirui11\")\n",
        "    if tok:\n",
        "        login(token=tok)\n",
        "        print(\"✅ HF 登录成功\")\n",
        "    else:\n",
        "        print(\"⚠️ HUGGINGFACE_TOKEN 为空\")\n",
        "except Exception as e:\n",
        "    print(\"❌ 未配置 zhuxirui11 Secret\")\n",
        "    print(\"\\n请按以下步骤配置：\")\n",
        "    print(\"1. 点击右侧 'Add-ons' → 'Secrets'\")\n",
        "    print(\"2. Label 填写：zhuxirui11\")\n",
        "    print(\"3. Value 填写你的 HF token（从 https://huggingface.co/settings/tokens 获取）\")\n",
        "    print(\"4. 保存后重新运行本 Cell\")\n",
        "    print(\"\\n⚠️ MedGemma 需要先在 https://huggingface.co/google/medgemma-1.5-4b-it 申请访问权限\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 3: 环境检查"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, torch\n",
        "\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    USE_BF16 = torch.cuda.get_device_capability(0)[0] >= 8\n",
        "    DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
        "    print(f\"Precision: {'BF16' if USE_BF16 else 'FP16 (P100)'}\")\n",
        "else:\n",
        "    DTYPE = torch.float32\n",
        "    print(\"⚠️ No GPU, using FP32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 4: 导入与路径配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, gc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 确保 TF_ENV 优先\n",
        "TF_ENV = \"/kaggle/working/tf_env\"\n",
        "if TF_ENV not in sys.path:\n",
        "    sys.path.insert(0, TF_ENV)\n",
        "elif sys.path[0] != TF_ENV:\n",
        "    sys.path.remove(TF_ENV)\n",
        "    sys.path.insert(0, TF_ENV)\n",
        "\n",
        "# Kaggle 数据集路径\n",
        "DATASET_ROOT = \"/kaggle/input/datasets/simhadrisadaram/mimic-cxr-dataset/official_data_iccv_final\"\n",
        "CSV_CANDIDATES = [\n",
        "    \"/kaggle/input/datasets/xiruizhu1111/clean-data/mimic_eval_single_image_final_233.csv\",\n",
        "    \"/kaggle/input/datasets/xiruizhu1111/clean-train-data/mimic_eval_single_image_final_233.csv\",\n",
        "    \"/kaggle/input/clean-data/mimic_eval_single_image_final_233.csv\",\n",
        "    \"/kaggle/working/mimic_eval_single_image_final_233.csv\",\n",
        "]\n",
        "\n",
        "# 查找 CSV 文件\n",
        "CSV_PATH = None\n",
        "for p in CSV_CANDIDATES:\n",
        "    if os.path.exists(p):\n",
        "        CSV_PATH = p\n",
        "        break\n",
        "\n",
        "if not CSV_PATH:\n",
        "    print(\"❌ 未找到 CSV 文件！\")\n",
        "    print(\"尝试的路径：\")\n",
        "    for p in CSV_CANDIDATES:\n",
        "        print(f\"  - {p}\")\n",
        "    print(\"\\n请检查：\")\n",
        "    print(\"1. 是否已添加包含 CSV 的数据集？（右侧 Add Data）\")\n",
        "    print(\"2. CSV 文件名是否为 'mimic_eval_single_image_final_233.csv'？\")\n",
        "    raise FileNotFoundError(\"CSV 文件不存在\")\n",
        "\n",
        "# 验证 transformers 版本（必须是 4.x，不能是 5.x）\n",
        "import transformers\n",
        "tf_ver = transformers.__version__\n",
        "if tf_ver.startswith('5.'):\n",
        "    raise RuntimeError(\n",
        "        f\"❌ transformers {tf_ver} 不兼容！需要 4.47.1\\n\"\n",
        "        f\"解决方法：重新运行 Cell 1 安装，确保看到 'transformers-4.47.1'，然后 Restart Session\"\n",
        "    )\n",
        "print(f\"✅ transformers: {tf_ver}\")\n",
        "\n",
        "print(f\"Dataset: {DATASET_ROOT}\")\n",
        "print(f\"✅ CSV: {CSV_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 4.5: 检查 CSV 结构（首次运行时查看）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 检查 CSV 文件结构\n",
        "import pandas as pd\n",
        "\n",
        "df_check = pd.read_csv(CSV_PATH)\n",
        "\n",
        "print(f\"✅ CSV 文件加载成功！\")\n",
        "print(f\"总行数: {len(df_check)}\")\n",
        "print(f\"\\n列名: {list(df_check.columns)}\")\n",
        "print(f\"\\n前 3 行数据预览：\")\n",
        "print(df_check.head(3))\n",
        "\n",
        "# 检查关键列是否存在\n",
        "print(\"\\n=== 关键列检查 ===\")\n",
        "has_img_path = \"Image_Path\" in df_check.columns\n",
        "has_ground_truth = \"Ground_Truth\" in df_check.columns or \"text\" in df_check.columns\n",
        "has_subject_id = \"subject_id\" in df_check.columns\n",
        "has_view = \"View\" in df_check.columns\n",
        "\n",
        "print(f\"Image_Path 列: {'✅ 存在' if has_img_path else '❌ 不存在（将查找 PA/AP/Lateral 列）'}\")\n",
        "print(f\"Ground_Truth/text 列: {'✅ 存在' if has_ground_truth else '❌ 不存在'}\")\n",
        "print(f\"subject_id 列: {'✅ 存在' if has_subject_id else '⚠️ 不存在'}\")\n",
        "print(f\"View 列: {'✅ 存在' if has_view else '⚠️ 不存在（将默认使用 PA）'}\")\n",
        "\n",
        "if not has_img_path:\n",
        "    # 检查是否有 PA/AP/Lateral 列\n",
        "    view_cols = [c for c in ['PA', 'AP', 'Lateral'] if c in df_check.columns]\n",
        "    if view_cols:\n",
        "        print(f\"\\n找到视图列: {view_cols}\")\n",
        "        print(f\"示例路径: {df_check[view_cols[0]].iloc[0]}\")\n",
        "    else:\n",
        "        print(\"\\n❌ 警告：未找到图片路径列（Image_Path 或 PA/AP/Lateral）\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 5: 加载 MedGemma 原始模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 清除 transformers 缓存（torch 已在 env check 加载，不动）\n",
        "for k in list(sys.modules.keys()):\n",
        "    if k == \"transformers\" or k.startswith(\"transformers.\"):\n",
        "        del sys.modules[k]\n",
        "\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "model_id = \"google/medgemma-1.5-4b-it\"\n",
        "print(f\"Loading MedGemma ({DTYPE})...\")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    mem_gb = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "    print(f\"✅ Model loaded, GPU memory: {mem_gb:.2f} GB\")\n",
        "    torch.cuda.reset_peak_memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 6: 生成报告"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# 简化 prompt（参考 MedGemma 官方示例）\n",
        "PROMPT_TEMPLATE = \"Generate a radiology report for this {view} chest X-ray.\"\n",
        "\n",
        "def fix_image_path(path):\n",
        "    \"\"\"修正 CSV 中的图片路径为实际 Kaggle datasets 路径\"\"\"\n",
        "    if pd.isna(path) or not path:\n",
        "        return None\n",
        "    path = str(path).strip()\n",
        "    \n",
        "    # 如果路径已经是正确的 datasets 路径，直接返回\n",
        "    if path.startswith(\"/kaggle/input/datasets/\"):\n",
        "        return path if os.path.exists(path) else None\n",
        "    \n",
        "    # 修正旧路径：/kaggle/input/mimic-cxr-dataset/... → /kaggle/input/datasets/simhadrisadaram/mimic-cxr-dataset/...\n",
        "    if path.startswith(\"/kaggle/input/mimic-cxr-dataset/\"):\n",
        "        new_path = path.replace(\"/kaggle/input/mimic-cxr-dataset/\", \"/kaggle/input/datasets/simhadrisadaram/mimic-cxr-dataset/\")\n",
        "        return new_path if os.path.exists(new_path) else None\n",
        "    \n",
        "    # 如果是相对路径，拼接 DATASET_ROOT\n",
        "    if not path.startswith(\"/\"):\n",
        "        full = os.path.join(DATASET_ROOT, path)\n",
        "        return full if os.path.exists(full) else None\n",
        "    \n",
        "    return path if os.path.exists(path) else None\n",
        "\n",
        "def get_single_image_path(cell_val):\n",
        "    if pd.isna(cell_val):\n",
        "        return None\n",
        "    s = str(cell_val).strip().replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', \"\").split(\",\")[0].strip()\n",
        "    if \"files\" in s:\n",
        "        rel = \"files\" + s.split(\"files\", 1)[1]\n",
        "    else:\n",
        "        rel = s.strip(\"/\")\n",
        "    full = os.path.join(DATASET_ROOT, rel) if not rel.startswith(\"/\") else rel\n",
        "    return fix_image_path(full)\n",
        "\n",
        "def generate_report(model, processor, img_path, view=\"PA\"):\n",
        "    if not os.path.exists(img_path):\n",
        "        return \"\"\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "    except:\n",
        "        return \"\"\n",
        "    prompt = PROMPT_TEMPLATE.format(view=view)\n",
        "    msgs = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img}, {\"type\": \"text\", \"text\": prompt}]}]\n",
        "    inp = processor.apply_chat_template(msgs, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=DTYPE)\n",
        "    L = inp[\"input_ids\"].shape[-1]\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(\n",
        "            **inp,\n",
        "            max_length=None,  # 禁用默认 max_length\n",
        "            max_new_tokens=300,  # 使用相对长度\n",
        "            min_new_tokens=5,  # 强制最少生成\n",
        "            pad_token_id=0,  # 显式设置 pad token\n",
        "            do_sample=False\n",
        "        )\n",
        "    txt = processor.decode(out[0][L:], skip_special_tokens=True)\n",
        "    return re.sub(r'\\s+', ' ', txt.replace(\"Findings:\", \"\").replace(\"Impression:\", \"\")).strip()\n",
        "\n",
        "# 生成报告\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "GT_COL = \"Ground_Truth\" if \"Ground_Truth\" in df.columns else \"text\"\n",
        "IMG_COL = \"Image_Path\" if \"Image_Path\" in df.columns else None\n",
        "\n",
        "rows_out = []\n",
        "NUM = min(50, len(df))\n",
        "\n",
        "for idx, row in tqdm(df.head(NUM).iterrows(), total=NUM, desc=\"Generating reports\"):\n",
        "    path, view = None, \"PA\"\n",
        "    if IMG_COL:\n",
        "        path = fix_image_path(row.get(IMG_COL))  # 修正路径\n",
        "        view = row.get(\"View\", \"PA\")\n",
        "    else:\n",
        "        for c, v in [(\"PA\", \"PA\"), (\"AP\", \"AP\"), (\"Lateral\", \"Lateral\")]:\n",
        "            if c in df.columns and (p := get_single_image_path(row.get(c))):\n",
        "                path, view = p, v\n",
        "                break\n",
        "    if not path:\n",
        "        continue\n",
        "    gt = str(row.get(GT_COL) or \"\").strip()\n",
        "    if not gt or gt.startswith(\"You are\"):\n",
        "        continue\n",
        "    rep = generate_report(model, processor, path, view)\n",
        "    rows_out.append({\n",
        "        \"subject_id\": row[\"subject_id\"],\n",
        "        \"View\": view,\n",
        "        \"Image_Path\": path,\n",
        "        \"Ground_Truth\": gt,\n",
        "        \"Generated_Report\": rep\n",
        "    })\n",
        "\n",
        "df_sub = pd.DataFrame(rows_out)\n",
        "print(f\"\\n✅ Generated {len(df_sub)} reports\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 7: RadGraph F1 评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from radgraph import F1RadGraph\n",
        "import numpy as np\n",
        "\n",
        "refs = df_sub[\"Ground_Truth\"].fillna(\"\").tolist()\n",
        "hyps = df_sub[\"Generated_Report\"].fillna(\"\").tolist()\n",
        "\n",
        "f1radgraph = F1RadGraph(reward_level=\"all\", model_type=\"modern-radgraph-xl\")\n",
        "mean_reward, reward_list, _, _ = f1radgraph(hyps=hyps, refs=refs)\n",
        "rg_e, rg_er, rg_er_bar = mean_reward\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"原始 MedGemma (W4A16/FP16) RadGraph F1\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"RG_E (Entity):           {float(rg_e)*100:.2f}\")\n",
        "print(f\"RG_ER (Entity+Relation): {float(rg_er)*100:.2f}  ← 论文常用\")\n",
        "print(f\"RG_ER_bar (Complete):    {float(rg_er_bar)*100:.2f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "ORIGINAL_SCORES = {\"rg_e\": float(rg_e), \"rg_er\": float(rg_er), \"rg_er_bar\": float(rg_er_bar)}\n",
        "ORIGINAL_GPU_GB = torch.cuda.max_memory_allocated(0) / (1024**3) if torch.cuda.is_available() else 0\n",
        "print(f\"\\nPeak GPU memory: {ORIGINAL_GPU_GB:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 8: 保存结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# 释放模型\n",
        "del model, processor\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 保存结果\n",
        "df_sub.to_csv(\"/kaggle/working/original_medgemma_results.csv\", index=False)\n",
        "with open(\"/kaggle/working/original_scores.json\", \"w\") as f:\n",
        "    json.dump({\"scores\": ORIGINAL_SCORES, \"gpu_gb\": ORIGINAL_GPU_GB}, f)\n",
        "\n",
        "print(\"✅ 结果已保存至 /kaggle/working/\")\n",
        "print(\"\\n下一步：运行 W4A4/W4A8 Notebook 进行量化对比\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
