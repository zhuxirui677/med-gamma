{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sCsnfhDs6um",
        "outputId": "423fa561-345c-461e-8dbb-677db56c872a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m142.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "Successfully installed transformers-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66S9xYe8P0__",
        "outputId": "e359c028-e331-4666-e837-67c64ed8a765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHxwuNOXuqNl"
      },
      "source": [
        "ä¸‹è½½MIMIC CXR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlmJ0MjrupKF",
        "outputId": "48de0aa6-675f-4811-f3ea-d69d582eb033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ å¼€å§‹åˆ©ç”¨ Colab é«˜é€Ÿç½‘ç»œä¸‹è½½ 18GB æ•°æ®...\n",
            "Using Colab cache for faster access to the 'mimic-cxr-dataset' dataset.\n",
            "âœ… ä¸‹è½½å®Œæˆï¼\n",
            "ğŸ“‚ æ•°æ®å­˜å‚¨è·¯å¾„: /kaggle/input/mimic-cxr-dataset\n",
            "\n",
            "--- æ–‡ä»¶å¤¹å†…å®¹é¢„è§ˆ ---\n",
            "mimic-cxr-dataset/\n",
            "    official_data_iccv_final/\n",
            "        files/\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "print(\"ğŸš€ å¼€å§‹åˆ©ç”¨ Colab é«˜é€Ÿç½‘ç»œä¸‹è½½ 18GB æ•°æ®...\")\n",
        "\n",
        "# 1. è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶è§£å‹ (æ— éœ€é…ç½® API Keyï¼Œkagglehub ç°åœ¨æ”¯æŒå…ç™»å½•ä¸‹è½½å…¬å¼€é›†)\n",
        "# å¦‚æœæç¤ºéœ€è¦ç™»å½•ï¼Œå®ƒä¼šç»™ä½ ä¸€ä¸ªé“¾æ¥ï¼Œç‚¹è¿›å»å¤åˆ¶ Token è´´å›æ¥å³å¯\n",
        "path = kagglehub.dataset_download(\"simhadrisadaram/mimic-cxr-dataset\")\n",
        "\n",
        "print(\"âœ… ä¸‹è½½å®Œæˆï¼\")\n",
        "print(f\"ğŸ“‚ æ•°æ®å­˜å‚¨è·¯å¾„: {path}\")\n",
        "\n",
        "# 2. éªŒè¯ä¸€ä¸‹æ–‡ä»¶åœ¨ä¸åœ¨\n",
        "print(\"\\n--- æ–‡ä»¶å¤¹å†…å®¹é¢„è§ˆ ---\")\n",
        "# çœ‹çœ‹é‡Œé¢æœ‰å“ªäº›æ–‡ä»¶å¤¹\n",
        "for root, dirs, files in os.walk(path):\n",
        "    level = root.replace(path, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    if level > 1: break # åªçœ‹å‰ä¸¤å±‚ï¼Œé˜²æ­¢åˆ·å±"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7WgwIiJS__V"
      },
      "source": [
        "å¯¹é½è·¯å¾„ä¸Šçš„å›¾ç‰‡é“¾æ¥-ç”Ÿæˆstep1æ•°æ®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1XEast0yRhU",
        "outputId": "2280c978-9033-445c-fce6-2471d77a175f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ•°æ®é›†ç›®å½•æ£€æµ‹æ­£å¸¸ã€‚\n",
            "ğŸ“‚ è¯»å– CSV: /content/drive/MyDrive/medgamma/mimic_eval_cleaned.csv\n",
            "ğŸ”„ æ­£åœ¨æ¸…æ´—å¹¶ä¿®æ­£è·¯å¾„...\n",
            "------------------------------\n",
            "âœ… Step 1 ä¿®å¤æˆåŠŸï¼è·¯å¾„å·²å®Œç¾å¯¹é½ã€‚\n",
            "éªŒè¯è·¯å¾„: /kaggle/input/mimic-cxr-dataset/official_data_iccv_final/files/p10/p10075925/s51010496/2d783c8a-492984b7-28aaf571-bfc30156-61ab26f6.jpg\n",
            "å·²ä¿å­˜æ¸…æ´—åçš„æ–‡ä»¶: mimic_eval_ready_step1.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# ==========================================\n",
        "# 1. è®¾ç½®è·¯å¾„ï¼ˆä¿®å¤ï¼šä½¿ç”¨ kagglehub å®é™…ä¸‹è½½è·¯å¾„ï¼‰\n",
        "# ==========================================\n",
        "# kagglehub ä¸‹è½½åä¼šè¿”å›å®é™…è·¯å¾„ï¼Œä½¿ç”¨å˜é‡ path\n",
        "dataset_root = f\"{path}/official_data_iccv_final\"\n",
        "\n",
        "if not os.path.exists(dataset_root):\n",
        "    print(f\"âŒ æœªæ‰¾åˆ°æ•°æ®é›†ç›®å½•: {dataset_root}\")\n",
        "    sys.exit(\"è¯·å…ˆè¿è¡Œ kagglehub ä¸‹è½½ä»£ç ï¼\")\n",
        "else:\n",
        "    print(f\"âœ… æ•°æ®é›†ç›®å½•æ£€æµ‹æ­£å¸¸ã€‚\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. è¯»å– CSV å¹¶ä¿®æ­£è·¯å¾„ (å«å¼ºåŠ›æ¸…æ´—)\n",
        "# ==========================================\n",
        "# âœ… ä¿®å¤ï¼šä½¿ç”¨ 233 ç‰ˆæœ¬ CSV\n",
        "csv_path = \"/content/drive/MyDrive/medgamma/mimic_eval_single_image_final_233.csv\"\n",
        "\n",
        "# Check if the file exists at the specified path\n",
        "if not os.path.exists(csv_path):\n",
        "    # If not found, list directory contents for user to verify\n",
        "    print(f\"âŒ æ‰¾ä¸åˆ° CSV æ–‡ä»¶åœ¨é¢„æœŸè·¯å¾„: {csv_path}\")\n",
        "    print(\"--- /content/drive/MyDrive/medgamma/ ç›®å½•å†…å®¹ ---\")\n",
        "    try:\n",
        "        for item in os.listdir('/content/drive/MyDrive/medgamma/'):\n",
        "            print(item)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Directory not found.\")\n",
        "    print(\"--------------------------------------\")\n",
        "    raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ° csv æ–‡ä»¶: {csv_path}\")\n",
        "\n",
        "print(f\"ğŸ“‚ è¯»å– CSV: {csv_path}\")\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "def fix_image_path(path_in_csv):\n",
        "    \"\"\"\n",
        "    âœ… ä¿®å¤ï¼šå°† 233 CSV ä¸­çš„è·¯å¾„å¯¹é½åˆ° kagglehub ä¸‹è½½çš„å®é™…è·¯å¾„\n",
        "    \"\"\"\n",
        "    if pd.isna(path_in_csv): \n",
        "        return None\n",
        "\n",
        "    # 1. æ¸…æ´—å­—ç¬¦ä¸²\n",
        "    path_str = str(path_in_csv).strip()\n",
        "    path_str = path_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', \"\")\n",
        "\n",
        "    # 2. æå–ä» files/ å¼€å§‹çš„ç›¸å¯¹è·¯å¾„\n",
        "    if 'files' in path_str:\n",
        "        relative = path_str.split('files', 1)[1]\n",
        "        relative = 'files' + relative\n",
        "    else:\n",
        "        relative = path_str.strip('/')\n",
        "\n",
        "    # 3. æ‹¼æ¥å®Œæ•´è·¯å¾„\n",
        "    full_path = os.path.join(dataset_root, relative)\n",
        "    \n",
        "    # 4. éªŒè¯æ–‡ä»¶å­˜åœ¨\n",
        "    return full_path if os.path.exists(full_path) else None\n",
        "\n",
        "# âœ… åº”ç”¨ä¿®æ­£ï¼ˆ233 CSV ä½¿ç”¨ Image_Path åˆ—ï¼‰\n",
        "print(\"ğŸ”„ æ­£åœ¨æ¸…æ´—å¹¶ä¿®æ­£è·¯å¾„...\")\n",
        "if 'Image_Path' in df.columns:\n",
        "    df['Image_Path'] = df['Image_Path'].apply(fix_image_path)\n",
        "else:\n",
        "    # å…¼å®¹æ—§æ ¼å¼ï¼ˆPA/AP/Lateral åˆ—ï¼‰\n",
        "    for col in ['AP', 'PA', 'Lateral']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(fix_image_path)\n",
        "\n",
        "# ==========================================\n",
        "# 3. éªŒè¯è·¯å¾„å¯¹é½ï¼ˆâœ… ä¿®å¤ï¼šæ”¯æŒ Image_Pathï¼‰\n",
        "# ==========================================\n",
        "found_valid = False\n",
        "sample_path = \"\"\n",
        "\n",
        "# ä¼˜å…ˆæ£€æŸ¥ Image_Path åˆ—ï¼ˆ233 CSV æ ¼å¼ï¼‰\n",
        "if 'Image_Path' in df.columns:\n",
        "    valid_series = df['Image_Path'].dropna()\n",
        "    for test_path in valid_series.head(10):\n",
        "        if test_path and os.path.exists(test_path):\n",
        "            found_valid = True\n",
        "            sample_path = test_path\n",
        "            break\n",
        "else:\n",
        "    # å…¼å®¹æ—§æ ¼å¼ï¼ˆPA/AP/Lateralï¼‰\n",
        "    for col in ['PA', 'AP', 'Lateral']:\n",
        "        if col not in df.columns: continue\n",
        "        valid_series = df[col].dropna()\n",
        "        if valid_series.empty: continue\n",
        "        for test_path in valid_series.head(5):\n",
        "            if os.path.exists(test_path):\n",
        "                found_valid = True\n",
        "                sample_path = test_path\n",
        "                break\n",
        "        if found_valid: break\n",
        "\n",
        "print(\"-\" * 30)\n",
        "if found_valid:\n",
        "    print(\"âœ… Step 1 ä¿®å¤æˆåŠŸï¼è·¯å¾„å·²å®Œç¾å¯¹é½ã€‚\")\n",
        "    print(f\"éªŒè¯è·¯å¾„: {sample_path}\")\n",
        "    # ä¿å­˜è¿™ä»½å¹²å‡€çš„æ•°æ®ç»™ Step 2 ç”¨\n",
        "    df.to_csv(\"mimic_eval_ready_step1.csv\", index=False)\n",
        "    print(\"å·²ä¿å­˜æ¸…æ´—åçš„æ–‡ä»¶: mimic_eval_ready_step1.csv\")\n",
        "else:\n",
        "    print(\"âŒ è·¯å¾„å¯¹é½ä¾ç„¶å¤±è´¥ã€‚\")\n",
        "    if not valid_series.empty:\n",
        "        print(f\"ç¨‹åºè®¡ç®—å‡ºçš„è·¯å¾„ (ä»ä¸å­˜åœ¨): {valid_series.iloc[0]}\")\n",
        "        print(\"è¯·æ£€æŸ¥ä½ çš„ CSV é‡Œçš„è·¯å¾„æ ¼å¼æ˜¯å¦éå¸¸ç‰¹æ®Šï¼Œæˆ–è€… dataset_root æ˜¯å¦æ­£ç¡®ã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jinZ64Els6un"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/google/medgemma-1.5-4b-it\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/medgemma-1.5-4b-it)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ğŸ™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfRiSeWds6un"
      },
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ğŸ¤—"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzu_0Cqhulgy"
      },
      "source": [
        "è¯·åœ¨ Colab Secrets ä¸­è®¾ç½® HF_TOKENï¼Œæˆ–è¿è¡Œ: from google.colab import userdata; login(token=userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139,
          "referenced_widgets": [
            "1c58112713d5494fbc7c1cd4e85a935d",
            "35b4f142c7b543d18d1ba5bf184cfa53",
            "5a7e79bac35c45ffa64d68cc7b1ddb4d",
            "4630014ea3d6460882e32981bd695677",
            "d256895df8e844469b87756b3f86e4e4",
            "57d662a77a81431a812673bbfa0f8a97",
            "0dbe9e38a4964c95a2790d6567bd6712",
            "e8ed0c33a31b4b078d79c8e112f91e2a",
            "aef96c6cb98e46f9a5e0226cde2dc6bc",
            "d09a067bc8494674ba91b71c7691c900",
            "fc7d8ad817cf420b934fb853ac15397f",
            "87929da1bb2b45bcab772db5ab79048d",
            "bb9d038b63fe488cb912b45891e8cbab",
            "0cdad0076ea04e09bbc2e748d69c9e31",
            "498fd95b7736478b88187f86fc967cb2",
            "e0a93e180112406b8de70d94172152f5",
            "577c48d835e04255adec5775498ef708",
            "95e59ba9483a4acbb9562a86565d5db1",
            "7639255bbe9449cc8af5ebfdc142b826",
            "3f0a32785322452baf37aa2af279d3e6"
          ]
        },
        "id": "WsZkOAers6un",
        "outputId": "39a1c963-d3c2-48ff-c878-585fce8ba421"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c58112713d5494fbc7c1cd4e85a935d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J68xFyo5uwg6",
        "outputId": "77482acd-195b-4478-8ff1-bb3b738d623e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ¨¡å‹å·²åœ¨å†…å­˜ä¸­ï¼Œç›´æ¥ä½¿ç”¨ã€‚\n",
            "ğŸš€ å¼€å§‹æ‰¹é‡ç”ŸæˆæŠ¥å‘Š (å•å›¾æ¨¡å¼)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/342 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "  0%|          | 1/342 [00:04<23:44,  4.18s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "  1%|          | 2/342 [00:08<23:45,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import ast\n",
        "import re\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# ==========================================\n",
        "# 0. ç¡®ä¿æ¨¡å‹å·²åŠ è½½ (é˜²æ­¢ modelæœªå®šä¹‰æŠ¥é”™)\n",
        "# ==========================================\n",
        "model_id = \"google/medgemma-1.5-4b-it\"\n",
        "\n",
        "if 'model' not in globals():\n",
        "    print(f\"ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹: {model_id}...\")\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    model = AutoModelForImageTextToText.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "else:\n",
        "    print(\"âœ… æ¨¡å‹å·²åœ¨å†…å­˜ä¸­ï¼Œç›´æ¥ä½¿ç”¨ã€‚\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. å®šä¹‰ç¼ºå¤±çš„ç”Ÿæˆå‡½æ•° (æ ¸å¿ƒè¡¥ä¸)\n",
        "# ==========================================\n",
        "def generate_one_report(image_path, view_position):\n",
        "    \"\"\"\n",
        "    è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼Œè°ƒç”¨æ¨¡å‹ç”ŸæˆæŠ¥å‘Š\n",
        "    \"\"\"\n",
        "    # æç¤ºè¯\n",
        "    prompt_text = (\n",
        "        f\"You are an expert radiologist. Describe this {view_position} view chest X-ray. \"\n",
        "        \"Provide a concise report consisting of Findings and Impression. \"\n",
        "        \"Focus on the heart, lungs, mediastinum, pleural space, and bones. \"\n",
        "        \"Do NOT use bullet points, asterisks, or section headers. \"\n",
        "        \"Do NOT include disclaimers or 'AI' warnings. \"\n",
        "        \"Output pure medical text only.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # åŠ è½½å›¾ç‰‡\n",
        "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return f\"ERROR_IMAGE_LOAD: {e}\"\n",
        "\n",
        "    # æ„å»ºè¾“å…¥\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": pil_image},\n",
        "                {\"type\": \"text\", \"text\": prompt_text}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, tokenize=True,\n",
        "        return_dict=True, return_tensors=\"pt\"\n",
        "    ).to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    # æ¨ç†\n",
        "    with torch.inference_mode():\n",
        "        generation = model.generate(**inputs, max_new_tokens=300, do_sample=False)\n",
        "        generation = generation[0][input_len:]\n",
        "\n",
        "    # âœ… è§£ç å¹¶å¼ºåŠ›æ¸…æ´—\n",
        "    raw_text = processor.decode(generation, skip_special_tokens=True)\n",
        "    clean_text = raw_text.replace(\"Findings:\", \"\").replace(\"Impression:\", \"\").replace(\"**\", \"\").replace(\"*\", \"\")\n",
        "    import re\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "    return clean_text\n",
        "\n",
        "# ==========================================\n",
        "# 2. ä½ çš„è·¯å¾„æå–å·¥å…· (ä¿æŒä¸å˜)\n",
        "# ==========================================\n",
        "def get_single_image_path(cell_value):\n",
        "    if pd.isna(cell_value): return None\n",
        "\n",
        "    val_str = str(cell_value).strip()\n",
        "    target_path = \"\"\n",
        "\n",
        "    # å¤„ç†åˆ—è¡¨å­—ç¬¦ä¸² ['...']\n",
        "    if val_str.startswith('[') and val_str.endswith(']'):\n",
        "        try:\n",
        "            path_list = ast.literal_eval(val_str)\n",
        "            if len(path_list) > 0:\n",
        "                target_path = path_list[0] # å¼ºåˆ¶å–ç¬¬ä¸€ä¸ª\n",
        "            else:\n",
        "                return None\n",
        "        except:\n",
        "            target_path = val_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', \"\").split(',')[0]\n",
        "    else:\n",
        "        target_path = val_str\n",
        "\n",
        "    # æ‹¼æ¥è·¯å¾„\n",
        "    dataset_root = \"/kaggle/input/mimic-cxr-dataset/official_data_iccv_final\"\n",
        "    clean_relative = str(target_path).strip().strip(\"'\").strip('\"')\n",
        "\n",
        "    if 'files' in clean_relative:\n",
        "        clean_relative = 'files' + clean_relative.split('files', 1)[1]\n",
        "    else:\n",
        "        clean_relative = clean_relative.strip('/')\n",
        "\n",
        "    full_path = os.path.join(dataset_root, clean_relative)\n",
        "\n",
        "    if os.path.exists(full_path):\n",
        "        return full_path\n",
        "    return None\n",
        "\n",
        "# ==========================================\n",
        "# 3. ä½ çš„ä¸»å¾ªç¯\n",
        "# ==========================================\n",
        "# ç¡®ä¿ df å­˜åœ¨\n",
        "if 'df' not in globals():\n",
        "    # å¦‚æœ df ä¸¢äº†ï¼Œå°è¯•é‡æ–°è¯»å–\n",
        "    if os.path.exists(\"mimic_eval_ready_step1.csv\"):\n",
        "        df = pd.read_csv(\"mimic_eval_ready_step1.csv\")\n",
        "    else:\n",
        "        df = pd.read_csv(\"mimic_eval_cleaned.csv\")\n",
        "\n",
        "print(\"ğŸš€ å¼€å§‹æ‰¹é‡ç”ŸæˆæŠ¥å‘Š (å•å›¾æ¨¡å¼)...\")\n",
        "results = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    try:\n",
        "        # âœ… ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ Image_Path åˆ—ï¼ˆ233 CSV æ ¼å¼ï¼‰\n",
        "        final_img_path = None\n",
        "        used_view = None\n",
        "\n",
        "        if 'Image_Path' in df.columns:\n",
        "            # 233 CSV æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨ Image_Path åˆ—\n",
        "            final_img_path = row.get('Image_Path')\n",
        "            used_view = row.get('View', 'PA')  # ä» View åˆ—è·å–ï¼Œé»˜è®¤ PA\n",
        "        else:\n",
        "            # å…¼å®¹æ—§æ ¼å¼ï¼šPA/AP/Lateral åˆ—\n",
        "            path = get_single_image_path(row.get('PA'))\n",
        "            if path: final_img_path, used_view = path, 'PA'\n",
        "            \n",
        "            if not final_img_path:\n",
        "                path = get_single_image_path(row.get('AP'))\n",
        "                if path: final_img_path, used_view = path, 'AP'\n",
        "            \n",
        "            if not final_img_path:\n",
        "                path = get_single_image_path(row.get('Lateral'))\n",
        "                if path: final_img_path, used_view = path, 'Lateral'\n",
        "\n",
        "        # è·³è¿‡æ— æ•ˆè·¯å¾„\n",
        "        if not final_img_path or not os.path.exists(final_img_path): \n",
        "            continue\n",
        "\n",
        "        # --- ç”Ÿæˆ ---\n",
        "        raw_report = generate_one_report(final_img_path, used_view)\n",
        "\n",
        "        # âœ… Ground_Truth ä»æ­£ç¡®çš„åˆ—è¯»å–\n",
        "        gt_col = 'Ground_Truth' if 'Ground_Truth' in df.columns else 'text'\n",
        "        ground_truth = str(row.get(gt_col, ''))\n",
        "\n",
        "        # --- ä¿å­˜ ---\n",
        "        results.append({\n",
        "            \"subject_id\": row.get('subject_id', idx),\n",
        "            \"View\": used_view,\n",
        "            \"Image_Path\": final_img_path,\n",
        "            \"Ground_Truth\": ground_truth,\n",
        "            \"Generated_Report\": raw_report  # å·²åœ¨ generate_one_report ä¸­æ¸…æ´—\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "# ä¿å­˜ç»“æœ\n",
        "output_filename = \"mimic_eval_single_image_final.csv\"\n",
        "pd.DataFrame(results).to_csv(output_filename, index=False)\n",
        "print(f\"\\nğŸ‰ å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {output_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hheKkwiUGyd"
      },
      "source": [
        "ä¸Šä¸€ä¸ªæ“ä½œä¼šä¸¢å¤±æ•°æ®ï¼Œè¿™ä¸ªä¼šæ‰“å°è·³è¿‡æ•°æ®çš„åŸå› "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "collapsed": true,
        "id": "9OO46oA8T64m",
        "outputId": "5982739f-271c-4858-8916-1c72bc2ae417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ¨¡å‹å·²åœ¨å†…å­˜ä¸­ï¼Œç›´æ¥ä½¿ç”¨ã€‚\n",
            "ğŸš€ å¼€å§‹æ‰¹é‡ç”ŸæˆæŠ¥å‘Š (æ€»è¡Œæ•°: 342)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/342 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "  0%|          | 0/342 [00:05<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2705156220.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# --- ç”ŸæˆæŠ¥å‘Š ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mraw_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_one_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# æ£€æŸ¥ç”Ÿæˆæ˜¯å¦è¿”å›äº†é”™è¯¯ä¿¡æ¯ï¼ˆå¯¹åº” generate_one_report é‡Œçš„ try-exceptï¼‰\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2705156220.py\u001b[0m in \u001b[0;36mgenerate_one_report\u001b[0;34m(image_path, view_position)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# æ¨ç†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mgeneration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mgeneration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2637\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2638\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2640\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2841\u001b[0m                 \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_model_for_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m             \u001b[0mprefill_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m             model_kwargs = self._update_model_kwargs_for_generation(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \"\"\"\n\u001b[0;32m-> 1080\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1081\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, **lm_kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m             )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         outputs = self.language_model(\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    999\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    585\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_feedforward_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_feedforward_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/compressed_tensors/quantization/lifecycle/forward.py\u001b[0m in \u001b[0;36mwrapped_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;31m# calibrate and (fake) quantize weights when applicable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0munquantized_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             self.weight.data = forward_quantize(\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/compressed_tensors/quantization/lifecycle/forward.py\u001b[0m in \u001b[0;36mforward_quantize\u001b[0;34m(module, value, base_name, args)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mzero_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{base_name}_zero_point\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m     return fake_quantize(\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/compressed_tensors/quantization/lifecycle/forward.py\u001b[0m in \u001b[0;36mfake_quantize\u001b[0;34m(x, scale, zero_point, args, g_idx, global_scale)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfake\u001b[0m \u001b[0mquantized\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m     return _process_quantization(\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import ast\n",
        "import re\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# ==========================================\n",
        "# 0. ç¡®ä¿æ¨¡å‹å·²åŠ è½½ (é˜²æ­¢ modelæœªå®šä¹‰æŠ¥é”™)\n",
        "# ==========================================\n",
        "model_id = \"google/medgemma-1.5-4b-it\"\n",
        "\n",
        "if 'model' not in globals():\n",
        "    print(f\"ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹: {model_id}...\")\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    model = AutoModelForImageTextToText.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "else:\n",
        "    print(\"âœ… æ¨¡å‹å·²åœ¨å†…å­˜ä¸­ï¼Œç›´æ¥ä½¿ç”¨ã€‚\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. å®šä¹‰ç¼ºå¤±çš„ç”Ÿæˆå‡½æ•° (æ ¸å¿ƒè¡¥ä¸)\n",
        "# ==========================================\n",
        "def generate_one_report(image_path, view_position):\n",
        "    \"\"\"\n",
        "    è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼Œè°ƒç”¨æ¨¡å‹ç”ŸæˆæŠ¥å‘Š\n",
        "    \"\"\"\n",
        "    # æç¤ºè¯\n",
        "    prompt_text = (\n",
        "        f\"You are an expert radiologist. Describe this {view_position} view chest X-ray. \"\n",
        "        \"Provide a concise report consisting of Findings and Impression. \"\n",
        "        \"Focus on the heart, lungs, mediastinum, pleural space, and bones. \"\n",
        "        \"Do NOT use bullet points, asterisks, or section headers. \"\n",
        "        \"Do NOT include disclaimers or 'AI' warnings. \"\n",
        "        \"Output pure medical text only.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # åŠ è½½å›¾ç‰‡\n",
        "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return f\"ERROR_IMAGE_LOAD: {e}\"\n",
        "\n",
        "    # æ„å»ºè¾“å…¥\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": pil_image},\n",
        "                {\"type\": \"text\", \"text\": prompt_text}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, tokenize=True,\n",
        "        return_dict=True, return_tensors=\"pt\"\n",
        "    ).to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    # æ¨ç†\n",
        "    with torch.inference_mode():\n",
        "        generation = model.generate(**inputs, max_new_tokens=300, do_sample=False)\n",
        "        generation = generation[0][input_len:]\n",
        "\n",
        "    # âœ… è§£ç å¹¶å¼ºåŠ›æ¸…æ´—\n",
        "    raw_text = processor.decode(generation, skip_special_tokens=True)\n",
        "    clean_text = raw_text.replace(\"Findings:\", \"\").replace(\"Impression:\", \"\").replace(\"**\", \"\").replace(\"*\", \"\")\n",
        "    import re\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "    return clean_text\n",
        "\n",
        "# ==========================================\n",
        "# 2. ä½ çš„è·¯å¾„æå–å·¥å…· (ä¿æŒä¸å˜)\n",
        "# ==========================================\n",
        "def get_single_image_path(cell_value):\n",
        "    if pd.isna(cell_value): return None\n",
        "\n",
        "    val_str = str(cell_value).strip()\n",
        "    target_path = \"\"\n",
        "\n",
        "    # å¤„ç†åˆ—è¡¨å­—ç¬¦ä¸² ['...']\n",
        "    if val_str.startswith('[') and val_str.endswith(']'):\n",
        "        try:\n",
        "            path_list = ast.literal_eval(val_str)\n",
        "            if len(path_list) > 0:\n",
        "                target_path = path_list[0] # å¼ºåˆ¶å–ç¬¬ä¸€ä¸ª\n",
        "            else:\n",
        "                return None\n",
        "        except:\n",
        "            target_path = val_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', \"\").split(',')[0]\n",
        "    else:\n",
        "        target_path = val_str\n",
        "\n",
        "    # æ‹¼æ¥è·¯å¾„\n",
        "    dataset_root = \"/kaggle/input/mimic-cxr-dataset/official_data_iccv_final\"\n",
        "    clean_relative = str(target_path).strip().strip(\"'\").strip('\"')\n",
        "\n",
        "    if 'files' in clean_relative:\n",
        "        clean_relative = 'files' + clean_relative.split('files', 1)[1]\n",
        "    else:\n",
        "        clean_relative = clean_relative.strip('/')\n",
        "\n",
        "    full_path = os.path.join(dataset_root, clean_relative)\n",
        "\n",
        "    if os.path.exists(full_path):\n",
        "        return full_path\n",
        "    return None\n",
        "\n",
        "# ==========================================\n",
        "# 3. ä½ çš„ä¸»å¾ªç¯\n",
        "# ==========================================\n",
        "# ç¡®ä¿ df å­˜åœ¨\n",
        "if 'df' not in globals():\n",
        "    if os.path.exists(\"mimic_eval_ready_step1.csv\"):\n",
        "        df = pd.read_csv(\"mimic_eval_ready_step1.csv\")\n",
        "    else:\n",
        "        # Update to the found path\n",
        "        df = pd.read_csv(\"/content/drive/MyDrive/medgamma/mimic_eval_single_image_final_233.csv\")\n",
        "\n",
        "print(f\"ğŸš€ å¼€å§‹æ‰¹é‡ç”ŸæˆæŠ¥å‘Š (æ€»è¡Œæ•°: {len(df)})...\")\n",
        "results = []\n",
        "skipped_count = 0\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    try:\n",
        "        # --- æ‰¾å›¾é€»è¾‘ ---\n",
        "        final_img_path = None\n",
        "        used_view = None\n",
        "\n",
        "        # ä¼˜å…ˆçº§ï¼šPA -> AP -> Lateral\n",
        "        # 1. å°è¯• PA\n",
        "        path = get_single_image_path(row.get('PA'))\n",
        "        if path:\n",
        "            final_img_path, used_view = path, 'PA'\n",
        "\n",
        "        # 2. å¦‚æœ PA æ²¡æ‰¾åˆ°ï¼Œå°è¯• AP\n",
        "        if not final_img_path:\n",
        "            path = get_single_image_path(row.get('AP'))\n",
        "            if path: final_img_path, used_view = path, 'AP'\n",
        "\n",
        "        # 3. å¦‚æœå‰ä¸¤ä¸ªéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯• Lateral\n",
        "        if not final_img_path:\n",
        "            path = get_single_image_path(row.get('Lateral'))\n",
        "            if path: final_img_path, used_view = path, 'Lateral'\n",
        "\n",
        "        # --- æ£€æŸ¥ç‚¹ï¼šå¦‚æœæœ€ç»ˆæ²¡æ‰¾åˆ°æœ‰æ•ˆè·¯å¾„ ---\n",
        "        if not final_img_path:\n",
        "            print(f\"âš ï¸ Skipping Index {idx} (Subject {row['subject_id']}): åŸå›  - åœ¨æœ¬åœ°ç£ç›˜æ‰¾ä¸åˆ°ä»»ä½•æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶ã€‚\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # --- ç”ŸæˆæŠ¥å‘Š ---\n",
        "        raw_report = generate_one_report(final_img_path, used_view)\n",
        "\n",
        "        # æ£€æŸ¥ç”Ÿæˆæ˜¯å¦è¿”å›äº†é”™è¯¯ä¿¡æ¯ï¼ˆå¯¹åº” generate_one_report é‡Œçš„ try-exceptï¼‰\n",
        "        if \"ERROR_IMAGE_LOAD\" in raw_report:\n",
        "            print(f\"âŒ Skipping Index {idx} (Subject {row['subject_id']}): åŸå›  - å›¾ç‰‡åŠ è½½å¤±è´¥ ({raw_report})\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # --- æ¸…æ´— ---\n",
        "        clean_report = raw_report.replace(\"Findings:\", \"\").replace(\"Impression:\", \"\")\n",
        "        clean_report = re.sub(r'\\s+', ' ', clean_report).strip()\n",
        "\n",
        "        # --- ä¿å­˜æˆåŠŸç»“æœ ---\n",
        "        results.append({\n",
        "            \"subject_id\": row['subject_id'],\n",
        "            \"View\": used_view,\n",
        "            \"Image_Path\": final_img_path,\n",
        "            \"Ground_Truth\": str(row['text']),\n",
        "            \"Generated_Report\": clean_report\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        # æ•è·æ¨ç†è¿‡ç¨‹ä¸­çš„å…¶ä»–å¼‚å¸¸ï¼ˆå¦‚æ˜¾å­˜æº¢å‡ºç­‰ï¼‰\n",
        "        print(f\"ğŸ”¥ Skipping Index {idx} (Subject {row['subject_id']}): åŸå›  - è¿è¡Œæ—¶å´©æºƒ: {e}\")\n",
        "        skipped_count += 1\n",
        "        continue\n",
        "\n",
        "# ä¿å­˜ç»“æœ\n",
        "output_filename = \"mimic_eval_single_image_final.csv\"\n",
        "pd.DataFrame(results).to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\nâœ… å¤„ç†å®Œæˆï¼\")\n",
        "print(f\"ğŸ“Š ç»Ÿè®¡ç»“æœ:\")\n",
        "print(f\"   - åŸå§‹æ€»æ•°: {len(df)}\")\n",
        "print(f\"   - æˆåŠŸç”Ÿæˆ: {len(results)}\")\n",
        "print(f\"   - è·³è¿‡æ€»æ•°: {skipped_count}\")\n",
        "print(f\"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyZzkUlfzPeI"
      },
      "source": [
        "æ“ä½œï¼š\n",
        "å°è¯•æ”¹å˜text\n",
        "æ€è€ƒï¼štokenæœ€å°æ˜¯å¤šå°‘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599,
          "referenced_widgets": [
            "eccd8d3320fe47f6b6600b1f2e67fc3c",
            "8d920a94680141b0b7c5460f809016a2",
            "b292fb035b1247ec93ea742c7d60ac9a",
            "ceee0be3171941469e7a9c59af7141e9",
            "c5f111415e5a48018e56915febe4eeff",
            "fd7e3945d9544cc69771b54a79a196a2",
            "74b9e9370ec3416588aee68ebc5d228a",
            "fc9d1aa163db43b497e1faf7c41aa58b",
            "ac22c8d24fb5432e83be55ed08840d1e",
            "4cd7b855627743dcbeb7e1f43194d9f7",
            "1daa82817455488b97159af2374e698f"
          ]
        },
        "collapsed": true,
        "id": "NSFD6MR6wRCe",
        "outputId": "f3d37cc5-275b-4adf-cddf-ceb5b80f389a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eccd8d3320fe47f6b6600b1f2e67fc3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided chest X-ray, here's a description:\n",
            "\n",
            "**Overall Impression:**\n",
            "\n",
            "The X-ray shows a chest view with a normal heart size and mediastinal structures. There are some areas of increased opacity in the lungs, particularly in the left lung.\n",
            "\n",
            "**Specific Findings:**\n",
            "\n",
            "*   **Heart:** The heart size appears within normal limits.\n",
            "*   **Lungs:** There are some areas of increased opacity in the lungs, particularly in the left lung. These could represent consolidation, nodules, or other abnormalities.\n",
            "*   **Mediastinum:** The mediastinum appears normal in width and contour.\n",
            "*   **Bones:** The ribs and clavicles appear intact.\n",
            "*   **Diaphragm:** The diaphragm is visible and appears normal.\n",
            "*   **Soft Tissues/Possible Considerations:**\n",
            "    *   The increased opacity in the lungs warrants further investigation.\n",
            "    *   The patient's age and sex are not provided, which can influence the interpretation of the X-ray.\n",
            "    *   The clinical history is not provided, which is important for context.\n",
            "    *   Comparison to prior studies would be helpful to assess any changes.\n",
            "\n",
            "**Disclaimer:** This is a preliminary interpretation based on the provided image. A definitive diagnosis requires a complete clinical evaluation, including the patient's history, physical examination, and potentially additional imaging studies.\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "*   Further evaluation is needed to determine the cause of the increased opacity in the lungs.\n",
            "*   Consider a CT scan of the chest for a more detailed assessment.\n",
            "*   Obtain a complete clinical history and compare to prior studies if available.\n",
            "\n",
            "**Important Note:** I am an AI and cannot provide medical advice. This description is for informational purposes only and should not be used to make any medical decisions. Always consult with a qualified healthcare professional for diagnosis and treatment.\n"
          ]
        }
      ],
      "source": [
        "# Make sure to install the accelerate library first via `pip install accelerate`\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "model_id = \"google/medgemma-1.5-4b-it\"\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\n",
        "image_url = \"https://storage.googleapis.com/kagglesdsdata/datasets/6850723/11020923/official_data_iccv_final/files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20260204%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20260204T073745Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=20c9cc4cf99f05fc1d6452ee3e7a003c611ab7eb576691d0626884226015bb4e72aa1ff1be53d5739eeeaa7a52b2afc2824cfb8243d85d15adc23c5edd0c40946fa8882a39ef0281f1e5347596a94d6a8dd087f7d0b13db4013683c9d6f1c4393065b9f3dee11afe677cd381431c90c9438a641a64480f4f372b7e25c485b356fbc24f904be90ff23093e96e2e4a2deaaedf63b1aa796752f0eb9bb161b2a6652c52ab38639aa4fe61bf17ccc393ff6d3e43d0bdea89b63c859408b26d6d6fb4e8d39ce718c64f515f65b647f3515f43eec0e51f10f3ac29010ec61a87544c258348664749e763adaf63f07a3a038f547a128da2de6f32f6354947d4d9447846\"\n",
        "image = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": \"Describe this X-rayï¼Œfrom Overall Impression/Specific Findingsï¼šHeartã€Lungsã€Mediastinumã€Bonesã€Diaphragmã€Soft Tissues/Possible Considerationsï¼šAge and Sexã€Clinical Historyã€Comparison to Prior Studies\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages, add_generation_prompt=True, tokenize=True,\n",
        "    return_dict=True, return_tensors=\"pt\"\n",
        ").to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    generation = model.generate(**inputs, max_new_tokens=2000, do_sample=False)\n",
        "    generation = generation[0][input_len:]\n",
        "\n",
        "decoded = processor.decode(generation, skip_special_tokens=True)\n",
        "print(decoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG6vjWNgTcy4"
      },
      "source": [
        "# ä¹‹å‰çš„æµ‹è¯•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "343c030b347041ffa2ad23b9793154d0",
            "d4d0b7888c8c4da8ab3a0bae24688e72",
            "11b7020d30d74335a172dd65e288aa90",
            "5f544e073f1d466ab559eaedeb4c6494",
            "4b980f2fd44d41ba90f2ad31b91bb789",
            "ba3c54989cdd497ca44d6a9e62b1b4c0",
            "dd80b20e7ec047c4824c04a94f5b34bb",
            "5c0072ea1d6b44a5ab531d8e515e2d9b",
            "6f0da691048542feafc2edf23c4af313",
            "7a96a7ed58334dac9f60879bb49332dc",
            "01f207a643064985be800b19b8081ce9"
          ]
        },
        "id": "SmnbXzqMklX5",
        "outputId": "04edcc1b-f48a-453d-efa6-18764703238a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading google/medgemma-1.5-4b-it...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "343c030b347041ffa2ad23b9793154d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading image...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating report with NEW Prompt...\n",
            "\n",
            "==============================\n",
            "ç”Ÿæˆçš„æŠ¥å‘Š (New Style):\n",
            "==============================\n",
            "Findings: The heart size is normal. The mediastinal contours are unremarkable. The lungs are clear except for multiple small nodules in the right lower lobe and left lower lobe. There is no pleural effusion or pneumothorax. The bony structures are intact.\n",
            "\n",
            "Impression: Multiple small nodules in the right lower lobe and left lower lobe.\n"
          ]
        }
      ],
      "source": [
        "# Make sure to install the accelerate library first via `pip install accelerate`\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "model_id = \"google/medgemma-1.5-4b-it\"\n",
        "\n",
        "print(f\"Loading {model_id}...\")\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# åŸå§‹ URL (å¦‚æœæŠ¥é”™ 403 Forbiddenï¼Œè¯´æ˜è¿™ä¸ªç­¾åé“¾æ¥è¿‡æœŸäº†ï¼Œéœ€è¦å» Kaggle é‡æ–°å¤åˆ¶)\n",
        "image_url = \"https://storage.googleapis.com/kagglesdsdata/datasets/6850723/11020923/official_data_iccv_final/files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20260204%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20260204T073745Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=20c9cc4cf99f05fc1d6452ee3e7a003c611ab7eb576691d0626884226015bb4e72aa1ff1be53d5739eeeaa7a52b2afc2824cfb8243d85d15adc23c5edd0c40946fa8882a39ef0281f1e5347596a94d6a8dd087f7d0b13db4013683c9d6f1c4393065b9f3dee11afe677cd381431c90c9438a641a64480f4f372b7e25c485b356fbc24f904be90ff23093e96e2e4a2deaaedf63b1aa796752f0eb9bb161b2a6652c52ab38639aa4fe61bf17ccc393ff6d3e43d0bdea89b63c859408b26d6d6fb4e8d39ce718c64f515f65b647f3515f43eec0e51f10f3ac29010ec61a87544c258348664749e763adaf63f07a3a038f547a128da2de6f32f6354947d4d9447846\"\n",
        "\n",
        "print(\"Downloading image...\")\n",
        "try:\n",
        "    image = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw).convert(\"RGB\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {e}\")\n",
        "    print(\"âš ï¸ æç¤º: Kaggle çš„é•¿é“¾æ¥æœ‰å®æ•ˆæ€§ï¼Œè¯·æ£€æŸ¥æ˜¯å¦è¿‡æœŸã€‚\")\n",
        "    # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œè¿™é‡Œä¼šæŠ¥é”™åœæ­¢\n",
        "    raise e\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šPrompt è°ƒæ•´\n",
        "# ==========================================\n",
        "# è¿™ä¸€æ®µ Prompt æ˜¯ä¸“é—¨ä¸ºäº†å»æ‰ç»“æ„åŒ–åˆ—è¡¨ã€å»æ‰ AI åºŸè¯è€Œè®¾è®¡çš„\n",
        "prompt_text = (\n",
        "    \"You are an expert radiologist. Describe this PA view chest X-ray. \"\n",
        "    \"Provide a concise report consisting of Findings and Impression. \"\n",
        "    \"Focus on the heart, lungs, mediastinum, pleural space, and bones. \"\n",
        "    \"Do NOT use bullet points, asterisks, or section headers. \"\n",
        "    \"Do NOT include disclaimers or 'AI' warnings. \"\n",
        "    \"Output pure medical text only.\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": prompt_text}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# å¤„ç†è¾“å…¥\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages, add_generation_prompt=True, tokenize=True,\n",
        "    return_dict=True, return_tensors=\"pt\"\n",
        ").to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "print(\"Generating report with NEW Prompt...\")\n",
        "with torch.inference_mode():\n",
        "    # max_new_tokens è®¾ä¸º 300ï¼Œé˜²æ­¢å®ƒåºŸè¯è¿ç¯‡\n",
        "    generation = model.generate(**inputs, max_new_tokens=300, do_sample=False)\n",
        "    generation = generation[0][input_len:]\n",
        "\n",
        "decoded = processor.decode(generation, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"ç”Ÿæˆçš„æŠ¥å‘Š (New Style):\")\n",
        "print(\"=\"*30)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zarjJe_ByvM"
      },
      "source": [
        "#ä¸‹é¢æµ‹è¯•ç°æˆAWQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90cd728f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# List of likely folders based on your Drive structure\n",
        "possible_folders = [\n",
        "    \"/content/drive/MyDrive/medgemmaæ•°æ®\",\n",
        "    \"/content/drive/MyDrive/mimic_cxr_medgemma_ready\",\n",
        "    \"/content/drive/MyDrive/mimic_cxr_clean_outputs\",\n",
        "    \"/content/drive/MyDrive/medgamma\",\n",
        "    \"/content/drive/MyDrive/mimic_cxr_raw\",\n",
        "    \"/content/drive/MyDrive/\"\n",
        "]\n",
        "\n",
        "target_files = [\"mimic_eval_cleaned.csv\", \"mimic_eval_ready_step1.csv\"]\n",
        "\n",
        "print(\"ğŸ” Checking likely folders for dataset files...\")\n",
        "\n",
        "found_files = {}\n",
        "\n",
        "for folder in possible_folders:\n",
        "    if not os.path.exists(folder): continue\n",
        "\n",
        "    for filename in target_files:\n",
        "        path = os.path.join(folder, filename)\n",
        "        if os.path.exists(path):\n",
        "            print(f\"âœ… Found {filename} at: {path}\")\n",
        "            found_files[filename] = path\n",
        "\n",
        "if not found_files:\n",
        "    print(\"âŒ Could not find the CSV files in the common subfolders.\")\n",
        "    print(\"Please confirm the exact path in your Drive.\")\n",
        "else:\n",
        "    print(\"\\nğŸ“ Ready to proceed with:\")\n",
        "    for k, v in found_files.items():\n",
        "        print(f\" - {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1550d89",
        "outputId": "7544ccc5-b6be-432d-c04c-f1e80cc38415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Contents of /content/drive/MyDrive/ ---\n",
            "Colab Notebooks\n",
            "system-boot\n",
            "lab3\n",
            "medgemmaæ•°æ®\n",
            "mimic_cxr_clean_outputs\n",
            "mimic_cxr_medgemma_ready\n",
            "mimic_cxr_1img1report_from_firstpass\n",
            "mimic_cxr_raw\n",
            "libs\n",
            "Balancing Innovation and Governance:.gdoc\n",
            "lab4\n",
            "d40e7d5a1c6ed8c3f8f7c334972b9b33.mp4\n",
            "5cee1641cd048aba3b92a9a863fa2f41.mp4\n",
            "wall_follower.py\n",
            "plot_wall_distance.py\n",
            "Lab4.gdoc\n",
            "å½•å±2026-02-05 10.39.02.mov\n",
            "å½•å±2026-02-05 10.41.36.mov\n",
            "Tesla Optimus.gslides\n",
            "Personal Project Log â€“ Lab 4: Wall Following with PID Control.gdoc\n",
            "å½•å±2026-02-05 10.39.24.mov\n",
            "Weekly Key Takeaways.gdoc\n",
            "mimic_train_cleaned.gsheet\n",
            "AWQ é‡åŒ–ï¼š.gdoc\n",
            "ä¸‹ä¸€å‘¨é¡¹ç›®è¦æ±‚.gdoc\n",
            "medgamma\n",
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"--- Contents of /content/drive/MyDrive/ ---\")\n",
        "try:\n",
        "    for item in os.listdir('/content/drive/MyDrive/'):\n",
        "        print(item)\n",
        "except FileNotFoundError:\n",
        "    print(\"Google Drive is not mounted or the path is incorrect.\")\n",
        "print(\"------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Xw08Vhi_4H7_",
        "outputId": "ed76504b-ca75-4fcd-e8e9-08df338d4ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: compressed-tensors in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from compressed-tensors) (5.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors) (2.12.3)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.12/dist-packages (from compressed-tensors) (0.7.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->compressed-tensors) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers->compressed-tensors) (1.3.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->compressed-tensors) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->compressed-tensors) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->compressed-tensors) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->compressed-tensors) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->compressed-tensors) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers->compressed-tensors) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->compressed-tensors) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->compressed-tensors) (4.67.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers->compressed-tensors) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers->compressed-tensors) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers->compressed-tensors) (1.5.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7.0->compressed-tensors) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7.0->compressed-tensors) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers->compressed-tensors) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->compressed-tensors) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->compressed-tensors) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->compressed-tensors) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->compressed-tensors) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->compressed-tensors) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install compressed-tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567,
          "referenced_widgets": [
            "82e5e26ca9b843f7ad5f63a58c323861",
            "9224092cf9c2402a84252cb4c00c78ec",
            "59bd09fca1db4ccabd510bb47ac2f6bf",
            "340196720e4e4238b268f45a1a0fe9e9",
            "e130447fa64948a281f806d90063b802",
            "20bd04dd011f4801ad42fc1b51394c91",
            "5fb288ab2c0a4007bac9f8a6d404df1c",
            "3eefacd5c1c54b7b85bd3866069d7c5e",
            "f0af105ede7346fc894f8f439faa61bb",
            "7b107becf045417fa87fdf2d21491b59",
            "5bedadcd9002498b9152b7c890e4d758"
          ]
        },
        "collapsed": true,
        "id": "xOXHLdkm0faZ",
        "outputId": "01f6f788-27c7-4952-dc37-cf87931bb7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning up GPU memory from previous runs...\n",
            "Loading model: MedGemmaImpact/medgemma-1.5-4b-it-awq...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Compressing model: 238it [00:00, 392.13it/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82e5e26ca9b843f7ad5f63a58c323861",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/1360 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tied weights mapping and config for this model specifies to tie model.language_model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading image...\n",
            "Failed to download image: cannot identify image file <_io.BytesIO object at 0x7d310532ae80>\n"
          ]
        },
        {
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file <_io.BytesIO object at 0x7d310532ae80>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4212683325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to download image: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# --- 3. æ„å»ºè¾“å…¥ ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4212683325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading image...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"example\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to download image: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3578\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3579\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3580\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7d310532ae80>"
          ]
        }
      ],
      "source": [
        "# 0. å®‰è£…å¿…è¦çš„åº“ (ç¡®ä¿è¿è¡Œ)\n",
        "!pip install accelerate transformers>=4.38.0 compressed-tensors\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# --- å…³é”®æ­¥éª¤ï¼šæ¸…ç†æ˜¾å­˜ ---\n",
        "print(\"Cleaning up GPU memory from previous runs...\")\n",
        "if 'llm' in globals():\n",
        "    del llm\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "if 'inputs' in globals():\n",
        "    del inputs\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- é…ç½® ---\n",
        "model_id = \"MedGemmaImpact/medgemma-1.5-4b-it-awq\"\n",
        "# ä½¿ç”¨ç¨³å®šçš„ç¤ºä¾‹å›¾ç‰‡ (Wikimedia Commons)\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Chest_Xray_PA_3-8-2010.png/600px-Chest_Xray_PA_3-8-2010.png\"\n",
        "\n",
        "# --- 1. åŠ è½½æ¨¡å‹ (è‡ªåŠ¨é€‚é…æ˜¾å­˜) ---\n",
        "print(f\"Loading model: {model_id}...\")\n",
        "dtype = torch.float16\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=dtype, # æ··åˆç²¾åº¦\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# --- 2. å‡†å¤‡å›¾åƒ ---\n",
        "print(\"Downloading image...\")\n",
        "try:\n",
        "    image = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw).convert(\"RGB\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to download image: {e}\")\n",
        "    raise e\n",
        "\n",
        "# --- 3. æ„å»ºè¾“å…¥ ---\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": \"Describe this X-rayï¼Œfrom Overall Impression/Specific Findingsï¼šHeartã€Lungsã€Mediastinumã€Bonesã€Diaphragmã€Soft Tissues/Possible Considerationsï¼šAge and Sexã€Clinical Historyã€Comparison to Prior Studies\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# ç”Ÿæˆçº¯æ–‡æœ¬ Prompt\n",
        "text_prompt = processor.tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# ç»“åˆå›¾ç‰‡ç”Ÿæˆæœ€ç»ˆ Tensor\n",
        "# æ³¨æ„ï¼šä¸è¦æ‰‹åŠ¨è½¬dtypeï¼Œè®©processorå’Œmodelè‡ªå·±å¤„ç†\n",
        "inputs = processor(\n",
        "    text=text_prompt,\n",
        "    images=image,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "for k, v in inputs.items():\n",
        "    if torch.is_floating_point(v):\n",
        "        inputs[k] = v.to(dtype=dtype)\n",
        "\n",
        "# --- 4. æ¨ç†ç”Ÿæˆ ---\n",
        "print(\"Generating report...\")\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    generation = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False\n",
        "    )\n",
        "    generation = generation[0][input_len:]\n",
        "\n",
        "# --- 5. è§£ç è¾“å‡º ---\n",
        "decoded = processor.decode(generation, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*20 + \" GENERATED REPORT \" + \"=\"*20)\n",
        "print(decoded)\n",
        "print(\"=\"*58)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw6M6gVUNUX8"
      },
      "source": [
        "L4 runtimeå°è¯•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKe1OutXQley"
      },
      "source": [
        "# L4 runtimeå°è¯•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szZYshEaQJZ1",
        "outputId": "89fcbcd7-2e65-4df3-ec15-2e404c1d89c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: huggingface-hub 0.36.0\n",
            "Uninstalling huggingface-hub-0.36.0:\n",
            "  Successfully uninstalled huggingface-hub-0.36.0\n",
            "Found existing installation: vllm 0.15.0\n",
            "Uninstalling vllm-0.15.0:\n",
            "  Successfully uninstalled vllm-0.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, which is not installed.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-generativeai 0.8.6 requires google-ai-generativelanguage==0.6.15, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-genai 1.60.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 16.0 which is incompatible.\n",
            "cuml-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.1 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\n",
            "gradio 5.50.0 requires pydantic<=2.12.3,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
            "gradio-client 1.14.0 requires websockets<16.0,>=13.0, but you have websockets 16.0 which is incompatible.\n",
            "pylibcudf-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\n",
            "bigframes 2.33.0 requires rich<14,>=12.4.4, but you have rich 14.3.2 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.4 which is incompatible.\n",
            "langchain-core 1.2.7 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\n",
            "pylibraft-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\n",
            "google-adk 1.23.0 requires fastapi<0.124.0,>=0.115.0, but you have fastapi 0.128.0 which is incompatible.\n",
            "google-adk 1.23.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 16.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 46.0.4 which is incompatible.\n",
            "rmm-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Repair Complete. Please RESTART SESSION (Runtime -> Restart Session) if you still see errors.\n"
          ]
        }
      ],
      "source": [
        "# 1. å¼ºåˆ¶å¸è½½è¢«æ±¡æŸ“çš„åº“\n",
        "!pip uninstall -y huggingface_hub vllm\n",
        "\n",
        "# 2. é‡æ–°å®‰è£…çº¯å‡€ç‰ˆ\n",
        "# --upgrade --force-reinstall ç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½è¢«åˆ·æ–°\n",
        "!pip install --upgrade --force-reinstall huggingface_hub vllm transformers>=4.38.0 accelerate\n",
        "\n",
        "print(\"âœ… Repair Complete. Please RESTART SESSION (Runtime -> Restart Session) if you still see errors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "ddmzCZhUMRYh",
        "outputId": "28ca9a15-2501-49c7-a52b-786bc2ff41fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error importing huggingface_hub.hf_api: cannot import name 'reset_sessions' from 'huggingface_hub.utils' (/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/__init__.py)\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'reset_sessions' from 'huggingface_hub.utils' (/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3127007302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install vllm transformers>=4.38.0 accelerate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSamplingParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODULE_ATTRS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODULE_ATTRS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__package__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcreate_sort_beams_key_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0;31m from vllm.config import (\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mAttentionConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mCompilationConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoadConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoRAConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from vllm.config.model import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mModelConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0miter_architecture_defaults\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/config/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcurrent_platform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from vllm.transformers_utils.config import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mConfigFormat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mget_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_safetensors_metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;31m# To update the static imports, please run the following command and commit the changes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[0;31m# ```\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m \u001b[0;31m# # Use script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;31m# python utils/check_static_imports.py --update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m from ._commit_api import (\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mCommitOperation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mCommitOperationAdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/_commit_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXetAuthorizationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXetRefreshTokenError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_download\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhf_hub_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlfs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUploadInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlfs_upload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_lfs_batch_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from .utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mRevisionNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mOfflineModeIsEnabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSoftTemporaryDirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'reset_sessions' from 'huggingface_hub.utils' (/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# --- é…ç½® ---\n",
        "model_id = \"MedGemmaImpact/medgemma-1.5-4b-it-awq\"\n",
        "image_url = \"https://storage.googleapis.com/kagglesdsdata/datasets/6850723/11020923/official_data_iccv_final/files/p10/p10000032/s50414267/174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20260202%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20260202T002856Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=9f7da0c7f913527891d03d6ed2fd15b35be2afef9058102b9b11faaa65de38d2cc1180cf8911316e2e25b53b8d188081a6d09787594001daacfc77aed2b729443e0a56e2d25b58a246918b9530ab2e07ed9489b6f489214a918c2850323a7342ff6cafff4724f13d2f972cdf60508cff0539d01d67e3ba78ce7b6feb386a5d83e91d9a650801c549f809381aca81a68d16bd2ac425114f858a3b39f86ffe4d93c68de1bc56ddbc7989f448b67b82300e1333325707d0afac89be8748f6fbbc9c79ef8dbaab79124336910db3970299a4c7dbf43c8cc773820c4e472b832339126ff15bdd47292fca2f4a5cb933d2a87b065b280957beda9e73248e1f27d742bf\"\n",
        "\n",
        "# --- 1. å‡†å¤‡ Prompt ---\n",
        "print(\"Building Prompt...\")\n",
        "# è¿™ä¸€æ­¥åªç”¨ CPUï¼Œä¸ä¼šå ç”¨æ˜¾å­˜\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"Describe this X-rayï¼Œfrom Overall Impression/Specific Findingsï¼šHeartã€Lungsã€Mediastinumã€Bonesã€Diaphragmã€Soft Tissues/Possible Considerationsï¼šAge and Sexã€Clinical Historyã€Comparison to Prior Studies\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "full_prompt = processor.tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# --- 2. åˆå§‹åŒ– vLLM (L4 æ»¡è¡€ç‰ˆ) ---\n",
        "print(\"Loading vLLM Engine on L4...\")\n",
        "llm = LLM(\n",
        "    model=model_id,\n",
        "    trust_remote_code=True,\n",
        "    quantization=\"compressed-tensors\",\n",
        "    dtype=\"bfloat16\",  # L4 æ˜¾å¡æ”¯æŒè¿™ä¸ªåŸå‚ç²¾åº¦ï¼Œæ•ˆæœæœ€å¥½ï¼\n",
        "    max_model_len=2048,\n",
        "    limit_mm_per_prompt={\"image\": 1},\n",
        "    gpu_memory_utilization=0.9\n",
        ")\n",
        "\n",
        "# --- 3. æ¨ç† ---\n",
        "inputs = [{\n",
        "    \"prompt\": full_prompt,\n",
        "    \"multi_modal_data\": {\"image\": image_url}\n",
        "}]\n",
        "\n",
        "sampling_params = SamplingParams(temperature=0.2, max_tokens=512)\n",
        "\n",
        "print(\"Starting Inference...\")\n",
        "outputs = llm.generate(inputs, sampling_params)\n",
        "\n",
        "for o in outputs:\n",
        "    print(\"\\n\" + \"=\"*20 + \" GENERATED REPORT \" + \"=\"*20)\n",
        "    print(o.outputs[0].text)\n",
        "    print(\"=\"*58)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiRPWn3ZTyED"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies for AWQ quantization\n",
        "!pip install autoawq transformers accelerate\n",
        "\n",
        "%cd /content/drive/MyDrive/medgamma\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewpoO2hrVFXk",
        "outputId": "efa7428d-ef75-4e8e-abb1-0754e67ba391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/74.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q autoawq transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbctRxyeYgmg",
        "outputId": "a6645811-5e99-4b46-bb7b-b22c144cfe7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m133.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.6.0 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q \"torch==2.6.0\" \"transformers==4.51.3\" \"autoawq\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmpx0a6qZZkT",
        "outputId": "1f734c45-1199-4d60-b562-c3ca4c4bb24f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m230.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/3.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m199.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \"torch==2.6.0\" \"torchvision==0.21.0\" \"torchaudio==2.6.0\" \"transformers==4.51.3\" \"autoawq\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVE_NL5SUZsd",
        "outputId": "8d5a7314-c431-4c6c-f17c-497d692f0830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/medgamma\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 17, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py\", line 25, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/chat_template_utils.py\", line 37, in <module>\n",
            "    from PIL.Image import Image\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/Image.py\", line 63, in <module>\n",
            "    from defusedxml import ElementTree\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/defusedxml/ElementTree.py\", line 12, in <module>\n",
            "    from xml.etree.ElementTree import ParseError\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1128, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 757, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/medgamma\n",
        "\n",
        "!python scripts/evaluate_awq_model.py \\\n",
        "  --original_model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
        "  --quantized_model \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \\\n",
        "  --eval_data \"/content/mimic_eval_ready_step1.csv\" \\\n",
        "  --prompt_file \"/content/drive/MyDrive/medgamma/prompts/example_prompt.txt\" \\\n",
        "  --num_samples 100 \\\n",
        "  --output_dir \"/content/drive/MyDrive/medgamma/evaluation_results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGVFq7WLXCdz",
        "outputId": "229b6a63-9440-4913-e6b9-1a738e9b6971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Feb 10 00:41:12 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   37C    P0             65W /  400W |    8721MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919,
          "referenced_widgets": [
            "675660d07bc04df6a9f0640e4a98b064",
            "aced7535dc22470d89d505375fb7651c",
            "bf227b1952d34ad08794deb9392c0056",
            "70af394ca22f46849b9bf4a216996f3a",
            "73024119dac84b59b2ddf5a480bd73ae",
            "8c9afa0f475a4e229bdf5df00fd9a727",
            "c020ec8eb8cc446fa1ca0b794f0653d1",
            "a1d5363d707b41e2805298e8593bae22",
            "cbe180b16e94454daf30a1b53ba27645",
            "8349816faca941a08950299691a201f6",
            "d8bc41422c274bf2b27f946a258fd90e"
          ]
        },
        "id": "7468f012",
        "outputId": "88ea1363-6b30-44ee-8592-7e396e8661a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m258.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m128.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\n",
            "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mğŸš€ Loading google/medgemma-1.5-4b-it in 4-bit mode...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "675660d07bc04df6a9f0640e4a98b064",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model loaded successfully!\n",
            "ğŸ” Looking for a test image in the dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ–¼ï¸ Using local image: /kaggle/input/mimic-cxr-dataset/official_data_iccv_final/files/p10/p10075925/s51010496/2d783c8a-492984b7-28aaf571-bfc30156-61ab26f6.jpg\n",
            "ğŸ“ Generating report...\n",
            "\n",
            "==============================\n",
            "Based on the provided chest X-ray, here's a description of the findings and a potential impression:\n",
            "\n",
            "**Findings:**\n",
            "\n",
            "*   **Cardiomegaly:** The heart appears enlarged (cardiomegaly).\n",
            "*   **Pulmonary Vascularity:** There is increased prominence of the pulmonary vessels, suggesting possible pulmonary hypertension or fluid overload.\n",
            "*   **Lung Fields:** The lung fields appear somewhat hazy, which could be due to various factors, including fluid, edema, or other underlying conditions.\n",
            "*   **Bones:** The bony structures (ribs, clavicles, and shoulders) appear intact.\n",
            "\n",
            "**Impression:**\n",
            "\n",
            "The X-ray shows cardiomegaly and increased pulmonary vascularity. The hazy appearance of the lung fields could be due to fluid or edema. Further evaluation is needed to determine the underlying cause of these findings.\n",
            "\n",
            "**Disclaimer:** This is a preliminary interpretation based on a single image. A definitive diagnosis requires a complete clinical evaluation, including patient history, physical examination, and potentially additional imaging or laboratory tests. This information should not be used for self-diagnosis or treatment. Always consult with a qualified healthcare professional for any medical concerns.\n",
            "==============================\n"
          ]
        }
      ],
      "source": [
        "# 1. Fix Environment & Install Dependencies\n",
        "# Re-installing stable versions to fix the conflict created by the previous downgrade\n",
        "!pip install -U -q torch torchvision torchaudio transformers bitsandbytes accelerate\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import ast\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "from PIL import Image\n",
        "\n",
        "# --- Configuration ---\n",
        "model_id = \"google/medgemma-1.5-4b-it\"\n",
        "csv_path = \"/content/drive/MyDrive/medgamma/mimic_eval_single_image_final_233.csv\"\n",
        "dataset_root = \"/kaggle/input/mimic-cxr-dataset/official_data_iccv_final\"\n",
        "\n",
        "# --- 2. Load Model in 4-bit (NF4) ---\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(f\"ğŸš€ Loading {model_id} in 4-bit mode...\")\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "\n",
        "# --- 3. Get a Real Image from CSV ---\n",
        "def get_first_valid_image(csv_path, dataset_root):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"âš ï¸ CSV not found at {csv_path}\")\n",
        "        return None\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # Check first few rows for a valid image\n",
        "    for idx, row in df.head(10).iterrows():\n",
        "        for col in ['PA', 'AP', 'Lateral']:\n",
        "            if pd.isna(row.get(col)): continue\n",
        "\n",
        "            # Clean path string\n",
        "            val_str = str(row[col]).strip()\n",
        "            path_str = val_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', \"\")\n",
        "\n",
        "            if 'files' in path_str:\n",
        "                relative = 'files' + path_str.split('files', 1)[1]\n",
        "            else:\n",
        "                relative = path_str.strip('/')\n",
        "\n",
        "            full_path = os.path.join(dataset_root, relative)\n",
        "            if os.path.exists(full_path):\n",
        "                return full_path\n",
        "    return None\n",
        "\n",
        "print(\"ğŸ” Looking for a test image in the dataset...\")\n",
        "test_image_path = get_first_valid_image(csv_path, dataset_root)\n",
        "\n",
        "if test_image_path:\n",
        "    print(f\"ğŸ–¼ï¸ Using local image: {test_image_path}\")\n",
        "    image = Image.open(test_image_path).convert(\"RGB\")\n",
        "else:\n",
        "    print(\"âš ï¸ Local image not found in CSV. Creating a dummy image for testing.\")\n",
        "    image = Image.new('RGB', (224, 224), color='gray')\n",
        "\n",
        "# --- 4. Run Inference ---\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": \"Describe this X-ray. Findings and Impression.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages, add_generation_prompt=True, tokenize=True,\n",
        "    return_dict=True, return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "print(\"ğŸ“ Generating report...\")\n",
        "with torch.inference_mode():\n",
        "    generation = model.generate(**inputs, max_new_tokens=300, do_sample=False)\n",
        "    generation = generation[0][input_len:]\n",
        "\n",
        "decoded = processor.decode(generation, skip_special_tokens=True)\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(decoded)\n",
        "print(\"=\"*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2OGEWEbawxP",
        "outputId": "9de8b9ab-6d02-424e-fff1-fd5552255797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/588.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m588.0/588.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for radgraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q radgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0339f597",
        "outputId": "29f10a33-9708-4e52-a130-1fae772caef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Successfully patched 'evaluate_awq_model.py' to fix the device attribute error.\n",
            "ğŸš€ Restarting Evaluation...\n",
            "/usr/local/lib/python3.12/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
            "I have left this message as the final dev message to help you transition.\n",
            "\n",
            "Important Notice:\n",
            "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
            "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
            "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
            "\n",
            "Alternative:\n",
            "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
            "\n",
            "For further inquiries, feel free to reach out:\n",
            "- X: https://x.com/casper_hansen_\n",
            "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
            "\n",
            "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py:1480: DeprecationWarning: `torch.jit.script` is deprecated. Please switch to `torch.compile` or `torch.export`.\n",
            "  warnings.warn(\n",
            "======================================================================\n",
            "MedGamma AWQ é‡åŒ–è¯„ä¼°\n",
            "======================================================================\n",
            "\n",
            "åŠ è½½æ¨¡å‹...\n",
            "  åŠ è½½åŸå§‹æ¨¡å‹...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading weights: 100% 291/291 [00:04<00:00, 59.51it/s, Materializing param=model.norm.weight]\n",
            "  åŸå§‹æ¨¡å‹è€—æ—¶: 9.01ç§’\n",
            "\n",
            "åŠ è½½è¯„ä¼°æ•°æ®: /content/mimic_eval_ready_step1.csv\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 403, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 399, in main\n",
            "    evaluator.run_evaluation(num_samples=args.num_samples)\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 214, in run_evaluation\n",
            "    df = self.load_eval_data(num_samples)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 121, in load_eval_data\n",
            "    df = pd.read_csv(self.eval_data_path)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\", line 873, in get_handle\n",
            "    handle = open(\n",
            "             ^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/mimic_eval_ready_step1.csv'\n",
            "sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "script_path = \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\"\n",
        "\n",
        "# 1. Read the script\n",
        "if os.path.exists(script_path):\n",
        "    with open(script_path, \"r\") as f:\n",
        "        code = f.read()\n",
        "\n",
        "    # 2. Apply the fix: Replace .to(model.device) with .to(next(model.parameters()).device)\n",
        "    # This works for both standard HF models and AutoAWQ wrappers\n",
        "    fixed_code = code.replace(\".to(model.device)\", \".to(next(model.parameters()).device)\")\n",
        "\n",
        "    # 3. Save the fixed script\n",
        "    with open(script_path, \"w\") as f:\n",
        "        f.write(fixed_code)\n",
        "\n",
        "    print(\"âœ… Successfully patched 'evaluate_awq_model.py' to fix the device attribute error.\")\n",
        "else:\n",
        "    print(f\"âŒ Script not found at {script_path}\")\n",
        "\n",
        "# 4. Run the evaluation again (using the Mistral model from your command)\n",
        "print(\"ğŸš€ Restarting Evaluation...\")\n",
        "!python /content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py \\\n",
        "  --original_model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
        "  --quantized_model \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \\\n",
        "  --eval_data \"/content/mimic_eval_ready_step1.csv\" \\\n",
        "  --prompt_file \"/content/drive/MyDrive/medgamma/prompts/example_prompt.txt\" \\\n",
        "  --num_samples 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8e9e223",
        "outputId": "64ade6f1-4179-4a72-819b-55364491731f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        self.single_sequence_token_type_id = None\n",
            "\n",
            "        # Reverse-engineer the tokenizer for two sequences\n",
            "        from radgraph.allennlp.common import cached_transformers\n",
            "\n",
            "        tokenizer_with_special_tokens = cached_transformers.get_tokenizer(\n",
            "            model_name, **tokenizer_kwargs\n",
            "        )\n",
            "        dummy_output = tokenizer_with_special_tokens.encode_plus(\n",
            "            token_a,\n",
            "            token_b,\n",
            "            add_special_tokens=True,\n",
            "            return_token_type_ids=True,\n",
            "            return_attention_mask=False,\n",
            "        )\n",
            "        dummy_a = self.tokenizer.encode(token_a, add_special_tokens=False)[0]\n",
            "        assert dummy_a in dummy_output[\"input_ids\"]\n",
            "        dummy_b = self.tokenizer.encode(token_b, add_special_tokens=False)[0]\n",
            "        assert dummy_b in dummy_output[\"input_ids\"]\n",
            "        assert dummy_a != dummy_b\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sed -n '115,135p' /usr/local/lib/python3.12/dist-packages/radgraph/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "patch_radgraph",
        "outputId": "762c18fd-4d24-4efa-e67b-91a4d2890bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Patching /usr/local/lib/python3.12/dist-packages/radgraph/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py...\n",
            "âš ï¸ '.encode_plus(' not found in file. It might have been patched already.\n",
            "ğŸš€ Restarting Evaluation...\n",
            "/usr/local/lib/python3.12/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
            "I have left this message as the final dev message to help you transition.\n",
            "\n",
            "Important Notice:\n",
            "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
            "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
            "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
            "\n",
            "Alternative:\n",
            "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
            "\n",
            "For further inquiries, feel free to reach out:\n",
            "- X: https://x.com/casper_hansen_\n",
            "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
            "\n",
            "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py:1480: DeprecationWarning: `torch.jit.script` is deprecated. Please switch to `torch.compile` or `torch.export`.\n",
            "  warnings.warn(\n",
            "======================================================================\n",
            "MedGamma AWQ é‡åŒ–è¯„ä¼°\n",
            "======================================================================\n",
            "\n",
            "åŠ è½½æ¨¡å‹...\n",
            "  åŠ è½½åŸå§‹æ¨¡å‹...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading weights: 100% 291/291 [00:04<00:00, 60.61it/s, Materializing param=model.norm.weight]\n",
            "  åŸå§‹æ¨¡å‹è€—æ—¶: 7.91ç§’\n",
            "\n",
            "åŠ è½½è¯„ä¼°æ•°æ®: /content/mimic_eval_ready_step1.csv\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 403, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 399, in main\n",
            "    evaluator.run_evaluation(num_samples=args.num_samples)\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 214, in run_evaluation\n",
            "    df = self.load_eval_data(num_samples)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 121, in load_eval_data\n",
            "    df = pd.read_csv(self.eval_data_path)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\", line 873, in get_handle\n",
            "    handle = open(\n",
            "             ^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/mimic_eval_ready_step1.csv'\n",
            "sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path to the failing file in radgraph\n",
        "radgraph_file = \"/usr/local/lib/python3.12/dist-packages/radgraph/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py\"\n",
        "\n",
        "if os.path.exists(radgraph_file):\n",
        "    print(f\"ğŸ”§ Patching {radgraph_file}...\")\n",
        "    with open(radgraph_file, \"r\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Replace .encode_plus( with ( to use __call__ which is the standard way now\n",
        "    # The line is: dummy_output = tokenizer_with_special_tokens.encode_plus(\n",
        "    if \".encode_plus(\" in content:\n",
        "        new_content = content.replace(\".encode_plus(\", \"(\")\n",
        "        with open(radgraph_file, \"w\") as f:\n",
        "            f.write(new_content)\n",
        "        print(\"âœ… Patch applied successfully!\")\n",
        "    else:\n",
        "        print(\"âš ï¸ '.encode_plus(' not found in file. It might have been patched already.\")\n",
        "else:\n",
        "    print(\"âŒ RadGraph file not found. Check installation.\")\n",
        "\n",
        "# Restart Evaluation\n",
        "print(\"ğŸš€ Restarting Evaluation...\")\n",
        "!python /content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py \\\n",
        "  --original_model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
        "  --quantized_model \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \\\n",
        "  --eval_data \"/content/mimic_eval_ready_step1.csv\" \\\n",
        "  --prompt_file \"/content/drive/MyDrive/medgamma/prompts/example_prompt.txt\" \\\n",
        "  --num_samples 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GZPIwtTjVbl",
        "outputId": "b57dffaf-85e8-44f2-c5a5-2b7d7a3adc23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… RadGraph patches applied\n"
          ]
        }
      ],
      "source": [
        "# Patch 1: encode_plus -> __call__\n",
        "radgraph_file = \"/usr/local/lib/python3.12/dist-packages/radgraph/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py\"\n",
        "content = open(radgraph_file).read()\n",
        "if \".encode_plus(\" in content:\n",
        "    open(radgraph_file, \"w\").write(content.replace(\".encode_plus(\", \"(\"))\n",
        "\n",
        "# Patch 2: build_inputs_with_special_tokens\n",
        "radgraph_file = \"/usr/local/lib/python3.12/dist-packages/radgraph/allennlp/data/token_indexers/pretrained_transformer_indexer.py\"\n",
        "content = open(radgraph_file).read()\n",
        "old = \"self._tokenizer.build_inputs_with_special_tokens(segment)\"\n",
        "new = \"self._tokenizer.build_inputs_with_special_tokens(segment) if hasattr(self._tokenizer, 'build_inputs_with_special_tokens') else segment\"\n",
        "if old in content:\n",
        "    open(radgraph_file, \"w\").write(content.replace(old, new))\n",
        "\n",
        "print(\"âœ… RadGraph patches applied\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbW4rRjGEChl",
        "outputId": "4ca994be-0831-4105-b773-e142372691ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mimic_eval_cleaned.csv\n",
            "mimic_eval_single_image_final.csv\n",
            "mimic_eval_with_scores.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/medgamma | grep mimic_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loKIDfnIXiGR",
        "outputId": "2eb2d9b1-0aad-4d73-cd90-daf3222819a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 4, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 11, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 13, in <module>\n",
            "    from pip._vendor.rich.console import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 63, in <module>\n",
            "    from .scope import render_scope\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/scope.py\", line 7, in <module>\n",
            "    from .table import Table\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/table.py\", line 17, in <module>\n",
            "    from ._ratio import ratio_distribute, ratio_reduce\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/_ratio.py\", line 2, in <module>\n",
            "    from fractions import Fraction\n",
            "  File \"/usr/lib/python3.12/fractions.py\", line 6, in <module>\n",
            "    from decimal import Decimal\n",
            "  File \"/usr/lib/python3.12/decimal.py\", line 102, in <module>\n",
            "    from _decimal import *\n",
            "  File \"<frozen importlib._bootstrap>\", line 1349, in _find_and_load\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"torch==2.6.0\" \"torchvision==0.21.0\" \"torchaudio==2.6.0\" \"transformers==4.51.3\" \"autoawq\" \"autoawq-kernels\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sNv-mjgSUDN",
        "outputId": "a7fe7c4d-4e24-4ee9-eefe-af3b6893083f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q autoawq-kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn0IyY1gUozh"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"torch==2.6.0\" \"torchvision==0.21.0\" \"torchaudio==2.6.0\" \"transformers==4.51.3\" \"autoawq\" \"autoawq-kernels\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkNEa_U7TWB0",
        "outputId": "8d24749a-d729-4211-9664-7cbf263eb870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "awq_ext FAILED: /usr/local/lib/python3.12/dist-packages/awq_ext.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs\n"
          ]
        }
      ],
      "source": [
        "!python - << 'PY'\n",
        "try:\n",
        "    import awq_ext\n",
        "    print(\"awq_ext OK\")\n",
        "except Exception as e:\n",
        "    print(\"awq_ext FAILED:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LppUZuB8WXPp",
        "outputId": "f55a69cb-3f18-4ab2-f98d-3d29bd35f637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/medgamma\n",
            "/usr/local/lib/python3.12/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
            "I have left this message as the final dev message to help you transition.\n",
            "\n",
            "Important Notice:\n",
            "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
            "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
            "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
            "\n",
            "Alternative:\n",
            "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
            "\n",
            "For further inquiries, feel free to reach out:\n",
            "- X: https://x.com/casper_hansen_\n",
            "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
            "\n",
            "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
            "2026-02-10 02:20:18.530581: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-02-10 02:20:18.549552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770690018.580140   30272 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770690018.586761   30272 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770690018.603690   30272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770690018.603736   30272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770690018.603742   30272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770690018.603746   30272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-10 02:20:18.608800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/matplotlib/_fontconfig_pattern.py:64: PyparsingDeprecationWarning: 'oneOf' deprecated - use 'one_of'\n",
            "  prop = Group((name + Suppress(\"=\") + comma_separated(value)) | oneOf(_CONSTANTS))\n",
            "/usr/local/lib/python3.12/dist-packages/matplotlib/_fontconfig_pattern.py:85: PyparsingDeprecationWarning: 'parseString' deprecated - use 'parse_string'\n",
            "  parse = parser.parseString(pattern)\n",
            "/usr/local/lib/python3.12/dist-packages/matplotlib/_fontconfig_pattern.py:89: PyparsingDeprecationWarning: 'resetCache' deprecated - use 'reset_cache'\n",
            "  parser.resetCache()\n",
            "/usr/local/lib/python3.12/dist-packages/matplotlib/_mathtext.py:45: PyparsingDeprecationWarning: 'enablePackrat' deprecated - use 'enable_packrat'\n",
            "  ParserElement.enablePackrat()\n",
            "WARNING:matplotlib.style.core:In /usr/local/lib/python3.12/dist-packages/matplotlib/mpl-data/stylelib/classic.mplstyle: 'parseString' deprecated - use 'parse_string'\n",
            "WARNING:matplotlib.style.core:In /usr/local/lib/python3.12/dist-packages/matplotlib/mpl-data/stylelib/classic.mplstyle: 'resetCache' deprecated - use 'reset_cache'\n",
            "======================================================================\n",
            "MedGamma AWQ é‡åŒ–è¯„ä¼°\n",
            "======================================================================\n",
            "\n",
            "åŠ è½½æ¨¡å‹...\n",
            "  åŠ è½½åŸå§‹æ¨¡å‹...\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.45s/it]\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "  åŸå§‹æ¨¡å‹è€—æ—¶: 5.40ç§’\n",
            "\n",
            "åŠ è½½è¯„ä¼°æ•°æ®: /content/drive/MyDrive/medgamma/mimic_eval_cleaned.csv\n",
            "  åŠ è½½äº† 100 ä¸ªæ ·æœ¬\n",
            "\n",
            "æµ‹é‡æ˜¾å­˜å ç”¨ï¼ˆåŸå§‹æ¨¡å‹ï¼‰...\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "\n",
            "åŸå§‹æ¨¡å‹é€Ÿåº¦æµ‹è¯•...\n",
            "\n",
            "æµ‹è¯•æ¨ç†é€Ÿåº¦ï¼ˆ10 æ¬¡è¿è¡Œï¼‰...\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "\n",
            "ç”ŸæˆæŠ¥å‘Šï¼ˆåŸå§‹æ¨¡å‹ï¼Œ100 ä¸ªæ ·æœ¬ï¼‰...\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "object address  : 0x7a8812046920\n",
            "object refcount : 3\n",
            "object type     : 0xa2a4e0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/medgamma\n",
        "!python scripts/evaluate_awq_model.py \\\n",
        "  --original_model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
        "  --quantized_model \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \\\n",
        "  --eval_data \"/content/drive/MyDrive/medgamma/mimic_eval_cleaned.csv\" \\\n",
        "  --prompt_file \"/content/drive/MyDrive/medgamma/prompts/example_prompt.txt\" \\\n",
        "  --num_samples 100 \\\n",
        "  --output_dir \"/content/drive/MyDrive/medgamma/evaluation_results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_djt-caD2zT",
        "outputId": "85822103-9b66-40b2-c12e-dfb10634be43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/medgamma\n",
            "/usr/local/lib/python3.12/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
            "I have left this message as the final dev message to help you transition.\n",
            "\n",
            "Important Notice:\n",
            "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
            "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
            "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
            "\n",
            "Alternative:\n",
            "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
            "\n",
            "For further inquiries, feel free to reach out:\n",
            "- X: https://x.com/casper_hansen_\n",
            "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
            "\n",
            "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py:1480: DeprecationWarning: `torch.jit.script` is deprecated. Please switch to `torch.compile` or `torch.export`.\n",
            "  warnings.warn(\n",
            "======================================================================\n",
            "MedGamma AWQ é‡åŒ–è¯„ä¼°\n",
            "======================================================================\n",
            "\n",
            "åŠ è½½æ¨¡å‹...\n",
            "  åŠ è½½åŸå§‹æ¨¡å‹...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading weights: 100% 291/291 [00:04<00:00, 61.35it/s, Materializing param=model.norm.weight]\n",
            "  åŸå§‹æ¨¡å‹è€—æ—¶: 7.80ç§’\n",
            "\n",
            "åŠ è½½è¯„ä¼°æ•°æ®: /content/drive/MyDrive/medgamma/mimic_eval_cleaned.csv\n",
            "  åŠ è½½äº† 100 ä¸ªæ ·æœ¬\n",
            "\n",
            "æµ‹é‡æ˜¾å­˜å ç”¨ï¼ˆåŸå§‹æ¨¡å‹ï¼‰...\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "\n",
            "åŸå§‹æ¨¡å‹é€Ÿåº¦æµ‹è¯•...\n",
            "\n",
            "æµ‹è¯•æ¨ç†é€Ÿåº¦ï¼ˆ10 æ¬¡è¿è¡Œï¼‰...\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "\n",
            "ç”ŸæˆæŠ¥å‘Šï¼ˆåŸå§‹æ¨¡å‹ï¼Œ100 ä¸ªæ ·æœ¬ï¼‰...\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "  åŠ è½½é‡åŒ–æ¨¡å‹...\n",
            "  [WARNING] awq_ext æœªå®‰è£…ï¼ŒèåˆåŠ é€Ÿä¸å¯ç”¨\n",
            "config.json: 100% 904/904 [00:00<00:00, 4.15MB/s]\n",
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Downloading (incomplete total...):   0% 1.52k/4.15G [00:00<546:59:12, 2.11kB/s]\n",
            "Downloading (incomplete total...): 4.15GB [00:07, 508MB/s]                \n",
            "Fetching 10 files: 100% 10/10 [00:07<00:00,  1.28it/s]\n",
            "Download complete: : 4.15GB [00:07, 508MB/s]              `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Download complete: : 4.15GB [00:07, 529MB/s]\n",
            "Replacing layers...: 100% 32/32 [00:07<00:00,  4.09it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/awq/models/base.py:541: UserWarning: Skipping fusing modules because AWQ extension is not installed.No module named 'awq_ext'\n",
            "  warnings.warn(\"Skipping fusing modules because AWQ extension is not installed.\" + msg)\n",
            "  é‡åŒ–æ¨¡å‹è€—æ—¶: 20.50ç§’\n",
            "\n",
            "æµ‹é‡æ˜¾å­˜å ç”¨ï¼ˆé‡åŒ–æ¨¡å‹ï¼‰...\n",
            "é‡åŒ–æ¨¡å‹é€Ÿåº¦æµ‹è¯•...\n",
            "\n",
            "æµ‹è¯•æ¨ç†é€Ÿåº¦ï¼ˆ10 æ¬¡è¿è¡Œï¼‰...\n",
            "\n",
            "ç”ŸæˆæŠ¥å‘Šï¼ˆé‡åŒ–æ¨¡å‹ï¼Œ100 ä¸ªæ ·æœ¬ï¼‰...\n",
            "\n",
            "è®¡ç®— RadGraph F1 Scores...\n",
            "Using device: cuda:0\n",
            "modern-radgraph-xl.tar.gz: 100% 579M/579M [00:02<00:00, 213MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/radgraph/radgraph.py:105: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=model_dir)\n",
            "config.json: 1.19kB [00:00, 3.27MB/s]\n",
            "tokenizer_config.json: 20.8kB [00:00, 46.3MB/s]\n",
            "tokenizer.json: 2.13MB [00:00, 15.7MB/s]\n",
            "special_tokens_map.json: 100% 694/694 [00:00<00:00, 2.61MB/s]\n",
            "\n",
            "è®¡ç®— RadGraph F1 Scores...\n",
            "Using device: cuda:0\n",
            "\n",
            "ä¿å­˜ç»“æœåˆ° /content/drive/MyDrive/medgamma/evaluation_results...\n",
            "ç»“æœå·²ä¿å­˜\n",
            "\n",
            "======================================================================\n",
            "è¯„ä¼°æ‘˜è¦\n",
            "======================================================================\n",
            "\n",
            "F1 Score:\n",
            "  åŸå§‹æ¨¡å‹:  0.1032\n",
            "  é‡åŒ–æ¨¡å‹:  0.0998\n",
            "  ä¸‹é™:      3.28%\n",
            "\n",
            "æ¨ç†é€Ÿåº¦:\n",
            "  åŸå§‹æ¨¡å‹:  28.59 tokens/s\n",
            "  é‡åŒ–æ¨¡å‹:  15.66 tokens/s\n",
            "  åŠ é€Ÿ:      0.55x\n",
            "\n",
            "æ˜¾å­˜å ç”¨:\n",
            "  åŸå§‹æ¨¡å‹:  13.51 GB\n",
            "  é‡åŒ–æ¨¡å‹:  3.89 GB\n",
            "  å‡å°‘:      71.23%\n",
            "\n",
            "======================================================================\n",
            "sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/medgamma\n",
        "!python scripts/evaluate_awq_model.py \\\n",
        "  --original_model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
        "  --quantized_model \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \\\n",
        "  --eval_data \"/content/drive/MyDrive/medgamma/mimic_eval_cleaned.csv\" \\\n",
        "  --prompt_file \"/content/drive/MyDrive/medgamma/prompts/example_prompt.txt\" \\\n",
        "  --num_samples 100 \\\n",
        "  --output_dir \"/content/drive/MyDrive/medgamma/evaluation_results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "fEPTFdud7AvO",
        "outputId": "2e674170-4de3-48d0-955e-d0bf17bad3b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/medgamma\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "æœªæ‰¾åˆ°æ—§ load_models ä»£ç ï¼Œè¯·æ‰‹åŠ¨åŒæ­¥é¡¹ç›®",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m æœªæ‰¾åˆ°æ—§ load_models ä»£ç ï¼Œè¯·æ‰‹åŠ¨åŒæ­¥é¡¹ç›®\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/medgamma\n",
        "\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "\n",
        "path = Path(\"scripts/evaluate_awq_model.py\")\n",
        "text = path.read_text()\n",
        "\n",
        "old = \"    def load_models(self):\\n        print(\\\"\\\\nåŠ è½½æ¨¡å‹...\\\")\\n\\n        print(\\\"  åŠ è½½åŸå§‹æ¨¡å‹...\\\")\\n        start = time.time()\\n        self.original_model = AutoModelForCausalLM.from_pretrained(\\n            self.original_model_path,\\n            device_map=\\\"auto\\\",\\n            torch_dtype=torch.float16,\\n            trust_remote_code=True,\\n        )\\n        self.original_tokenizer = AutoTokenizer.from_pretrained(\\n            self.original_model_path, trust_remote_code=True\\n        )\\n        original_load_time = time.time() - start\\n        print(f\\\"  åŸå§‹æ¨¡å‹è€—æ—¶: {original_load_time:.2f}ç§’\\\")\\n\\n        print(\\\"  åŠ è½½é‡åŒ–æ¨¡å‹...\\\")\\n        start = time.time()\\n        self.quantized_model = AutoAWQForCausalLM.from_quantized(\\n            self.quantized_model_path,\\n            fuse_layers=True,\\n            trust_remote_code=True,\\n            safetensors=True,\\n        )\\n        self.quantized_tokenizer = AutoTokenizer.from_pretrained(\\n            self.quantized_model_path, trust_remote_code=True\\n        )\\n        quantized_load_time = time.time() - start\\n        print(f\\\"  é‡åŒ–æ¨¡å‹è€—æ—¶: {quantized_load_time:.2f}ç§’\\\")\\n\\n        return {\\n            \\\"original_load_time\\\": original_load_time,\\n            \\\"quantized_load_time\\\": quantized_load_time,\\n        }\\n\"\n",
        "new = \"    def _get_device(self, model):\\n        try:\\n            return next(model.parameters()).device\\n        except StopIteration:\\n            return torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\n    def load_models(self):\\n        print(\\\"\\\\nåŠ è½½æ¨¡å‹...\\\")\\n\\n        print(\\\"  åŠ è½½åŸå§‹æ¨¡å‹...\\\")\\n        start = time.time()\\n        self.original_model = AutoModelForCausalLM.from_pretrained(\\n            self.original_model_path,\\n            device_map=\\\"auto\\\",\\n            torch_dtype=torch.float16,\\n            trust_remote_code=True,\\n        )\\n        self.original_tokenizer = AutoTokenizer.from_pretrained(\\n            self.original_model_path, trust_remote_code=True\\n        )\\n        original_load_time = time.time() - start\\n        print(f\\\"  åŸå§‹æ¨¡å‹è€—æ—¶: {original_load_time:.2f}ç§’\\\")\\n\\n        return {\\\"original_load_time\\\": original_load_time}\\n\\n    def load_quantized_model(self):\\n        print(\\\"  åŠ è½½é‡åŒ–æ¨¡å‹...\\\")\\n        try:\\n            import awq_ext  # noqa: F401\\n        except Exception:\\n            print(\\\"  [WARNING] awq_ext æœªå®‰è£…ï¼ŒèåˆåŠ é€Ÿä¸å¯ç”¨\\\")\\n        start = time.time()\\n        self.quantized_model = AutoAWQForCausalLM.from_quantized(\\n            self.quantized_model_path,\\n            fuse_layers=True,\\n            trust_remote_code=True,\\n            safetensors=True,\\n        )\\n        self.quantized_tokenizer = AutoTokenizer.from_pretrained(\\n            self.quantized_model_path, trust_remote_code=True\\n        )\\n        quantized_load_time = time.time() - start\\n        print(f\\\"  é‡åŒ–æ¨¡å‹è€—æ—¶: {quantized_load_time:.2f}ç§’\\\")\\n        return {\\\"quantized_load_time\\\": quantized_load_time}\\n\"\n",
        "\n",
        "if old not in text:\n",
        "    raise SystemExit(\"æœªæ‰¾åˆ°æ—§ load_models ä»£ç ï¼Œè¯·æ‰‹åŠ¨åŒæ­¥é¡¹ç›®\")\n",
        "text = text.replace(old, new)\n",
        "\n",
        "text = text.replace(\n",
        "    \"inputs = tokenizer(prompt, return_tensors=\\\"pt\\\", truncation=True, max_length=512).to(model.device)\",\n",
        "    \"device = self._get_device(model)\\n        inputs = tokenizer(prompt, return_tensors=\\\"pt\\\", truncation=True, max_length=512).to(device)\"\n",
        ")\n",
        "text = text.replace(\n",
        "    \"inputs = tokenizer(prompt, return_tensors=\\\"pt\\\").to(model.device)\",\n",
        "    \"device = self._get_device(model)\\n            inputs = tokenizer(prompt, return_tensors=\\\"pt\\\").to(device)\"\n",
        ")\n",
        "\n",
        "text = text.replace(\n",
        "    \"dummy_input = torch.randint(0, 1000, (1, 50)).to(model.device)\",\n",
        "    \"try:\\n                device = next(model.parameters()).device\\n            except StopIteration:\\n                device = torch.device(\\\"cuda\\\")\\n            dummy_input = torch.randint(0, 1000, (1, 50)).to(device)\"\n",
        ")\n",
        "\n",
        "old_block = \"        print(\\\"\\\\næµ‹é‡æ˜¾å­˜å ç”¨...\\\")\\n        original_mem = self.measure_memory(self.original_model)\\n        quantized_mem = self.measure_memory(self.quantized_model)\\n        results[\\\"memory\\\"] = {\\n            \\\"original\\\": original_mem,\\n            \\\"quantized\\\": quantized_mem,\\n            \\\"reduction_percent\\\": (\\n                (original_mem[\\\"peak_gb\\\"] - quantized_mem[\\\"peak_gb\\\"])\\n                / original_mem[\\\"peak_gb\\\"]\\n                * 100\\n            )\\n            if original_mem[\\\"peak_gb\\\"] > 0\\n            else 0,\\n        }\\n\\n        print(\\\"\\\\nåŸå§‹æ¨¡å‹é€Ÿåº¦æµ‹è¯•...\\\")\\n        original_speed = self.benchmark_inference_speed(\\n            self.original_model,\\n            self.original_tokenizer,\\n            test_prompts,\\n            num_runs=min(10, num_samples),\\n        )\\n        print(\\\"é‡åŒ–æ¨¡å‹é€Ÿåº¦æµ‹è¯•...\\\")\\n        quantized_speed = self.benchmark_inference_speed(\\n            self.quantized_model,\\n            self.quantized_tokenizer,\\n            test_prompts,\\n            num_runs=min(10, num_samples),\\n        )\\n        results[\\\"inference_speed\\\"] = {\\n            \\\"original\\\": original_speed,\\n            \\\"quantized\\\": quantized_speed,\\n            \\\"speedup\\\": quantized_speed[\\\"tokens_per_sec\\\"]\\n            / original_speed[\\\"tokens_per_sec\\\"],\\n        }\\n\\n        print(f\\\"\\\\nç”ŸæˆæŠ¥å‘Šï¼ˆ{num_samples} ä¸ªæ ·æœ¬ï¼‰...\\\")\\n        original_reports = []\\n        for prompt in test_prompts:\\n            report = self.generate_report(\\n                self.original_model, self.original_tokenizer, prompt\\n            )\\n            original_reports.append(report)\\n\\n        quantized_reports = []\\n        for prompt in test_prompts:\\n            report = self.generate_report(\\n                self.quantized_model, self.quantized_tokenizer, prompt\\n            )\\n            quantized_reports.append(report)\\n\"\n",
        "new_block = \"        print(\\\"\\\\næµ‹é‡æ˜¾å­˜å ç”¨ï¼ˆåŸå§‹æ¨¡å‹ï¼‰...\\\")\\n        original_mem = self.measure_memory(self.original_model)\\n\\n        print(\\\"\\\\nåŸå§‹æ¨¡å‹é€Ÿåº¦æµ‹è¯•...\\\")\\n        original_speed = self.benchmark_inference_speed(\\n            self.original_model,\\n            self.original_tokenizer,\\n            test_prompts,\\n            num_runs=min(10, num_samples),\\n        )\\n\\n        print(f\\\"\\\\nç”ŸæˆæŠ¥å‘Šï¼ˆåŸå§‹æ¨¡å‹ï¼Œ{num_samples} ä¸ªæ ·æœ¬ï¼‰...\\\")\\n        original_reports = []\\n        for prompt in test_prompts:\\n            report = self.generate_report(\\n                self.original_model, self.original_tokenizer, prompt\\n            )\\n            original_reports.append(report)\\n\\n        del self.original_model\\n        del self.original_tokenizer\\n        if torch.cuda.is_available():\\n            torch.cuda.empty_cache()\\n\\n        quant_load = self.load_quantized_model()\\n        results[\\\"load_times\\\"].update(quant_load)\\n\\n        print(\\\"\\\\næµ‹é‡æ˜¾å­˜å ç”¨ï¼ˆé‡åŒ–æ¨¡å‹ï¼‰...\\\")\\n        quantized_mem = self.measure_memory(self.quantized_model)\\n\\n        print(\\\"é‡åŒ–æ¨¡å‹é€Ÿåº¦æµ‹è¯•...\\\")\\n        quantized_speed = self.benchmark_inference_speed(\\n            self.quantized_model,\\n            self.quantized_tokenizer,\\n            test_prompts,\\n            num_runs=min(10, num_samples),\\n        )\\n\\n        print(f\\\"\\\\nç”ŸæˆæŠ¥å‘Šï¼ˆé‡åŒ–æ¨¡å‹ï¼Œ{num_samples} ä¸ªæ ·æœ¬ï¼‰...\\\")\\n        quantized_reports = []\\n        for prompt in test_prompts:\\n            report = self.generate_report(\\n                self.quantized_model, self.quantized_tokenizer, prompt\\n            )\\n            quantized_reports.append(report)\\n\\n        results[\\\"memory\\\"] = {\\n            \\\"original\\\": original_mem,\\n            \\\"quantized\\\": quantized_mem,\\n            \\\"reduction_percent\\\": (\\n                (original_mem[\\\"peak_gb\\\"] - quantized_mem[\\\"peak_gb\\\"])\\n                / original_mem[\\\"peak_gb\\\"]\\n                * 100\\n            )\\n            if original_mem[\\\"peak_gb\\\"] > 0\\n            else 0,\\n        }\\n\\n        results[\\\"inference_speed\\\"] = {\\n            \\\"original\\\": original_speed,\\n            \\\"quantized\\\": quantized_speed,\\n            \\\"speedup\\\": quantized_speed[\\\"tokens_per_sec\\\"]\\n            / original_speed[\\\"tokens_per_sec\\\"],\\n        }\\n\"\n",
        "\n",
        "if old_block not in text:\n",
        "    raise SystemExit(\"æœªæ‰¾åˆ°åŸå§‹è¯„ä¼°æ®µè½ï¼Œè¯·æ‰‹åŠ¨åŒæ­¥é¡¹ç›®\")\n",
        "text = text.replace(old_block, new_block)\n",
        "\n",
        "path.write_text(text)\n",
        "print(\"âœ… evaluate_awq_model.py å·²æ›´æ–°\")\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY0S4fsuqFFp",
        "outputId": "c39b4be1-e791-49e1-8c34-51ef6d466e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
            "I have left this message as the final dev message to help you transition.\n",
            "\n",
            "Important Notice:\n",
            "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
            "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
            "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
            "\n",
            "Alternative:\n",
            "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
            "\n",
            "For further inquiries, feel free to reach out:\n",
            "- X: https://x.com/casper_hansen_\n",
            "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
            "\n",
            "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py:1480: DeprecationWarning: `torch.jit.script` is deprecated. Please switch to `torch.compile` or `torch.export`.\n",
            "  warnings.warn(\n",
            "======================================================================\n",
            "MedGamma AWQ é‡åŒ–è¯„ä¼°\n",
            "======================================================================\n",
            "\n",
            "åŠ è½½æ¨¡å‹...\n",
            "  åŠ è½½åŸå§‹æ¨¡å‹...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading weights: 100% 291/291 [00:04<00:00, 61.18it/s, Materializing param=model.norm.weight]\n",
            "  åŸå§‹æ¨¡å‹è€—æ—¶: 8.20ç§’\n",
            "\n",
            "åŠ è½½è¯„ä¼°æ•°æ®: /content/mimic_eval_ready_step1.csv\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 403, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 399, in main\n",
            "    evaluator.run_evaluation(num_samples=args.num_samples)\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 214, in run_evaluation\n",
            "    df = self.load_eval_data(num_samples)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py\", line 121, in load_eval_data\n",
            "    df = pd.read_csv(self.eval_data_path)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\", line 873, in get_handle\n",
            "    handle = open(\n",
            "             ^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/mimic_eval_ready_step1.csv'\n",
            "sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/medgamma/scripts/evaluate_awq_model.py \\\n",
        "  --original_model \"mistralai/Mistral-7B-Instruct-v0.2\" \\\n",
        "  --quantized_model \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \\\n",
        "  --eval_data \"/content/mimic_eval_ready_step1.csv\" \\\n",
        "  --prompt_file \"/content/drive/MyDrive/medgamma/prompts/example_prompt.txt\" \\\n",
        "  --num_samples 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srBGfJA_6c1U",
        "outputId": "b702aa90-b236-4e39-b658-6f30a1a9692b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.9G\t/root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-AWQ\n"
          ]
        }
      ],
      "source": [
        "!du -sh /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-AWQ"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f207a643064985be800b19b8081ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cdad0076ea04e09bbc2e748d69c9e31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dbe9e38a4964c95a2790d6567bd6712": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "11b7020d30d74335a172dd65e288aa90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c0072ea1d6b44a5ab531d8e515e2d9b",
            "max": 883,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f0da691048542feafc2edf23c4af313",
            "value": 883
          }
        },
        "1c58112713d5494fbc7c1cd4e85a935d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_0dbe9e38a4964c95a2790d6567bd6712"
          }
        },
        "1daa82817455488b97159af2374e698f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20bd04dd011f4801ad42fc1b51394c91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "340196720e4e4238b268f45a1a0fe9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b107becf045417fa87fdf2d21491b59",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5bedadcd9002498b9152b7c890e4d758",
            "value": "â€‡1360/1360â€‡[00:02&lt;00:00,â€‡705.91it/s,â€‡Materializingâ€‡param=model.vision_tower.vision_model.post_layernorm.weight]"
          }
        },
        "343c030b347041ffa2ad23b9793154d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4d0b7888c8c4da8ab3a0bae24688e72",
              "IPY_MODEL_11b7020d30d74335a172dd65e288aa90",
              "IPY_MODEL_5f544e073f1d466ab559eaedeb4c6494"
            ],
            "layout": "IPY_MODEL_4b980f2fd44d41ba90f2ad31b91bb789"
          }
        },
        "35b4f142c7b543d18d1ba5bf184cfa53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8ed0c33a31b4b078d79c8e112f91e2a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aef96c6cb98e46f9a5e0226cde2dc6bc",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "3eefacd5c1c54b7b85bd3866069d7c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0a32785322452baf37aa2af279d3e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4630014ea3d6460882e32981bd695677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_87929da1bb2b45bcab772db5ab79048d",
            "style": "IPY_MODEL_bb9d038b63fe488cb912b45891e8cbab",
            "value": true
          }
        },
        "498fd95b7736478b88187f86fc967cb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4b980f2fd44d41ba90f2ad31b91bb789": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd7b855627743dcbeb7e1f43194d9f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "577c48d835e04255adec5775498ef708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57d662a77a81431a812673bbfa0f8a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0a93e180112406b8de70d94172152f5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_577c48d835e04255adec5775498ef708",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "59bd09fca1db4ccabd510bb47ac2f6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eefacd5c1c54b7b85bd3866069d7c5e",
            "max": 1360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0af105ede7346fc894f8f439faa61bb",
            "value": 1360
          }
        },
        "5a7e79bac35c45ffa64d68cc7b1ddb4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d09a067bc8494674ba91b71c7691c900",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fc7d8ad817cf420b934fb853ac15397f",
            "value": ""
          }
        },
        "5bedadcd9002498b9152b7c890e4d758": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c0072ea1d6b44a5ab531d8e515e2d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f544e073f1d466ab559eaedeb4c6494": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a96a7ed58334dac9f60879bb49332dc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_01f207a643064985be800b19b8081ce9",
            "value": "â€‡883/883â€‡[00:02&lt;00:00,â€‡703.72it/s,â€‡Materializingâ€‡param=model.vision_tower.vision_model.post_layernorm.weight]"
          }
        },
        "5fb288ab2c0a4007bac9f8a6d404df1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "675660d07bc04df6a9f0640e4a98b064": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aced7535dc22470d89d505375fb7651c",
              "IPY_MODEL_bf227b1952d34ad08794deb9392c0056",
              "IPY_MODEL_70af394ca22f46849b9bf4a216996f3a"
            ],
            "layout": "IPY_MODEL_73024119dac84b59b2ddf5a480bd73ae"
          }
        },
        "6f0da691048542feafc2edf23c4af313": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70af394ca22f46849b9bf4a216996f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8349816faca941a08950299691a201f6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d8bc41422c274bf2b27f946a258fd90e",
            "value": "â€‡883/883â€‡[00:03&lt;00:00,â€‡841.72it/s,â€‡Materializingâ€‡param=model.vision_tower.vision_model.post_layernorm.weight]"
          }
        },
        "73024119dac84b59b2ddf5a480bd73ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b9e9370ec3416588aee68ebc5d228a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7639255bbe9449cc8af5ebfdc142b826": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a96a7ed58334dac9f60879bb49332dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b107becf045417fa87fdf2d21491b59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82e5e26ca9b843f7ad5f63a58c323861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9224092cf9c2402a84252cb4c00c78ec",
              "IPY_MODEL_59bd09fca1db4ccabd510bb47ac2f6bf",
              "IPY_MODEL_340196720e4e4238b268f45a1a0fe9e9"
            ],
            "layout": "IPY_MODEL_e130447fa64948a281f806d90063b802"
          }
        },
        "8349816faca941a08950299691a201f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87929da1bb2b45bcab772db5ab79048d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9afa0f475a4e229bdf5df00fd9a727": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d920a94680141b0b7c5460f809016a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd7e3945d9544cc69771b54a79a196a2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_74b9e9370ec3416588aee68ebc5d228a",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "9224092cf9c2402a84252cb4c00c78ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20bd04dd011f4801ad42fc1b51394c91",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5fb288ab2c0a4007bac9f8a6d404df1c",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "95e59ba9483a4acbb9562a86565d5db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7639255bbe9449cc8af5ebfdc142b826",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3f0a32785322452baf37aa2af279d3e6",
            "value": "Connecting..."
          }
        },
        "a1d5363d707b41e2805298e8593bae22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac22c8d24fb5432e83be55ed08840d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aced7535dc22470d89d505375fb7651c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c9afa0f475a4e229bdf5df00fd9a727",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c020ec8eb8cc446fa1ca0b794f0653d1",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "aef96c6cb98e46f9a5e0226cde2dc6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b292fb035b1247ec93ea742c7d60ac9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc9d1aa163db43b497e1faf7c41aa58b",
            "max": 883,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac22c8d24fb5432e83be55ed08840d1e",
            "value": 883
          }
        },
        "ba3c54989cdd497ca44d6a9e62b1b4c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9d038b63fe488cb912b45891e8cbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf227b1952d34ad08794deb9392c0056": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d5363d707b41e2805298e8593bae22",
            "max": 883,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbe180b16e94454daf30a1b53ba27645",
            "value": 883
          }
        },
        "c020ec8eb8cc446fa1ca0b794f0653d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5f111415e5a48018e56915febe4eeff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe180b16e94454daf30a1b53ba27645": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ceee0be3171941469e7a9c59af7141e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cd7b855627743dcbeb7e1f43194d9f7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1daa82817455488b97159af2374e698f",
            "value": "â€‡883/883â€‡[00:02&lt;00:00,â€‡756.78it/s,â€‡Materializingâ€‡param=model.vision_tower.vision_model.post_layernorm.weight]"
          }
        },
        "d09a067bc8494674ba91b71c7691c900": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d256895df8e844469b87756b3f86e4e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0cdad0076ea04e09bbc2e748d69c9e31",
            "style": "IPY_MODEL_498fd95b7736478b88187f86fc967cb2",
            "tooltip": ""
          }
        },
        "d4d0b7888c8c4da8ab3a0bae24688e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba3c54989cdd497ca44d6a9e62b1b4c0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dd80b20e7ec047c4824c04a94f5b34bb",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "d8bc41422c274bf2b27f946a258fd90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd80b20e7ec047c4824c04a94f5b34bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0a93e180112406b8de70d94172152f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e130447fa64948a281f806d90063b802": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ed0c33a31b4b078d79c8e112f91e2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eccd8d3320fe47f6b6600b1f2e67fc3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d920a94680141b0b7c5460f809016a2",
              "IPY_MODEL_b292fb035b1247ec93ea742c7d60ac9a",
              "IPY_MODEL_ceee0be3171941469e7a9c59af7141e9"
            ],
            "layout": "IPY_MODEL_c5f111415e5a48018e56915febe4eeff"
          }
        },
        "f0af105ede7346fc894f8f439faa61bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc7d8ad817cf420b934fb853ac15397f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc9d1aa163db43b497e1faf7c41aa58b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd7e3945d9544cc69771b54a79a196a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
