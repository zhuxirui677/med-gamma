#!/usr/bin/env python3
"""
åœ¨ Kaggle/Colab ä¸­è¿è¡Œï¼Œå°† 233 å¼  CXR å›¾ç‰‡ä¸Šä¼ åˆ° Hugging Face Datasetã€‚
è¿è¡Œå‰ï¼špip install huggingface_hub pandas
ç¯å¢ƒå˜é‡ï¼šHF_TOKENï¼ˆä½ çš„ HF Write tokenï¼‰

ç”¨æ³•ï¼š
  export HF_TOKEN='hf_xxx'
  python upload_cxr_to_hf_dataset.py
"""

import os
import csv
from pathlib import Path
from huggingface_hub import HfApi, create_repo

HF_TOKEN = os.environ.get("HF_TOKEN")
if not HF_TOKEN:
    raise SystemExit("è¯·è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡")

# é…ç½®
CSV_PATH = "mimic_eval_single_image_final_233.csv"
DATASET_REPO = "cxr-233-images"  # æˆ– "ä½ çš„ç”¨æˆ·å/cxr-233-images"
DATASET_ROOT = "/kaggle/input/mimic-cxr-dataset"  # Kaggle è·¯å¾„
# Colab è‹¥ç”¨ kagglehub: dataset_path = kagglehub.dataset_download("simhadrisadaram/mimic-cxr-dataset")

# å¤‡é€‰è·¯å¾„ï¼ˆkagglehub ç¼“å­˜ï¼‰
ALT_ROOTS = [
    "/kaggle/input/mimic-cxr-dataset",
    "/root/.cache/kagglehub/datasets/simhadrisadaram/mimic-cxr-dataset/versions/2/official_data_iccv_final",
]

def find_csv():
    candidates = [
        CSV_PATH,
        "mimic_eval_single_image_final_233.csv",
        "/kaggle/working/mimic_eval_single_image_final_233.csv",
        "/kaggle/input/clean-data/mimic_eval_single_image_final_233.csv",
        "../mimic_eval_single_image_final_233.csv",
    ]
    try:
        candidates.append(str(Path(__file__).resolve().parent.parent / "mimic_eval_single_image_final_233.csv"))
    except Exception:
        pass
    for p in candidates:
        if os.path.exists(p):
            return p
    raise FileNotFoundError("æœªæ‰¾åˆ° mimic_eval_single_image_final_233.csvï¼Œè¯·æ”¾åœ¨å½“å‰ç›®å½•æˆ– /kaggle/working/")

def find_image_path(raw_path: str) -> str | None:
    """ä» CSV çš„ raw_path è§£æå‡ºå®é™…å¯è®¿é—®çš„æœ¬åœ°è·¯å¾„"""
    raw = raw_path.strip()
    if os.path.exists(raw):
        return raw
    # å°è¯•ä¸åŒæ ¹è·¯å¾„
    candidates = [
        raw,
        raw.replace("/root/.cache/kagglehub/datasets/simhadrisadaram/mimic-cxr-dataset/versions/2/official_data_iccv_final", "/kaggle/input/mimic-cxr-dataset/official_data_iccv_final"),
        os.path.join(DATASET_ROOT, raw.split("mimic-cxr-dataset/")[-1]) if "mimic-cxr-dataset" in raw else raw,
    ]
    for p in candidates:
        if p and os.path.exists(p):
            return p
    return None

def main():
    csv_path = find_csv()
    api = HfApi(token=HF_TOKEN)
    info = api.whoami()
    username = info["name"]
    repo_id = f"{username}/{DATASET_REPO}"
    create_repo(repo_id, repo_type="dataset", private=False, exist_ok=True, token=HF_TOKEN)
    print(f"âœ… Dataset: https://huggingface.co/datasets/{repo_id}")

    rows = []
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        fieldnames = list(reader.fieldnames)
        for i, row in enumerate(reader):
            raw_path = row.get("Image_Path", row.get("image_path", ""))
            local_path = find_image_path(raw_path)
            if local_path and os.path.exists(local_path):
                fname = f"{row['subject_id']}_{i}.jpg"
                api.upload_file(
                    path_or_fileobj=local_path,
                    path_in_repo=fname,
                    repo_id=repo_id,
                    repo_type="dataset",
                    token=HF_TOKEN,
                )
                url = f"https://huggingface.co/datasets/{repo_id}/resolve/main/{fname}"
                row["Image_Path"] = url  # ç”¨ URL æ›¿æ¢åŸè·¯å¾„ï¼Œå‰ç«¯å¯ç›´æ¥ç”¨
                print(f"  [{i+1}/233] {fname}")
            else:
                row["Image_Path"] = ""  # æ‰¾ä¸åˆ°åˆ™ç•™ç©º
                print(f"  [{i+1}/233] âš ï¸ æœªæ‰¾åˆ°: {raw_path[:60]}...")
            rows.append(row)

    out_csv = "reports-data-with-urls.csv"
    with open(out_csv, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
        w.writeheader()
        w.writerows(rows)
    print(f"\nâœ… å·²ç”Ÿæˆ: {out_csv}")
    print(f"ğŸ‘‰ å¤åˆ¶åˆ° medgamma-frontend/lib/reports-data.csv å¹¶æ›´æ–°å‰ç«¯")

if __name__ == "__main__":
    main()
