{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHDOlbZMl-Fx"
      },
      "source": [
        "# MedGemma 1.5 W4A8 é‡åŒ– - èƒ¸éƒ¨ X å…‰æŠ¥å‘Šç”Ÿæˆä¸ RadGraph F1 è¯„ä¼°\n",
        "\n",
        "**æ¨¡å‹**: Google MedGemma 1.5 (4B) + W4A8 é‡åŒ–\n",
        "\n",
        "**é‡åŒ–é…ç½®**: 4-bit æƒé‡ + 8-bit æ¿€æ´» (bitsandbytes)\n",
        "\n",
        "**æ•°æ®é›†**: MIMIC-CXR (233 samples)\n",
        "\n",
        "**è¯„ä¼°æŒ‡æ ‡**: RadGraph F1 Score\n",
        "\n",
        "**GPU**: A100 / H100\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ ç¯å¢ƒè¦æ±‚\n",
        "\n",
        "- **Python**: 3.10-3.12ï¼ˆColab é»˜è®¤ 3.12 å¯ç”¨ï¼‰\n",
        "- **GPU**: A100 æˆ– H100ï¼ˆW4A8 é‡åŒ–åéœ€çº¦ 5-7GB æ˜¾å­˜ï¼‰\n",
        "- **ä¾èµ–**: transformers 5.x, radgraph, bitsandbytes\n",
        "- **HuggingFace**: éœ€è·å– MedGemma è®¿é—®æƒé™å’Œ tokenï¼ˆè§ Step 2ï¼‰\n",
        "- **Google Drive**: éœ€ä¸Šä¼  `mimic_eval_single_image_final_233.csv`\n",
        "\n",
        "## ğŸš¨ ä½¿ç”¨å‰å¿…è¯»\n",
        "\n",
        "1. **Step 0**: æ£€æŸ¥ Python ç‰ˆæœ¬ï¼ˆ3.10-3.12 å‡å¯ï¼‰\n",
        "2. **Step 2**: âš ï¸ **å¿…é¡»å…ˆç™»å½• HuggingFace**ï¼ˆMedGemma æ˜¯ gated modelï¼‰\n",
        "   - ç”³è¯·è®¿é—®ï¼šhttps://huggingface.co/google/medgemma-1.5-4b-it\n",
        "   - è·å– tokenï¼šhttps://huggingface.co/settings/tokens\n",
        "   - åœ¨ Colab å·¦ä¾§ ğŸ”‘ Secrets ä¸­æ·»åŠ  tokenï¼ˆåç§°ï¼š`zhuxirui11` æˆ– `HF_TOKEN`ï¼‰\n",
        "3. **ä¾æ¬¡è¿è¡Œæ‰€æœ‰ cell**ï¼ˆæ— éœ€æ‰‹åŠ¨ Restart Runtimeï¼‰\n",
        "\n",
        "---\n",
        "\n",
        "## æµç¨‹\n",
        "\n",
        "0. æ£€æŸ¥ Python ç‰ˆæœ¬ï¼ˆ3.10-3.12ï¼‰\n",
        "1. å®‰è£…ä¾èµ–ï¼ˆtransformers + radgraph + **bitsandbytes**ï¼‰\n",
        "2. **ç™»å½• HuggingFace**ï¼ˆå¿…éœ€ï¼ï¼‰\n",
        "3. æŒ‚è½½ Google Drive\n",
        "4. ä¸‹è½½ MIMIC-CXR æ•°æ®é›†ï¼ˆkagglehubï¼‰\n",
        "5. å¯¹é½ 233 CSV çš„å›¾ç‰‡è·¯å¾„\n",
        "6. **åŠ è½½ MedGemma 1.5 W4A8 é‡åŒ–æ¨¡å‹**\n",
        "7. æ‰¹é‡ç”ŸæˆæŠ¥å‘Šï¼ˆ233 samplesï¼‰\n",
        "8. RadGraph F1 è¯„ä¼°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXiV-kzil-F0"
      },
      "source": [
        "## Step 0: æ£€æŸ¥ Python ç‰ˆæœ¬\n",
        "\n",
        "â„¹ï¸ **è¯´æ˜**ï¼š\n",
        "- **æ¨è**: Python 3.10 æˆ– 3.11\n",
        "- **Colab é»˜è®¤**: Python 3.12\n",
        "- **å…¼å®¹æ€§**: transformers å’Œ radgraph æ”¯æŒ 3.10-3.12\n",
        "- **W4A8 é‡åŒ–**: bitsandbytes åœ¨ 3.12 ä¸Šå¯èƒ½æœ‰å°é—®é¢˜\n",
        "\n",
        "å¦‚æœé‡åˆ° bitsandbytes ç›¸å…³é”™è¯¯ï¼Œå¯ä»¥å°è¯•ï¼š\n",
        "- Runtime â†’ Change runtime type â†’ é€‰æ‹© Python 3.10ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
        "- æˆ–è¿è¡Œï¼š`!conda install -y python=3.10`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl-HZxskl-F1",
        "outputId": "4e4b0a27-2463-4fb5-9802-fb393c3b85c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å½“å‰ Python ç‰ˆæœ¬: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "\n",
            "======================================================================\n",
            "â„¹ï¸ æ£€æµ‹åˆ° Python 3.12\n",
            "\n",
            "transformers å’Œ radgraph éƒ½æ”¯æŒ Python 3.12ï¼\n",
            "\n",
            "âš ï¸ æ³¨æ„ï¼šW4A8 é‡åŒ–ä½¿ç”¨çš„ bitsandbytes åœ¨ 3.12 ä¸Šå¯èƒ½æœ‰å°é—®é¢˜\n",
            "å¦‚æœé‡åˆ°å…¼å®¹æ€§é”™è¯¯ï¼Œå¯ä»¥å°è¯•ï¼š\n",
            "1. Runtime â†’ Change runtime type â†’ Python 3.10\n",
            "2. æˆ–è¿è¡Œï¼š!conda install -y python=3.10\n",
            "======================================================================\n",
            "\n",
            "âœ… ç»§ç»­ä½¿ç”¨ Python 3.12ï¼ˆå…ˆå°è¯•ï¼Œé‡åˆ°é—®é¢˜å†åˆ‡æ¢ï¼‰\n",
            "\n",
            "è·³è¿‡ Python å®‰è£…ï¼Œç›´æ¥è¿›å…¥ Step 1\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "print(f\"å½“å‰ Python ç‰ˆæœ¬: {sys.version}\")\n",
        "\n",
        "# æ£€æŸ¥ Python ç‰ˆæœ¬\n",
        "py_major = sys.version_info.major\n",
        "py_minor = sys.version_info.minor\n",
        "\n",
        "if py_major == 3 and py_minor in [10, 11]:\n",
        "    print(f\"âœ… Python 3.{py_minor} ç¬¦åˆæ¨èè¦æ±‚\")\n",
        "elif py_major == 3 and py_minor == 12:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"â„¹ï¸ æ£€æµ‹åˆ° Python 3.12\")\n",
        "    print(\"\\ntransformers å’Œ radgraph éƒ½æ”¯æŒ Python 3.12ï¼\")\n",
        "    print(\"\\nâš ï¸ æ³¨æ„ï¼šW4A8 é‡åŒ–ä½¿ç”¨çš„ bitsandbytes åœ¨ 3.12 ä¸Šå¯èƒ½æœ‰å°é—®é¢˜\")\n",
        "    print(\"å¦‚æœé‡åˆ°å…¼å®¹æ€§é”™è¯¯ï¼Œå¯ä»¥å°è¯•ï¼š\")\n",
        "    print(\"1. Runtime â†’ Change runtime type â†’ Python 3.10\")\n",
        "    print(\"2. æˆ–è¿è¡Œï¼š!conda install -y python=3.10\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nâœ… ç»§ç»­ä½¿ç”¨ Python 3.12ï¼ˆå…ˆå°è¯•ï¼Œé‡åˆ°é—®é¢˜å†åˆ‡æ¢ï¼‰\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ æœªçŸ¥ Python ç‰ˆæœ¬: {py_major}.{py_minor}\")\n",
        "    print(\"å»ºè®®ä½¿ç”¨ Python 3.10-3.12\")\n",
        "\n",
        "print(\"\\nè·³è¿‡ Python å®‰è£…ï¼Œç›´æ¥è¿›å…¥ Step 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXgUlQxWl-F2"
      },
      "source": [
        "## Step 1: å®‰è£…ä¾èµ–\n",
        "\n",
        "â„¹ï¸ **è¯´æ˜**ï¼šå®‰è£… W4A8 é‡åŒ–æ‰€éœ€çš„ä¾èµ–ï¼ˆtransformers + radgraph + bitsandbytesï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LJy4Jmgl-F3",
        "outputId": "2ef22b95-4334-4d95-fd5d-1650c90d283c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å½“å‰ Python ç‰ˆæœ¬: 3.12\n",
            "âœ… Python 3.12 å…¼å®¹\n",
            "\n",
            "æ­£åœ¨å®‰è£…ä¾èµ–...\n",
            "\n",
            "âœ… ä¾èµ–å®‰è£…å®Œæˆï¼ˆåŒ…å« bitsandbytesï¼‰\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# éªŒè¯ Python ç‰ˆæœ¬\n",
        "py_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
        "print(f\"å½“å‰ Python ç‰ˆæœ¬: {py_version}\")\n",
        "\n",
        "if sys.version_info.major == 3 and sys.version_info.minor in [10, 11, 12]:\n",
        "    print(f\"âœ… Python {py_version} å…¼å®¹\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ è­¦å‘Šï¼šå½“å‰ç‰ˆæœ¬ {py_version} æœªæµ‹è¯•ï¼Œæ¨è 3.10-3.12\")\n",
        "\n",
        "# å®‰è£…ä¾èµ–ï¼ˆW4A8 éœ€è¦ bitsandbytesï¼‰\n",
        "print(\"\\næ­£åœ¨å®‰è£…ä¾èµ–...\")\n",
        "!pip install -U -q transformers\n",
        "!pip install -q radgraph\n",
        "!pip install -q bitsandbytes  # W4A8 é‡åŒ–å¿…éœ€\n",
        "print(\"\\nâœ… ä¾èµ–å®‰è£…å®Œæˆï¼ˆåŒ…å« bitsandbytesï¼‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUrReO0Fl-F3"
      },
      "source": [
        "## Step 2: æŒ‚è½½ Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT0c-ED1l-F3",
        "outputId": "70ad9e4b-4469-4d12-e451-a33e66b59df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Drive å·²æŒ‚è½½\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ… Google Drive å·²æŒ‚è½½\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enpmsbWSmycd",
        "outputId": "ff040344-d7d2-49f4-a969-c74ca274c6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å½“å‰ Python ç‰ˆæœ¬: 3.12\n",
            "âœ… Python 3.12 å…¼å®¹\n",
            "\n",
            "æ­£åœ¨å®‰è£…ä¾èµ–...\n",
            "\n",
            "âœ… ä¾èµ–å®‰è£…å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# éªŒè¯ Python ç‰ˆæœ¬\n",
        "py_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
        "print(f\"å½“å‰ Python ç‰ˆæœ¬: {py_version}\")\n",
        "\n",
        "if sys.version_info.major == 3 and sys.version_info.minor in [10, 11, 12]:\n",
        "    print(f\"âœ… Python {py_version} å…¼å®¹\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ è­¦å‘Šï¼šå½“å‰ç‰ˆæœ¬ {py_version} æœªæµ‹è¯•ï¼Œæ¨è 3.10-3.12\")\n",
        "\n",
        "# å®‰è£…ä¾èµ–\n",
        "print(\"\\næ­£åœ¨å®‰è£…ä¾èµ–...\")\n",
        "!pip install -U -q transformers\n",
        "!pip install -q radgraph\n",
        "print(\"\\nâœ… ä¾èµ–å®‰è£…å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zhn-Pmgl-F3"
      },
      "source": [
        "## Step 3: ä¸‹è½½ MIMIC-CXR æ•°æ®é›†\n",
        "\n",
        "ä½¿ç”¨ kagglehub ä¸‹è½½ MIMIC-CXR æ•°æ®é›†ï¼ˆ~18GBï¼‰ã€‚é¦–æ¬¡è¿è¡Œéœ€è¦ 5-10 åˆ†é’Ÿã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2BpOCgGl-F4",
        "outputId": "50f8848a-b533-44f0-ddda-6057b2287c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ å¼€å§‹ä¸‹è½½ MIMIC-CXR æ•°æ®é›†ï¼ˆ~18GBï¼Œé¦–æ¬¡è¿è¡Œéœ€è¦ 5-10 åˆ†é’Ÿï¼‰...\n",
            "Using Colab cache for faster access to the 'mimic-cxr-dataset' dataset.\n",
            "âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼\n",
            "ğŸ“‚ å­˜å‚¨è·¯å¾„: /kaggle/input/mimic-cxr-dataset\n",
            "\n",
            "--- æ–‡ä»¶å¤¹ç»“æ„é¢„è§ˆ ---\n",
            "mimic-cxr-dataset/\n",
            "  official_data_iccv_final/\n",
            "    files/\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "print(\"ğŸš€ å¼€å§‹ä¸‹è½½ MIMIC-CXR æ•°æ®é›†ï¼ˆ~18GBï¼Œé¦–æ¬¡è¿è¡Œéœ€è¦ 5-10 åˆ†é’Ÿï¼‰...\")\n",
        "\n",
        "# kagglehub ä¼šè‡ªåŠ¨ç¼“å­˜ï¼Œç¬¬äºŒæ¬¡è¿è¡Œä¼šå¾ˆå¿«\n",
        "dataset_path = kagglehub.dataset_download(\"simhadrisadaram/mimic-cxr-dataset\")\n",
        "\n",
        "print(f\"âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼\")\n",
        "print(f\"ğŸ“‚ å­˜å‚¨è·¯å¾„: {dataset_path}\")\n",
        "\n",
        "# éªŒè¯æ–‡ä»¶ç»“æ„\n",
        "print(\"\\n--- æ–‡ä»¶å¤¹ç»“æ„é¢„è§ˆ ---\")\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    level = root.replace(dataset_path, '').count(os.sep)\n",
        "    indent = '  ' * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    if level > 1: break  # åªæ˜¾ç¤ºå‰ 2 å±‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrvvidZAl-F4"
      },
      "source": [
        "## Step 4: åŠ è½½ 233 CSV å¹¶å¯¹é½è·¯å¾„\n",
        "\n",
        "è¯»å– `mimic_eval_single_image_final_233.csv`ï¼Œå¹¶å°† CSV ä¸­çš„è·¯å¾„å¯¹é½åˆ° kagglehub ä¸‹è½½çš„å®é™…è·¯å¾„ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8SNZeIEl-F4",
        "outputId": "f1074abd-8bfa-4261-d984-a4f8b283431e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ è¯»å– CSV: /content/drive/MyDrive/medgamma/mimic_eval_single_image_final_233.csv\n",
            "âœ… åŠ è½½æˆåŠŸï¼å…± 233 æ¡æ•°æ®\n",
            "\n",
            "åˆ—å: ['subject_id', 'View', 'Image_Path', 'Ground_Truth', 'Generated_Report']\n",
            "\n",
            "ğŸ”„ æ­£åœ¨å¯¹é½å›¾ç‰‡è·¯å¾„...\n",
            "âœ… è·¯å¾„å¯¹é½å®Œæˆï¼æœ‰æ•ˆè·¯å¾„: 233/233\n",
            "ç¤ºä¾‹è·¯å¾„: /kaggle/input/mimic-cxr-dataset/official_data_iccv_final/files/p10/p10075925/s51010496/2d783c8a-492984b7-28aaf571-bfc30156-61ab26f6.jpg\n",
            "æ–‡ä»¶å­˜åœ¨: True\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# âœ… ä½¿ç”¨ 233 CSV\n",
        "csv_path = \"/content/drive/MyDrive/medgamma/mimic_eval_single_image_final_233.csv\"\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"âŒ æ‰¾ä¸åˆ° CSV æ–‡ä»¶: {csv_path}\")\n",
        "    print(\"\\nè¯·ç¡®ä¿ 233 CSV å·²ä¸Šä¼ åˆ° Google Drive çš„ medgamma æ–‡ä»¶å¤¹ä¸­ï¼\")\n",
        "    raise FileNotFoundError(csv_path)\n",
        "\n",
        "print(f\"ğŸ“‚ è¯»å– CSV: {csv_path}\")\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"âœ… åŠ è½½æˆåŠŸï¼å…± {len(df)} æ¡æ•°æ®\")\n",
        "print(f\"\\nåˆ—å: {list(df.columns)}\")\n",
        "\n",
        "# ==========================================\n",
        "# è·¯å¾„å¯¹é½å‡½æ•°\n",
        "# ==========================================\n",
        "dataset_root = f\"{dataset_path}/official_data_iccv_final\"\n",
        "\n",
        "def fix_image_path(path_in_csv):\n",
        "    \"\"\"\n",
        "    å°† 233 CSV ä¸­çš„è·¯å¾„å¯¹é½åˆ° kagglehub ä¸‹è½½çš„å®é™…è·¯å¾„\n",
        "    CSV æ ¼å¼: /kaggle/input/mimic-cxr-dataset/official_data_iccv_final/files/...\n",
        "    å®é™…æ ¼å¼: {dataset_root}/files/...\n",
        "    \"\"\"\n",
        "    if pd.isna(path_in_csv):\n",
        "        return None\n",
        "\n",
        "    path_str = str(path_in_csv).strip()\n",
        "\n",
        "    # æå–ä» files/ å¼€å§‹çš„ç›¸å¯¹è·¯å¾„\n",
        "    if 'files/' in path_str:\n",
        "        relative_part = path_str.split('files/', 1)[1]\n",
        "        full_path = os.path.join(dataset_root, 'files', relative_part)\n",
        "        return full_path if os.path.exists(full_path) else None\n",
        "\n",
        "    return None\n",
        "\n",
        "# åº”ç”¨è·¯å¾„ä¿®æ­£\n",
        "print(\"\\nğŸ”„ æ­£åœ¨å¯¹é½å›¾ç‰‡è·¯å¾„...\")\n",
        "if 'Image_Path' in df.columns:\n",
        "    df['Image_Path'] = df['Image_Path'].apply(fix_image_path)\n",
        "    # éªŒè¯\n",
        "    valid_count = df['Image_Path'].notna().sum()\n",
        "    print(f\"âœ… è·¯å¾„å¯¹é½å®Œæˆï¼æœ‰æ•ˆè·¯å¾„: {valid_count}/{len(df)}\")\n",
        "\n",
        "    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæœ‰æ•ˆè·¯å¾„\n",
        "    first_valid = df['Image_Path'].dropna().iloc[0]\n",
        "    print(f\"ç¤ºä¾‹è·¯å¾„: {first_valid}\")\n",
        "    print(f\"æ–‡ä»¶å­˜åœ¨: {os.path.exists(first_valid)}\")\n",
        "else:\n",
        "    print(\"âŒ CSV ä¸­æ²¡æœ‰ Image_Path åˆ—ï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9anM7QWcl-F5"
      },
      "source": [
        "## Step 6: åŠ è½½ MedGemma 1.5 W4A8 é‡åŒ–æ¨¡å‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOEEjrNvrmP4",
        "outputId": "d834e292-8806-4ef9-e3a5-8e027423ec4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… HuggingFace ç™»å½•æˆåŠŸï¼\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    # ä½¿ç”¨ä½ å·²è®¾ç½®çš„ Secret åç§°ï¼ˆzhuxirui11ï¼‰\n",
        "    hf_token = userdata.get('zhuxirui11')\n",
        "    login(token=hf_token)\n",
        "    print(\"âœ… HuggingFace ç™»å½•æˆåŠŸï¼\")\n",
        "except Exception as e:\n",
        "    print(\"âŒ ç™»å½•å¤±è´¥ï¼\")\n",
        "    print(f\"é”™è¯¯ä¿¡æ¯: {e}\")\n",
        "    print(\"\\nè¯·ç¡®è®¤ï¼š\")\n",
        "    print(\"1. å·²åœ¨ https://huggingface.co/google/medgemma-1.5-4b-it ç”³è¯·è®¿é—®æƒé™\")\n",
        "    print(\"2. å·²åœ¨ Colab å·¦ä¾§ ğŸ”‘ Secrets ä¸­æ·»åŠ  zhuxirui11\")\n",
        "    print(\"3. å·²å‹¾é€‰ 'Notebook access'ï¼ˆå¼€å…³æ˜¯è“è‰²çš„ï¼‰\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhEFjHqM-lof",
        "outputId": "585941dd-538e-4a43-de14-c78e6cd04db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu128)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "âœ… bitsandbytes å®‰è£…å®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "# å®‰è£… bitsandbytes\n",
        "!pip install -U bitsandbytes\n",
        "print(\"âœ… bitsandbytes å®‰è£…å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "dd9539a1e21442f98b829935f56e2d04",
            "69af1d7a832a486eafedaae356e43bdf",
            "523277e21be348698d9fdd42750744e2",
            "f2e6c18de336486286e7bb0ceca6c0d4",
            "4c7e58d3b8de40749a2e0d3b9855ee81",
            "a7f1fa646b6d472780b4e13f657304de",
            "35ca7e176e7b40228b9a8c093c11081a",
            "e45deffe714246c4b4e81cc97e4495ea",
            "38ede464f0df486a89346fd380d4b947",
            "ee50228985f442d5bc425d7e70851502",
            "a5b4946ab34c4fa3a0f442a9459e2dfa"
          ]
        },
        "id": "bXShqCp-l-F5",
        "outputId": "024523a1-59c9-4302-dddf-1bfb6c1f9100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹: google/medgemma-1.5-4b-it (W4A8 é‡åŒ–)...\n",
            "âš ï¸ é¦–æ¬¡åŠ è½½éœ€è¦ä¸‹è½½ ~8GB æƒé‡ï¼Œç„¶åè¿›è¡Œé‡åŒ–...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The image processor of type `Gemma3ImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd9539a1e21442f98b829935f56e2d04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… W4A8 é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸï¼\n",
            "Device: cuda:0\n",
            "GPU æ˜¾å­˜å ç”¨: 3.02 GB (é‡åŒ–åæ˜¾è‘—é™ä½)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"google/medgemma-1.5-4b-it\"\n",
        "\n",
        "print(f\"ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹: {model_id} (W4A8 é‡åŒ–)...\")\n",
        "print(\"âš ï¸ é¦–æ¬¡åŠ è½½éœ€è¦ä¸‹è½½ ~8GB æƒé‡ï¼Œç„¶åè¿›è¡Œé‡åŒ–...\\n\")\n",
        "\n",
        "# W4A8 é‡åŒ–é…ç½®ï¼ˆ4-bit æƒé‡ + 8-bit æ¿€æ´»ï¼Œæ›´é«˜ç²¾åº¦ï¼‰\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # 8-bit æ¿€æ´»ç”¨ FP16\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    llm_int8_enable_fp32_cpu_offload=False  # ä¿æŒ 8-bit æ¿€æ´»\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"âœ… W4A8 é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
        "print(f\"Device: {model.device}\")\n",
        "if torch.cuda.is_available():\n",
        "    mem_gb = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "    print(f\"GPU æ˜¾å­˜å ç”¨: {mem_gb:.2f} GB (é‡åŒ–åæ˜¾è‘—é™ä½)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II2YKcBil-F5"
      },
      "source": [
        "## Step 6: æ‰¹é‡ç”ŸæˆæŠ¥å‘Šï¼ˆ233 samplesï¼‰\n",
        "\n",
        "å¯¹ 233 å¼ èƒ¸éƒ¨ X å…‰å›¾ç‰‡ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9zYtToel-F5",
        "outputId": "6582fe34-f143-48c2-f8ed-8fca238e6e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸš€ å¼€å§‹æ‰¹é‡ç”ŸæˆæŠ¥å‘Šï¼ˆå…± 233 æ¡ï¼‰...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ç”ŸæˆæŠ¥å‘Š:   0%|          | 0/233 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   0%|          | 1/233 [00:06<24:52,  6.43s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   1%|          | 2/233 [00:12<23:30,  6.10s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   1%|â–         | 3/233 [00:18<24:01,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   2%|â–         | 4/233 [00:26<26:19,  6.90s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   2%|â–         | 5/233 [00:34<27:28,  7.23s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   3%|â–         | 6/233 [00:43<29:21,  7.76s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   3%|â–         | 7/233 [00:49<27:58,  7.43s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   3%|â–         | 8/233 [00:55<25:33,  6.81s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   4%|â–         | 9/233 [01:01<24:11,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   4%|â–         | 10/233 [01:07<23:28,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   5%|â–         | 11/233 [01:14<24:15,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   5%|â–Œ         | 12/233 [01:22<25:36,  6.95s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   6%|â–Œ         | 13/233 [01:29<25:58,  7.08s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   6%|â–Œ         | 14/233 [01:35<24:08,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   6%|â–‹         | 15/233 [01:40<22:52,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   7%|â–‹         | 16/233 [01:46<22:15,  6.15s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   7%|â–‹         | 17/233 [01:52<21:33,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   8%|â–Š         | 18/233 [01:57<20:59,  5.86s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   8%|â–Š         | 19/233 [02:03<20:53,  5.86s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   9%|â–Š         | 20/233 [02:09<20:34,  5.80s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   9%|â–‰         | 21/233 [02:14<20:17,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:   9%|â–‰         | 22/233 [02:21<21:13,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  10%|â–‰         | 23/233 [02:26<20:11,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  10%|â–ˆ         | 24/233 [02:32<19:48,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  11%|â–ˆ         | 25/233 [02:37<19:30,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  11%|â–ˆ         | 26/233 [02:42<18:53,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  12%|â–ˆâ–        | 27/233 [02:49<20:28,  5.96s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  12%|â–ˆâ–        | 28/233 [02:57<22:08,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  12%|â–ˆâ–        | 29/233 [03:03<21:51,  6.43s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  13%|â–ˆâ–        | 30/233 [03:09<20:51,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  13%|â–ˆâ–        | 31/233 [03:15<20:40,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  14%|â–ˆâ–        | 32/233 [03:22<21:59,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  14%|â–ˆâ–        | 33/233 [03:28<21:05,  6.33s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  15%|â–ˆâ–        | 34/233 [03:34<20:05,  6.06s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  15%|â–ˆâ–Œ        | 35/233 [03:42<22:38,  6.86s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  15%|â–ˆâ–Œ        | 36/233 [03:48<21:06,  6.43s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  16%|â–ˆâ–Œ        | 37/233 [03:57<24:04,  7.37s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  16%|â–ˆâ–‹        | 38/233 [04:03<22:13,  6.84s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  17%|â–ˆâ–‹        | 39/233 [04:09<21:25,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  17%|â–ˆâ–‹        | 40/233 [04:15<20:32,  6.39s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  18%|â–ˆâ–Š        | 41/233 [04:20<19:34,  6.12s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  18%|â–ˆâ–Š        | 42/233 [04:26<18:53,  5.94s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  18%|â–ˆâ–Š        | 43/233 [04:34<20:56,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  19%|â–ˆâ–‰        | 44/233 [04:40<20:01,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  19%|â–ˆâ–‰        | 45/233 [04:48<21:44,  6.94s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  20%|â–ˆâ–‰        | 46/233 [04:54<20:25,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  20%|â–ˆâ–ˆ        | 47/233 [05:07<26:12,  8.45s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  21%|â–ˆâ–ˆ        | 48/233 [05:13<23:48,  7.72s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  21%|â–ˆâ–ˆ        | 49/233 [05:18<21:44,  7.09s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  21%|â–ˆâ–ˆâ–       | 50/233 [05:23<19:15,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  22%|â–ˆâ–ˆâ–       | 51/233 [05:29<18:37,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  22%|â–ˆâ–ˆâ–       | 52/233 [05:39<21:59,  7.29s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  23%|â–ˆâ–ˆâ–       | 53/233 [05:44<20:10,  6.72s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  23%|â–ˆâ–ˆâ–       | 54/233 [05:49<18:30,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  24%|â–ˆâ–ˆâ–       | 55/233 [05:55<18:24,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  24%|â–ˆâ–ˆâ–       | 56/233 [06:03<19:28,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  24%|â–ˆâ–ˆâ–       | 57/233 [06:08<18:27,  6.29s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  25%|â–ˆâ–ˆâ–       | 58/233 [06:17<20:31,  7.04s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  25%|â–ˆâ–ˆâ–Œ       | 59/233 [06:23<19:06,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  26%|â–ˆâ–ˆâ–Œ       | 60/233 [06:28<18:05,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  26%|â–ˆâ–ˆâ–Œ       | 61/233 [06:34<17:59,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  27%|â–ˆâ–ˆâ–‹       | 62/233 [06:41<17:50,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  27%|â–ˆâ–ˆâ–‹       | 63/233 [06:50<20:36,  7.28s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  27%|â–ˆâ–ˆâ–‹       | 64/233 [06:56<19:14,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  28%|â–ˆâ–ˆâ–Š       | 65/233 [07:04<20:04,  7.17s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  28%|â–ˆâ–ˆâ–Š       | 66/233 [07:12<20:27,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  29%|â–ˆâ–ˆâ–‰       | 67/233 [07:17<18:50,  6.81s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  29%|â–ˆâ–ˆâ–‰       | 68/233 [07:23<17:40,  6.43s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  30%|â–ˆâ–ˆâ–‰       | 69/233 [07:28<16:52,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  30%|â–ˆâ–ˆâ–ˆ       | 70/233 [07:34<16:14,  5.98s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  30%|â–ˆâ–ˆâ–ˆ       | 71/233 [07:41<16:53,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  31%|â–ˆâ–ˆâ–ˆ       | 72/233 [07:46<16:06,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  31%|â–ˆâ–ˆâ–ˆâ–      | 73/233 [07:53<16:36,  6.23s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  32%|â–ˆâ–ˆâ–ˆâ–      | 74/233 [07:58<15:35,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  32%|â–ˆâ–ˆâ–ˆâ–      | 75/233 [08:03<14:45,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  33%|â–ˆâ–ˆâ–ˆâ–      | 76/233 [08:14<18:25,  7.04s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  33%|â–ˆâ–ˆâ–ˆâ–      | 77/233 [08:20<17:45,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  33%|â–ˆâ–ˆâ–ˆâ–      | 78/233 [08:26<16:49,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  34%|â–ˆâ–ˆâ–ˆâ–      | 79/233 [08:33<17:07,  6.67s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  34%|â–ˆâ–ˆâ–ˆâ–      | 80/233 [08:40<17:34,  6.89s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  35%|â–ˆâ–ˆâ–ˆâ–      | 81/233 [08:46<16:26,  6.49s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 82/233 [08:51<15:10,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 83/233 [08:56<14:43,  5.89s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 84/233 [09:02<14:21,  5.78s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 85/233 [09:07<13:40,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 86/233 [09:12<13:34,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 87/233 [09:19<14:08,  5.81s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 88/233 [09:24<13:46,  5.70s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 89/233 [09:30<13:35,  5.66s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 90/233 [09:35<13:32,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 91/233 [09:41<13:25,  5.67s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 92/233 [09:45<12:17,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 93/233 [09:52<13:33,  5.81s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 94/233 [09:59<13:49,  5.97s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 95/233 [10:04<13:24,  5.83s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 96/233 [10:10<13:08,  5.76s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 97/233 [10:16<13:02,  5.75s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 98/233 [10:22<13:39,  6.07s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 99/233 [10:27<12:50,  5.75s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/233 [10:34<13:06,  5.91s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 101/233 [10:40<13:06,  5.96s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 102/233 [10:46<13:11,  6.04s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 103/233 [10:55<14:47,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/233 [11:00<13:30,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 105/233 [11:06<13:12,  6.19s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 106/233 [11:12<13:07,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 107/233 [11:17<12:40,  6.04s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 108/233 [11:23<11:56,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 109/233 [11:28<11:52,  5.75s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 110/233 [11:34<11:38,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 111/233 [11:39<11:26,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 112/233 [11:45<11:17,  5.60s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 113/233 [11:52<11:53,  5.95s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 114/233 [12:00<13:25,  6.77s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 115/233 [12:05<12:00,  6.10s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 116/233 [12:10<11:37,  5.96s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 117/233 [12:16<11:13,  5.80s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 118/233 [12:21<10:59,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 119/233 [12:28<11:30,  6.06s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 120/233 [12:34<11:14,  5.97s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 121/233 [12:39<10:43,  5.75s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 122/233 [12:45<10:30,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 123/233 [12:51<10:29,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 124/233 [12:56<10:23,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 125/233 [13:04<11:11,  6.22s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 126/233 [13:09<10:41,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 127/233 [13:15<10:17,  5.82s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 128/233 [13:21<10:20,  5.91s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 129/233 [13:27<10:11,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 130/233 [13:32<09:51,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 131/233 [13:38<09:55,  5.83s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 132/233 [13:43<09:36,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 133/233 [13:51<10:14,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 134/233 [13:56<09:57,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 135/233 [14:02<09:37,  5.89s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 136/233 [14:08<09:22,  5.80s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 137/233 [14:12<08:45,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 138/233 [14:19<09:03,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 139/233 [14:23<08:26,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 140/233 [14:29<08:26,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 141/233 [14:36<09:04,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 142/233 [14:41<08:48,  5.81s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 143/233 [14:47<08:36,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 144/233 [14:52<08:24,  5.66s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 145/233 [14:58<08:20,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 146/233 [15:04<08:32,  5.89s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 147/233 [15:09<07:54,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 148/233 [15:15<08:02,  5.67s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 149/233 [15:20<07:37,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 150/233 [15:29<08:48,  6.37s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 151/233 [15:34<08:22,  6.13s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 152/233 [15:40<08:06,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 153/233 [15:45<07:30,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 154/233 [15:50<07:09,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 155/233 [15:57<07:38,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 156/233 [16:04<08:06,  6.32s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 157/233 [16:09<07:39,  6.05s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 158/233 [16:15<07:20,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 159/233 [16:21<07:13,  5.86s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 160/233 [16:25<06:39,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 161/233 [16:30<06:18,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 162/233 [16:35<06:08,  5.18s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 163/233 [16:41<06:16,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 164/233 [16:46<06:13,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 165/233 [16:52<06:12,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 166/233 [16:57<06:05,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 167/233 [17:03<06:02,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 168/233 [17:08<05:54,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 169/233 [17:16<06:32,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 170/233 [17:22<06:33,  6.24s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 171/233 [17:27<06:00,  5.81s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 172/233 [17:33<05:48,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 173/233 [17:39<05:50,  5.84s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 174/233 [17:46<06:04,  6.19s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 175/233 [17:51<05:48,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 176/233 [17:58<05:56,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 177/233 [18:05<05:52,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 178/233 [18:16<07:15,  7.92s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 179/233 [18:22<06:36,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 180/233 [18:29<06:24,  7.26s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 181/233 [18:35<05:49,  6.72s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 182/233 [18:41<05:37,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 183/233 [18:47<05:13,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 184/233 [18:52<04:59,  6.11s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 185/233 [19:00<05:12,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 186/233 [19:04<04:38,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 187/233 [19:08<04:05,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 188/233 [19:14<04:02,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 189/233 [19:20<04:02,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 190/233 [19:25<03:56,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 191/233 [19:31<03:51,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 192/233 [19:36<03:44,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 193/233 [19:42<03:40,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 194/233 [19:47<03:28,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 195/233 [19:53<03:38,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 196/233 [19:58<03:24,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 197/233 [20:05<03:25,  5.70s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 198/233 [20:10<03:17,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 199/233 [20:16<03:16,  5.79s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 200/233 [20:22<03:11,  5.81s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 201/233 [20:28<03:05,  5.79s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 202/233 [20:34<03:06,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 203/233 [20:39<02:47,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 204/233 [20:44<02:41,  5.57s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 205/233 [20:50<02:34,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 206/233 [20:55<02:28,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 207/233 [21:01<02:24,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 208/233 [21:06<02:18,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 209/233 [21:14<02:25,  6.08s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 210/233 [21:22<02:35,  6.74s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 211/233 [21:28<02:20,  6.41s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 212/233 [21:36<02:26,  6.98s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 213/233 [21:41<02:10,  6.51s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 214/233 [21:47<01:57,  6.19s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 215/233 [21:56<02:05,  6.98s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 216/233 [22:01<01:51,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 217/233 [22:06<01:38,  6.13s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 218/233 [22:12<01:29,  5.96s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 219/233 [22:29<02:10,  9.35s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 220/233 [22:36<01:50,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 221/233 [22:41<01:31,  7.62s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 222/233 [22:47<01:17,  7.01s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 223/233 [22:53<01:06,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 224/233 [22:58<00:57,  6.38s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 225/233 [23:04<00:49,  6.13s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 226/233 [23:10<00:42,  6.12s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 227/233 [23:16<00:35,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 228/233 [23:22<00:30,  6.02s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 229/233 [23:29<00:25,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 230/233 [23:37<00:20,  6.82s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 231/233 [23:42<00:12,  6.43s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 232/233 [23:47<00:05,  5.85s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "ç”ŸæˆæŠ¥å‘Š: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233/233 [23:52<00:00,  6.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼\n",
            "æˆåŠŸç”Ÿæˆ: 233 æ¡\n",
            "è·³è¿‡: 0 æ¡\n",
            "ç»“æœå·²ä¿å­˜è‡³: /content/drive/MyDrive/medgamma/medgemma_a4w8_reports_233.csv\n",
            "\n",
            "--- å‰ 3 æ¡æŠ¥å‘Šé¢„è§ˆ ---\n",
            "\n",
            "[1] Subject: 10075925\n",
            "Ground Truth: Mild pulmonary vascular congestion with mild to moderate interstitial pulmonary edema are new compar...\n",
            "Generated: The heart size is enlarged. There is increased opacity in the right lower lung field. The mediastinu...\n",
            "\n",
            "[2] Subject: 10174198\n",
            "Ground Truth: Lungs are clear without consolidation, effusion, or pneumothorax.  The cardiomediastinal silhouette ...\n",
            "Generated: FINDINGS: The heart size is normal. The mediastinal contours are normal. The lungs are clear. There ...\n",
            "\n",
            "[3] Subject: 10199765\n",
            "Ground Truth: Subtle patchy opacity along the left heart border on the frontal view, not substantiated on the late...\n",
            "Generated: The heart size is mildly enlarged. The lungs are clear without focal consolidation, pleural effusion...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_one_report(image_path, view_position):\n",
        "    \"\"\"\n",
        "    ä¸ºå•å¼ å›¾ç‰‡ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Š\n",
        "    \"\"\"\n",
        "    # æç¤ºè¯\n",
        "    prompt_text = (\n",
        "        f\"You are an expert radiologist. Describe this {view_position} view chest X-ray. \"\n",
        "        \"Provide a concise report consisting of Findings and Impression. \"\n",
        "        \"Focus on the heart, lungs, mediastinum, pleural space, and bones. \"\n",
        "        \"Do NOT use bullet points, asterisks, or section headers. \"\n",
        "        \"Do NOT include disclaimers or 'AI' warnings. \"\n",
        "        \"Output pure medical text only.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return f\"ERROR_IMAGE_LOAD: {e}\"\n",
        "\n",
        "    # æ„å»ºè¾“å…¥\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": pil_image},\n",
        "                {\"type\": \"text\", \"text\": prompt_text}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, tokenize=True,\n",
        "        return_dict=True, return_tensors=\"pt\"\n",
        "    ).to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    # æ¨ç†\n",
        "    with torch.inference_mode():\n",
        "        generation = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            do_sample=False\n",
        "        )\n",
        "        generation = generation[0][input_len:]  # è£å‰ªæ‰ prompt\n",
        "\n",
        "    # âœ… è§£ç å¹¶å¼ºåŠ›æ¸…æ´—ï¼Œå»é™¤æ ¼å¼æ ‡è®°\n",
        "    raw_text = processor.decode(generation, skip_special_tokens=True)\n",
        "    clean_text = raw_text.replace(\"Findings:\", \"\").replace(\"Impression:\", \"\")\n",
        "    clean_text = clean_text.replace(\"**\", \"\").replace(\"*\", \"\")  # å»é™¤ markdown\n",
        "    clean_text = clean_text.replace(\"###\", \"\").replace(\"##\", \"\").replace(\"#\", \"\")\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# ==========================================\n",
        "# æ‰¹é‡ç”Ÿæˆ\n",
        "# ==========================================\n",
        "print(f\"\\nğŸš€ å¼€å§‹æ‰¹é‡ç”ŸæˆæŠ¥å‘Šï¼ˆå…± {len(df)} æ¡ï¼‰...\\n\")\n",
        "\n",
        "results = []\n",
        "skipped_count = 0\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"ç”ŸæˆæŠ¥å‘Š\"):\n",
        "    try:\n",
        "        # âœ… ä½¿ç”¨ Image_Path åˆ—\n",
        "        img_path = row.get('Image_Path')\n",
        "        view = row.get('View', 'PA')\n",
        "\n",
        "        # éªŒè¯è·¯å¾„\n",
        "        if not img_path or not os.path.exists(img_path):\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # ç”ŸæˆæŠ¥å‘Š\n",
        "        generated_report = generate_one_report(img_path, view)\n",
        "\n",
        "        # æ£€æŸ¥é”™è¯¯\n",
        "        if \"ERROR_IMAGE_LOAD\" in generated_report:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # âœ… Ground_Truth åˆ—\n",
        "        gt_col = 'Ground_Truth' if 'Ground_Truth' in df.columns else 'text'\n",
        "        ground_truth = str(row.get(gt_col, '')).strip()\n",
        "\n",
        "        # ä¿å­˜ç»“æœ\n",
        "        results.append({\n",
        "            \"subject_id\": row.get('subject_id', idx),\n",
        "            \"View\": view,\n",
        "            \"Image_Path\": img_path,\n",
        "            \"Ground_Truth\": ground_truth,\n",
        "            \"Generated_Report\": generated_report\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error at index {idx}: {e}\")\n",
        "        skipped_count += 1\n",
        "        continue\n",
        "\n",
        "# ä¿å­˜ç»“æœ\n",
        "df_results = pd.DataFrame(results)\n",
        "output_path = \"/content/drive/MyDrive/medgamma/medgemma_a4w8_reports_233.csv\"\n",
        "df_results.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\nâœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼\")\n",
        "print(f\"æˆåŠŸç”Ÿæˆ: {len(results)} æ¡\")\n",
        "print(f\"è·³è¿‡: {skipped_count} æ¡\")\n",
        "print(f\"ç»“æœå·²ä¿å­˜è‡³: {output_path}\")\n",
        "\n",
        "# é¢„è§ˆå‰ 3 æ¡\n",
        "print(\"\\n--- å‰ 3 æ¡æŠ¥å‘Šé¢„è§ˆ ---\")\n",
        "for i in range(min(3, len(df_results))):\n",
        "    print(f\"\\n[{i+1}] Subject: {df_results.iloc[i]['subject_id']}\")\n",
        "    print(f\"Ground Truth: {df_results.iloc[i]['Ground_Truth'][:100]}...\")\n",
        "    print(f\"Generated: {df_results.iloc[i]['Generated_Report'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi8lfY9gHj1S",
        "outputId": "870422d4-1edf-4821-94e8-dc3e5cfac903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§¹ æ¸…ç† MedGemma æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜...\n",
            "  âš ï¸ model ä¸å­˜åœ¨ï¼ˆå¯èƒ½å·²æ¸…ç†ï¼‰\n",
            "  âš ï¸ processor ä¸å­˜åœ¨ï¼ˆå¯èƒ½å·²æ¸…ç†ï¼‰\n",
            "\n",
            "âœ… å·²é‡Šæ”¾æ˜¾å­˜ï¼Œå½“å‰å ç”¨: 0.48 GB\n",
            "âœ… å·²é‡ç½®å³°å€¼ç»Ÿè®¡\n",
            "ï¼ˆRadGraph è¯„ä¼°æ—¶ä¼šåŠ è½½ ~2-3 GBï¼‰\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Step 7.5: æ¸…ç† MedGemma æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜\n",
        "# ==========================================\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ§¹ æ¸…ç† MedGemma æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜...\")\n",
        "\n",
        "# å®¹é”™åˆ é™¤ï¼šæ£€æŸ¥å˜é‡æ˜¯å¦å­˜åœ¨\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "    print(\"  âœ… å·²åˆ é™¤ model\")\n",
        "else:\n",
        "    print(\"  âš ï¸ model ä¸å­˜åœ¨ï¼ˆå¯èƒ½å·²æ¸…ç†ï¼‰\")\n",
        "\n",
        "if 'processor' in globals():\n",
        "    del processor\n",
        "    print(\"  âœ… å·²åˆ é™¤ processor\")\n",
        "else:\n",
        "    print(\"  âš ï¸ processor ä¸å­˜åœ¨ï¼ˆå¯èƒ½å·²æ¸…ç†ï¼‰\")\n",
        "\n",
        "# æ¸…ç†ç¼“å­˜\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# é‡ç½®å³°å€¼ç»Ÿè®¡\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats(0)\n",
        "\n",
        "# æ£€æŸ¥é‡Šæ”¾åçš„æ˜¾å­˜\n",
        "if torch.cuda.is_available():\n",
        "    current_mem = torch.cuda.memory_allocated(0) / 1e9\n",
        "    print(f\"\\nâœ… å·²é‡Šæ”¾æ˜¾å­˜ï¼Œå½“å‰å ç”¨: {current_mem:.2f} GB\")\n",
        "    print(f\"âœ… å·²é‡ç½®å³°å€¼ç»Ÿè®¡\")\n",
        "    print(f\"ï¼ˆRadGraph è¯„ä¼°æ—¶ä¼šåŠ è½½ ~2-3 GBï¼‰\\n\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ æœªæ£€æµ‹åˆ° GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3HWXsYil-F6"
      },
      "source": [
        "## Step 7: RadGraph F1 è¯„ä¼°\n",
        "\n",
        "ä½¿ç”¨ RadGraph æŒ‡æ ‡è¯„ä¼°ç”ŸæˆæŠ¥å‘Šçš„è´¨é‡ã€‚\n",
        "\n",
        "**æŒ‡æ ‡è¯´æ˜ï¼š**\n",
        "- **RG_E**: Entity F1ï¼ˆå®ä½“åŒ¹é…ï¼‰\n",
        "- **RG_ER**: Entity + Relation F1ï¼ˆå®ä½“+å…³ç³»ï¼Œè®ºæ–‡å¸¸ç”¨æŒ‡æ ‡ï¼‰\n",
        "- **RG_ER_bar**: Complete Match F1ï¼ˆå®Œå…¨åŒ¹é…ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niTISKZow-tV",
        "outputId": "c6115bee-6be1-4eea-87d1-ccdd0ea00ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… RadGraph å…¼å®¹æ€§ä¿®å¤å·²åº”ç”¨: encode_plus, build_inputs_with_special_tokens\n"
          ]
        }
      ],
      "source": [
        "# âš ï¸ RadGraph å…¼å®¹æ€§ä¿®å¤ï¼ˆå¿…é¡»å…ˆè¿è¡Œè¿™ä¸ª cellï¼ï¼‰\n",
        "# ä¿®å¤ transformers 5.x ä¸ radgraph çš„å…¼å®¹æ€§é—®é¢˜\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "\n",
        "fixed_methods = []\n",
        "\n",
        "# 1. æ·»åŠ  encode_plus æ–¹æ³•\n",
        "if not hasattr(BertTokenizer, 'encode_plus'):\n",
        "    def encode_plus_wrapper(self, text, *args, **kwargs):\n",
        "        return self(text, *args, **kwargs)\n",
        "    BertTokenizer.encode_plus = encode_plus_wrapper\n",
        "    fixed_methods.append('encode_plus')\n",
        "\n",
        "# 2. æ·»åŠ  build_inputs_with_special_tokens æ–¹æ³•\n",
        "if not hasattr(BertTokenizer, 'build_inputs_with_special_tokens'):\n",
        "    def build_inputs_with_special_tokens_wrapper(self, token_ids_0, token_ids_1=None):\n",
        "        # BERT: [CLS] + tokens_0 + [SEP] + tokens_1 + [SEP]\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        return [self.cls_token_id] + token_ids_0 + [self.sep_token_id] + token_ids_1 + [self.sep_token_id]\n",
        "    BertTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens_wrapper\n",
        "    fixed_methods.append('build_inputs_with_special_tokens')\n",
        "\n",
        "# 3. æ·»åŠ  get_special_tokens_mask æ–¹æ³•ï¼ˆå¯èƒ½ä¹Ÿä¼šéœ€è¦ï¼‰\n",
        "if not hasattr(BertTokenizer, 'get_special_tokens_mask'):\n",
        "    def get_special_tokens_mask_wrapper(\n",
        "        self, token_ids_0, token_ids_1=None, already_has_special_tokens=False\n",
        "    ):\n",
        "        if already_has_special_tokens:\n",
        "            return super(BertTokenizer, self).get_special_tokens_mask(\n",
        "                token_ids_0=token_ids_0,\n",
        "                token_ids_1=token_ids_1,\n",
        "                already_has_special_tokens=True,\n",
        "            )\n",
        "        if token_ids_1 is None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "    BertTokenizer.get_special_tokens_mask = get_special_tokens_mask_wrapper\n",
        "    fixed_methods.append('get_special_tokens_mask')\n",
        "\n",
        "if fixed_methods:\n",
        "    print(f\"âœ… RadGraph å…¼å®¹æ€§ä¿®å¤å·²åº”ç”¨: {', '.join(fixed_methods)}\")\n",
        "else:\n",
        "    print(\"âœ… BertTokenizer å·²æœ‰æ‰€æœ‰å¿…éœ€æ–¹æ³•\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLgDyH6kl-F6",
        "outputId": "58516ea2-171a-4bb7-decd-af766bb34e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” å¼€å§‹ RadGraph F1 è¯„ä¼°...\n",
            "è¯„ä¼°æ ·æœ¬æ•°: 233\n",
            "\n",
            "ğŸ“Š è¯„ä¼°å‰æ˜¾å­˜: 0.48 GB\n",
            "\n",
            "Using device: cuda:0\n",
            "model_type not provided, defaulting to radgraph-xl\n",
            "\n",
            "============================================================\n",
            "RadGraph F1 è¯„ä¼°ç»“æœï¼ˆç™¾åˆ†åˆ¶ï¼‰\n",
            "============================================================\n",
            "RG_E (Entity):              25.91\n",
            "RG_ER (Entity+Relation):    33.39  â† è®ºæ–‡å¸¸ç”¨\n",
            "RG_ER_bar (Complete Match): 34.79\n",
            "============================================================\n",
            "\n",
            "è¯¦ç»†ç»Ÿè®¡ï¼ˆ233 ä¸ªæ ·æœ¬ï¼‰:\n",
            "  RG_E:       min=0.00, max=66.67, mean=25.91\n",
            "  RG_ER:      min=0.00, max=77.42, mean=33.39\n",
            "  RG_ER_bar:  min=0.00, max=77.78, mean=34.79\n",
            "\n",
            "ğŸ“Š GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ:\n",
            "  è¯„ä¼°å‰: 0.48 GB\n",
            "  å½“å‰:   0.48 GB\n",
            "  å³°å€¼:   2.80 GB  â† RadGraph è¯„ä¼°å³°å€¼\n",
            "\n",
            "âœ… è¯„ä¼°å®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "from radgraph import F1RadGraph\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ” å¼€å§‹ RadGraph F1 è¯„ä¼°...\")\n",
        "print(f\"è¯„ä¼°æ ·æœ¬æ•°: {len(df_results)}\\n\")\n",
        "\n",
        "# è®°å½•è¯„ä¼°å‰æ˜¾å­˜\n",
        "if torch.cuda.is_available():\n",
        "    before_eval = torch.cuda.memory_allocated(0) / 1e9\n",
        "    print(f\"ğŸ“Š è¯„ä¼°å‰æ˜¾å­˜: {before_eval:.2f} GB\\n\")\n",
        "\n",
        "# å‡†å¤‡æ•°æ®\n",
        "refs = df_results[\"Ground_Truth\"].tolist()\n",
        "hyps = df_results[\"Generated_Report\"].tolist()\n",
        "\n",
        "# è¿‡æ»¤æ‰ç©ºæŠ¥å‘Š\n",
        "valid_pairs = [(h, r) for h, r in zip(hyps, refs) if h and r and len(h.strip()) > 0]\n",
        "if len(valid_pairs) < len(hyps):\n",
        "    print(f\"âš ï¸ è·³è¿‡ {len(hyps) - len(valid_pairs)} æ¡ç©ºæŠ¥å‘Š\")\n",
        "\n",
        "hyps_clean, refs_clean = zip(*valid_pairs) if valid_pairs else ([], [])\n",
        "\n",
        "# åˆå§‹åŒ– RadGraphï¼ˆä½¿ç”¨ 'all' è·å–æ‰€æœ‰æŒ‡æ ‡ï¼‰\n",
        "f1radgraph = F1RadGraph(reward_level=\"all\")\n",
        "\n",
        "# è®¡ç®— F1\n",
        "results = f1radgraph(hyps=list(hyps_clean), refs=list(refs_clean))\n",
        "\n",
        "# è§£æè¿”å›å€¼ï¼šresults[0] = (simple, partial, complete)\n",
        "avg_scores = results[0]\n",
        "simple_f1 = float(avg_scores[0])    # RG_ER_bar (Complete Match)\n",
        "partial_f1 = float(avg_scores[1])   # RG_ER (Entity + Relation) â† è®ºæ–‡å¸¸ç”¨\n",
        "complete_f1 = float(avg_scores[2])  # RG_E (Entity)\n",
        "\n",
        "# æ˜¾ç¤ºç»“æœ\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RadGraph F1 è¯„ä¼°ç»“æœï¼ˆç™¾åˆ†åˆ¶ï¼‰\")\n",
        "print(\"=\"*60)\n",
        "print(f\"RG_E (Entity):              {complete_f1*100:.2f}\")\n",
        "print(f\"RG_ER (Entity+Relation):    {partial_f1*100:.2f}  â† è®ºæ–‡å¸¸ç”¨\")\n",
        "print(f\"RG_ER_bar (Complete Match): {simple_f1*100:.2f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡\n",
        "if len(results) > 1 and isinstance(results[1], tuple) and len(results[1]) >= 3:\n",
        "    simple_scores = results[1][0]\n",
        "    partial_scores = results[1][1]\n",
        "    complete_scores = results[1][2]\n",
        "\n",
        "    print(f\"\\nè¯¦ç»†ç»Ÿè®¡ï¼ˆ{len(simple_scores)} ä¸ªæ ·æœ¬ï¼‰:\")\n",
        "    print(f\"  RG_E:       min={min(complete_scores)*100:.2f}, max={max(complete_scores)*100:.2f}, mean={sum(complete_scores)/len(complete_scores)*100:.2f}\")\n",
        "    print(f\"  RG_ER:      min={min(partial_scores)*100:.2f}, max={max(partial_scores)*100:.2f}, mean={sum(partial_scores)/len(partial_scores)*100:.2f}\")\n",
        "    print(f\"  RG_ER_bar:  min={min(simple_scores)*100:.2f}, max={max(simple_scores)*100:.2f}, mean={sum(simple_scores)/len(simple_scores)*100:.2f}\")\n",
        "\n",
        "# æ˜¾ç¤ºæ˜¾å­˜ä¿¡æ¯\n",
        "if torch.cuda.is_available():\n",
        "    after_eval = torch.cuda.memory_allocated(0) / 1e9\n",
        "    peak_mem = torch.cuda.max_memory_allocated(0) / 1e9\n",
        "\n",
        "    print(f\"\\nğŸ“Š GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ:\")\n",
        "    print(f\"  è¯„ä¼°å‰: {before_eval:.2f} GB\")\n",
        "    print(f\"  å½“å‰:   {after_eval:.2f} GB\")\n",
        "    print(f\"  å³°å€¼:   {peak_mem:.2f} GB  â† RadGraph è¯„ä¼°å³°å€¼\")\n",
        "\n",
        "print(\"\\nâœ… è¯„ä¼°å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1riRrwipl-F6"
      },
      "source": [
        "## é™„å½•ï¼šRadGraph Patchï¼ˆå¦‚é‡åˆ°å…¼å®¹æ€§é—®é¢˜ï¼‰\n",
        "\n",
        "å¦‚æœ RadGraph æŠ¥é”™ `encode_plus` ç›¸å…³é”™è¯¯ï¼Œè¿è¡Œä¸‹é¢çš„ patchï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXSlvzn0l-F6",
        "outputId": "8a9d6aac-c945-4d3a-9182-09f554520b9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… RadGraph å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ— éœ€ patch\n"
          ]
        }
      ],
      "source": [
        "# RadGraph å…¼å®¹æ€§ä¿®å¤\n",
        "import os\n",
        "\n",
        "radgraph_file = \"/usr/local/lib/python3.12/dist-packages/radgraph/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py\"\n",
        "\n",
        "if os.path.exists(radgraph_file):\n",
        "    content = open(radgraph_file).read()\n",
        "    if \".encode_plus(\" in content:\n",
        "        open(radgraph_file, \"w\").write(content.replace(\".encode_plus(\", \"(\"))\n",
        "        print(\"âœ… RadGraph patch å·²åº”ç”¨\")\n",
        "    else:\n",
        "        print(\"âœ… RadGraph å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ— éœ€ patch\")\n",
        "else:\n",
        "    print(\"âš ï¸ RadGraph æ–‡ä»¶æœªæ‰¾åˆ°\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "35ca7e176e7b40228b9a8c093c11081a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38ede464f0df486a89346fd380d4b947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c7e58d3b8de40749a2e0d3b9855ee81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523277e21be348698d9fdd42750744e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e45deffe714246c4b4e81cc97e4495ea",
            "max": 883,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38ede464f0df486a89346fd380d4b947",
            "value": 883
          }
        },
        "69af1d7a832a486eafedaae356e43bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7f1fa646b6d472780b4e13f657304de",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_35ca7e176e7b40228b9a8c093c11081a",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "a5b4946ab34c4fa3a0f442a9459e2dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7f1fa646b6d472780b4e13f657304de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd9539a1e21442f98b829935f56e2d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69af1d7a832a486eafedaae356e43bdf",
              "IPY_MODEL_523277e21be348698d9fdd42750744e2",
              "IPY_MODEL_f2e6c18de336486286e7bb0ceca6c0d4"
            ],
            "layout": "IPY_MODEL_4c7e58d3b8de40749a2e0d3b9855ee81"
          }
        },
        "e45deffe714246c4b4e81cc97e4495ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee50228985f442d5bc425d7e70851502": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2e6c18de336486286e7bb0ceca6c0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee50228985f442d5bc425d7e70851502",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a5b4946ab34c4fa3a0f442a9459e2dfa",
            "value": "â€‡883/883â€‡[00:03&lt;00:00,â€‡838.30it/s,â€‡Materializingâ€‡param=model.vision_tower.vision_model.post_layernorm.weight]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
